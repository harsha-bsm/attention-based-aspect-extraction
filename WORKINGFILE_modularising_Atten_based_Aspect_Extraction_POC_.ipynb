{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WORKINGFILE_modularising_Atten_based_Aspect_Extraction_POC_.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harsha-bsm/attention-based-aspect-extraction/blob/main/WORKINGFILE_modularising_Atten_based_Aspect_Extraction_POC_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoEKsqQceLKm",
        "outputId": "f91021d1-c7d9-465c-f295-8766d7f52f28"
      },
      "source": [
        "# Mounting files on google drive which will ease file accessing \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/') "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJ6w8UfZssTh",
        "outputId": "86a5c917-a47e-4b38-cd16-1d51595d0637"
      },
      "source": [
        "#%%writefile /content/drive/MyDrive/REVIEWS/preprocess.py\n",
        "# importing necessary modules\n",
        "!pip install emot\n",
        "import emot\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import seaborn as sns\n",
        "import pandas as pd\n",
        "from gensim.models import FastText \n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense,Concatenate,TimeDistributed,Masking,GRU,Input,Dot,Reshape,Softmax\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Using NLTK for PoS tagging and Stopwords removal\n",
        "# Downloading the corresponding packages\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from gensim.models import FastText \n",
        "from random import seed\n",
        "from random import randint\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "DATA_PATH=\"/content/drive/My Drive/REVIEWS/\"\n",
        "raw_file=\"reviews.pickle\"\n",
        "preprocessed_file=\"preprocessed_df.pickle\"\n",
        "trained_embeddings=\"trained_embeddings\"\n",
        "padded_seqfile=\"padded_sequences.pickle\"\n",
        "train_again=True\n",
        "vocabtrain_again=False\n",
        "stopword=stopwords.words(fileids=\"english\")\n",
        "buffer_size=1024\n",
        "batch_size=100\n",
        "negative_samples=20\n",
        "\n",
        "\n",
        "#  removing some of the common contractions.\n",
        "def decontractions(phrase):\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "def convert_emojis(review):\n",
        "  for x in review:\n",
        "    if x in emot.emo_unicode.UNICODE_EMOJI.keys():\n",
        "      review=review.replace(x,\"\")\n",
        "  return review\n",
        "\n",
        "#Converting all the extra spaces into single space. This will help while splitting the data in the future.\n",
        "def removecharacters(review):\n",
        "  review= re.sub('[^A-Za-z0-9]+',' ',review)  #anything exept numbers and alphabets, replace them with space\n",
        "  review=re.sub(r\"\\n\",\" \",review)  #new lines into space\n",
        "  review=re.sub(r\"\\t\",\" \",review)  #tabs into space\n",
        "  review=re.sub(r\"\\v\",\" \",review)  #vertical tab into space\n",
        "  review=re.sub(r\"\\s\",\" \",review)   #all extra spaces into single space\n",
        "  return review.lower()\n",
        "\n",
        "def removenewords(review,wordcountdict):\n",
        "  return \" \".join([word for word in review.split(\" \") if  word in list(wordcountdict.keys())])\n",
        "\n",
        "def removestopwords(review,stopword):\n",
        "  return \" \".join([word for word in review.split(\" \") if not word in stopword])\n",
        "\n",
        "def word_count(review,wordcountdict,min_word_repeat):\n",
        "  return  \" \".join([word for word in review.split() if wordcountdict[word]>min_word_repeat] )\n",
        "\n",
        "\n",
        "\n",
        "def text_processing(stopword=stopword,min_word_repeat=10,sent_len_percentile=99.9,dump_aspickle=True,return_df=False):\n",
        "  #stop_words=set(stopwords.words('english'))\n",
        "  raw_path=os.path.join(DATA_PATH,raw_file)\n",
        "  preprocessed_path=os.path.join(DATA_PATH,preprocessed_file)\n",
        "  if os.path.isfile(preprocessed_path):\n",
        "    preprocessed_df=pd.read_pickle(preprocessed_path)\n",
        "  else:\n",
        "    df = pd.read_pickle(raw_path)\n",
        "    df[\"review\"]=df[\"review\"].apply(lambda x:x.strip(\"READ MORE\").lower())\n",
        "    df[\"review\"]=df[\"review\"].apply(convert_emojis)\n",
        "    df[\"review\"]=df[\"review\"].apply(decontractions)\n",
        "    df[\"review\"]=df[\"review\"].apply(removecharacters)\n",
        "    df[\"review\"]=pd.DataFrame(df[\"review\"].apply(lambda x:removestopwords(x,stopword)))\n",
        "    df[\"len\"]=df.review.str.split().apply(len)\n",
        "    l=np.percentile(df.len,sent_len_percentile)\n",
        "    df=df.loc[(df[\"len\"]>0) & (df[\"len\"]<l+1)]\n",
        "    tkn = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n') #tensorflow tokenising\n",
        "    tkn.fit_on_texts(df['review'].values)\n",
        "    word_countdict=tkn.word_counts  #word count dictionary\n",
        "    df[\"review\"]=pd.DataFrame(df[\"review\"].apply(lambda x:word_count(x,word_countdict,min_word_repeat)))\n",
        "    df[\"len\"]=df.review.str.split(\" \").apply(len)\n",
        "    if  dump_aspickle:\n",
        "      with open(preprocessed_path,\"wb\") as file:\n",
        "        pickle.dump(df.loc[df[\"len\"]>0],file)\n",
        "    preprocessed_df=df.loc[df[\"len\"]>0]\n",
        "  if return_df:\n",
        "    return preprocessed_df\n",
        "\n",
        "\n",
        "def tokenisation_on_traindata(return_tkn=True):\n",
        "  if os.path.isfile(os.path.join(DATA_PATH,preprocessed_file)):\n",
        "    train=pd.read_pickle(os.path.join(DATA_PATH,preprocessed_file))\n",
        "  else:\n",
        "    train=text_processing(stopword=stopword,min_word_repeat=10,sent_len_percentile=99.9,dump_aspickle=True,return_df=True)\n",
        "  tkn = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n') #tensorflow tokenising\n",
        "  tkn.fit_on_texts(train['review'].values)\n",
        "  with open(os.path.join(DATA_PATH,\"token.pickle\"),\"wb\") as file:\n",
        "    pickle.dump(tkn,file)\n",
        "  if return_tkn:\n",
        "    return tkn\n",
        "\n",
        "\n",
        "def training_vocab(embed_dim=100,negative_sampling=5,min_count=1,window=10,iter=250,sg=1,train_again=vocabtrain_again,return_model=True):\n",
        "  preprocessed_path=os.path.join(DATA_PATH,preprocessed_file)\n",
        "  if os.path.isfile(preprocessed_path):\n",
        "    df=pd.read_pickle(preprocessed_path)\n",
        "  else:\n",
        "    df=text_processing(stopword=set(stopwords.words('english')),return_df=True)\n",
        "  tkn = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "  tkn.fit_on_texts(df['review'].values)\n",
        "  word_countdict=tkn.word_counts\n",
        "  seq_texts=tkn.texts_to_sequences(df['review'].values)  #converting tokenised reviews into sequence of\n",
        "# integers with each integer corresponding to one word in the corpus\n",
        "  text=pd.Series(df['review'].values).apply(lambda x: x.split())  #spliting the reviews in a format suitable to  feed \n",
        "#into Fasttext for trainig vocab \n",
        "\n",
        "  trained_embeddings_path=os.path.join(DATA_PATH,trained_embeddings)\n",
        "  if os.path.isfile(trained_embeddings_path):\n",
        "    if train_again:\n",
        "      model = FastText(size=embed_dim, negative=negative_sampling, min_count=min_count,window=window,iter=iter,sg=sg) \n",
        "      model.build_vocab(text) \n",
        "      gensim_fasttext = model.train(sentences=text, \n",
        "                           sg=sg, ##skipgram\n",
        "                           epochs=iter, ##no of iterations\n",
        "                           size=embed_dim, ##dimentions of word embedding\n",
        "                           seed=1,\n",
        "                           total_examples=model.corpus_count)\n",
        "      model.save(trained_embeddings_path)\n",
        "    else:\n",
        "      model=FastText.load(trained_embeddings_path)\n",
        "  else:\n",
        "    model = FastText(size=embed_dim, negative=negative_sampling, min_count=min_count,window=window,iter=iter,sg=sg) \n",
        "    model.build_vocab(text)\n",
        "    gensim_fasttext = model.train(sentences=text, \n",
        "                           sg=sg, ##skipgram\n",
        "                           epochs=iter, ##no of iterations\n",
        "                           size=embed_dim, ##dimentions of word embedding\n",
        "                           seed=1,\n",
        "                           total_examples=model.corpus_count)\n",
        "    model.save(trained_embeddings_path)\n",
        "  \n",
        "  \n",
        "  if return_model:\n",
        "    return model\n",
        "\n",
        "def textinputsequence_padding(padding=\"post\",padded_seqagain=train_again,return_paddedsequences=True):\n",
        "  preprocessed_path=os.path.join(DATA_PATH,preprocessed_file)\n",
        "  padded_seqpath=os.path.join(DATA_PATH,padded_seqfile)\n",
        "  if os.path.isfile(preprocessed_path):\n",
        "    df=pd.read_pickle(preprocessed_path)\n",
        "  else:\n",
        "    df=text_processing(stopword=set(stopwords.words('english')),return_df=True)\n",
        "  if os.path.isfile(padded_seqpath):\n",
        "\n",
        "    if padded_seqagain:\n",
        "      tkn = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "      tkn.fit_on_texts(df['review'].values)\n",
        "      word_countdict=tkn.word_counts\n",
        "      seq_texts=tkn.texts_to_sequences(df['review'].values)\n",
        "      seq_texts=tf.keras.preprocessing.sequence.pad_sequences(seq_texts,\n",
        "                                                         maxlen=max(df.len),\n",
        "                                                         padding=padding)\n",
        "      with open(os.path.join(DATA_PATH,\"padded_sequences.pickle\"),\"wb\") as file:\n",
        "        pickle.dump(seq_texts,file)\n",
        "    else:\n",
        "      with open(padded_seqpath,\"rb\") as file:\n",
        "        seq_texts=pickle.load(file)\n",
        "  else:\n",
        "    tkn = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "    tkn.fit_on_texts(df['review'].values)\n",
        "    word_countdict=tkn.word_counts\n",
        "    seq_texts=tkn.texts_to_sequences(df['review'].values)\n",
        "    seq_texts=tf.keras.preprocessing.sequence.pad_sequences(seq_texts,\n",
        "                                                         maxlen=max(df.len),\n",
        "                                                         padding=padding)\n",
        "    with open(os.path.join(DATA_PATH,\"padded_sequences.pickle\"),\"wb\") as file:\n",
        "      pickle.dump(seq_texts,file)\n",
        "\n",
        "  if return_paddedsequences:\n",
        "    return seq_texts\n",
        "\n",
        "\n",
        "def generate_dataset(buffer_size=buffer_size,batch_size=batch_size,negative_samples=negative_samples):\n",
        "  if  os.path.isfile(os.path.join(DATA_PATH,padded_seqfile)):\n",
        "    with open(os.path.join(DATA_PATH,\"padded_sequences.pickle\"),\"rb\") as file:\n",
        "      seq_texts=pickle.load(file)\n",
        "  else:\n",
        "    seq_texts=textinputsequence_padding(padding=\"post\",padded_seqagain=train_again,return_paddedsequences=True)\n",
        "\n",
        "  def gendata():\n",
        "    seed(42)\n",
        "    for i in range(0,len(seq_texts)):\n",
        "      lis=[]\n",
        "      lent=[]\n",
        "      while len(lent)<negative_samples:\n",
        "        value = randint(0, len(seq_texts)-1)\n",
        "        if value==i:\n",
        "          continue\n",
        "        lis.append(seq_texts[value])\n",
        "        lent.append(value)\n",
        "      yield seq_texts[i],lis\n",
        "  dataset=tf.data.Dataset.from_generator(gendata, output_types=(tf.int32,tf.int32))\n",
        "  dataset=dataset.repeat(1).shuffle(buffer_size=buffer_size).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emot\n",
            "  Downloading emot-3.1-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 20 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 40 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 18 kB/s \n",
            "\u001b[?25hInstalling collected packages: emot\n",
            "Successfully installed emot-3.1\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bixEGnbIIa3M"
      },
      "source": [
        "#### Building custom attention layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp8YNv4UFCba",
        "outputId": "0638138e-98d8-44ca-ee91-dee0c236cfe0"
      },
      "source": [
        "#%%writefile /content/drive/MyDrive/REVIEWS/model.py\n",
        "\n",
        "\n",
        "\n",
        "# importing necessary modules\n",
        "\n",
        "import emot\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import seaborn as sns\n",
        "import pandas as pd\n",
        "from gensim.models import FastText \n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense,Concatenate,TimeDistributed,Masking,GRU,Input,Dot,Reshape,Softmax\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Using NLTK for PoS tagging and Stopwords removal\n",
        "# Downloading the corresponding packages\n",
        "import pickle\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from gensim.models import FastText \n",
        "from random import seed\n",
        "from random import randint\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "embed_outputdim=100\n",
        "aspects_k=10 #number of aspects\n",
        "\n",
        "def embedding_layerinit():\n",
        "  model=training_vocab(embed_dim=embed_outputdim,negative_sampling=5,min_count=1,window=10,iter=250,sg=1,train_again=vocabtrain_again,return_model=True)\n",
        "  trained_weights=np.vstack((np.zeros((1,100)),model.wv.vectors))\n",
        "  return trained_weights,model\n",
        "\n",
        "def inputlength():\n",
        "  seq=textinputsequence_padding(padding=\"post\",padded_seqagain=train_again,return_paddedsequences=True)\n",
        "  return seq.shape[1]\n",
        "\n",
        "\n",
        "def aspectlayer_weightinit():\n",
        "  trained_weights=embedding_layerinit()[0]\n",
        "  kmeans = KMeans(n_clusters=aspects_k,random_state=0,max_iter=500,n_jobs=-1).fit(trained_weights) #clustering the trained wordembeddings into 10 clusters\n",
        "  init=tf.constant_initializer(kmeans.cluster_centers_) #used for initialing the weights of final dense layer\n",
        "  return init\n",
        "\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      #self.embed_outputdim=embed_outputdim\n",
        "      self.soft = tf.keras.layers.Softmax(axis=-2,name=\"softmax_att\") #softmax layer\n",
        "      self.dot=tf.keras.layers.Dot(axes=(-1,-1),name=\"dot_att\")       #dot layer\n",
        "      self.w = tf.Variable(\n",
        "            initial_value=tf.random_normal_initializer()(shape=(embed_outputdim,embed_outputdim), dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )              #weights that captures essense between the word embedding and global context vector(or average of all the word embedding of the sentence)\n",
        "  \n",
        "    def call(self,embed_output, mask=None):\n",
        "\n",
        "      ys = tf.reduce_mean(embed_output,axis=-2) #average of all word embeddings of the sentence\n",
        "      ys=tf.expand_dims(ys,axis=-2)\n",
        "      eW=tf.matmul(embed_output, self.w)\n",
        "      eW = eW * tf.expand_dims(tf.cast(mask,tf.float32),-1) #maskpropagation. Preventing masked elements into calculations\n",
        "      f=self.dot([eW, ys])\n",
        "      f = f+tf.expand_dims(tf.cast(tf.math.equal(mask, False), f.dtype)*-1e9,-1) #multiplying all the masked elements by -1e9 so that the softmax step do not impact the vectors\n",
        "      f=self.soft(f)\n",
        "      zs=tf.math.reduce_sum(f*embed_output,axis=-2) #zs is aspect embedding space after attention mechanisms on words of the sentence\n",
        "                                                    # f - softmax - gives info about unimportant words in the sentence for extracting aspects \n",
        "      \n",
        "      return zs,f\n",
        "\n",
        "\n",
        "# custom model\n",
        "\n",
        "class model_(tf.keras.Model):\n",
        "  def __init__(self,embed_outputdim,aspects_k):\n",
        "    super().__init__()\n",
        "    self.embed_inputdim=len(embedding_layerinit()[1].wv.vocab)\n",
        "    self.inputlength=inputlength()\n",
        "    self.embed_outputdim=embed_outputdim\n",
        "    self.trained_weights=embedding_layerinit()[0]\n",
        "    self.embedding=Embedding(input_dim=self.embed_inputdim+1,output_dim=self.embed_outputdim,mask_zero=True,\n",
        "                             input_length=self.inputlength,weights=[self.trained_weights],name=\"embedding_layer\",trainable=True) #embedding layer. Zero masking\n",
        "    self.attention=Attention()\n",
        "    self.aspects_k=aspects_k #number of aspects\n",
        "    self.init=aspectlayer_weightinit()\n",
        "    self.k = tf.keras.layers.Dense(aspects_k,name=\"dim_reduction_layer\",activation=\"softmax\")\n",
        "    self.dense= self.trained_weights.shape[1]\n",
        "    self.final=tf.keras.layers.Dense(self.dense,name=\"final_dense\",kernel_initializer=self.init) #weights are initialised with embedding clusters\n",
        "  def call(self,input):\n",
        "    e=self.embedding(input[0])\n",
        "    mask = self.embedding.compute_mask(input[0]) #computing mask so that this this can be used while propagating mask for subsequent layers\n",
        "    zs=self.attention(e, mask = mask)[0] #aspect vector\n",
        "    pt=self.k(zs)  #dimensionality vector\n",
        "    rs=self.final(pt) #reconstructed vector\n",
        "\n",
        "    # building regulariser to be used in the loss. [(T*transpose(T))-I]\n",
        "    reg=tf.tensordot(tf.linalg.normalize(self.final.weights[0],axis=1)[1],tf.linalg.normalize(self.final.weights[0],axis=1)[1],axes=[[1],[1]])\n",
        "    reg=reg-tf.ones([self.aspects_k,self.aspects_k])\n",
        "    reg=tf.norm(reg, ord='euclidean', axis=None, keepdims=None, name=None)\n",
        "    # calculating loss\n",
        "    r=tf.expand_dims(rs,-2)\n",
        "    f=tf.tensordot(rs,zs,[[0,1],[0,1]])\n",
        "    a=self.embedding(input[1])\n",
        "    a=tf.reduce_mean(a,axis=-2)\n",
        "    loss=tf.reduce_sum(tf.nn.relu(1-tf.reduce_sum(tf.tensordot(a,r,[[0,2],[0,2]]))+f))+1*reg #this is the loss which is to be minimised\n",
        "    return loss,pt,rs \n",
        "    \n",
        "    \n",
        "  def get_config(self):\n",
        "    config = {\n",
        "                  'embed_outputdim': self.embed_outputdim,\n",
        "                  'aspects_k' : self.aspects_k}\n",
        "    return config\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6JtRvyYcArt"
      },
      "source": [
        "#%%writefile /content/drive/MyDrive/REVIEWS/config.py\n",
        "from nltk.corpus import stopwords\n",
        "DATA_PATH=\"/content/drive/My Drive/REVIEWS/\"\n",
        "raw_file=\"reviews.pickle\"\n",
        "preprocessed_file=\"preprocessed_df.pickle\"\n",
        "trained_embeddings=\"trained_embeddings\"\n",
        "padded_seqfile=\"padded_sequences.pickle\"\n",
        "train_again=True\n",
        "vocabtrain_again=False\n",
        "stopword=stopwords.words(fileids=\"english\")\n",
        "embed_outputdim=100\n",
        "aspects_k=10 #number of aspects\n",
        "buffer_size=1024\n",
        "batch_size=100\n",
        "negative_samples=20\n",
        "WEIGHTS_PATH=\"/content/drive/My Drive/REVIEWS/Weights\"\n",
        "CHECKPOINTS_PATH=\"/content/drive/My Drive/REVIEWS/checkpoints\"\n",
        "lr=0.001\n",
        "iterations=15\n",
        "return_bestweightspath=True\n",
        "\n",
        "MODEL_CONFIG = {'embed_outputdim': embed_outputdim,\n",
        "                  'aspects_k' : aspects_k}\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuqiB0fqI0B-"
      },
      "source": [
        "#### Building tensorflow dataset from generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVmKcEt1SXWN"
      },
      "source": [
        "def gendata():\n",
        "    seed(42)\n",
        "    for i in range(0,len(seq_texts)):\n",
        "      lis=[]\n",
        "      lent=[]\n",
        "      while len(lent)<negative_samples:\n",
        "        value = randint(0, len(seq_texts)-1)\n",
        "        if value==i:\n",
        "          continue\n",
        "        lis.append(seq_texts[value])\n",
        "        lent.append(value)\n",
        "      yield seq_texts[i],lis\n",
        "dataset=tf.data.Dataset.from_generator(gendata, output_types=(tf.int32,tf.int32))\n",
        "dataset=dataset.repeat(1).shuffle(buffer_size=1024).batch(100).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72dSrdejULaE"
      },
      "source": [
        "dataset=generate_dataset(buffer_size=1024,batch_size=100,negative_samples=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dt9__VhhTSP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkrZfTM0I_S3"
      },
      "source": [
        "#### Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "BI7-IMtxE_vQ",
        "outputId": "c11cb4bb-7a0e-4d9d-cf1c-83f0f255301b"
      },
      "source": [
        "\n",
        "#%%writefile /content/drive/MyDrive/REVIEWS/training.py\n",
        "import emot\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from gensim.models import FastText \n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense,Concatenate,TimeDistributed,Masking,GRU,Input,Dot,Reshape,Softmax\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Using NLTK for PoS tagging and Stopwords removal\n",
        "# Downloading the corresponding packages\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from gensim.models import FastText \n",
        "from random import seed\n",
        "from random import randint\n",
        "from sklearn.cluster import KMeans\n",
        "#from config import *\n",
        "#from model import model_\n",
        "\n",
        "WEIGHTS_PATH=\"/content/drive/My Drive/REVIEWS/Weights/\"\n",
        "CHECKPOINTS_PATH=\"/content/drive/My Drive/REVIEWS/checkpoints\"\n",
        "\n",
        "lr=0.001\n",
        "iterations=1\n",
        "return_bestweightspath=True \n",
        "\n",
        "def training(WEIGHTS_PATH,CHECKPOINTS_PATH,model_,lr=lr,iterations=iterations,return_bestweightspath=return_bestweightspath):\n",
        "  abae=model_(embed_outputdim,aspects_k)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "  @tf.function\n",
        "  def train_step(input):\n",
        "      with tf.GradientTape() as tape:\n",
        "        loss = abae(input)[0]\n",
        "      gradients = tape.gradient(loss, abae.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(gradients, abae.trainable_variables))\n",
        "      return loss, gradients\n",
        "  train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "##check point to save\n",
        "  ckpt = tf.train.Checkpoint(optimizer=optimizer, model=abae)\n",
        "  ckpt_manager = tf.train.CheckpointManager(ckpt, CHECKPOINTS_PATH, max_to_keep=3)\n",
        "  \n",
        "  loss_list=[]\n",
        "  weights_pathlist=[]\n",
        "  for k in range(0,iterations): # k - number of iterations\n",
        "    counter = 0\n",
        "\n",
        "  # navigating through each batch\n",
        "    for input in dataset:\n",
        "      loss_, gradients = train_step(input)\n",
        "      #adding loss to train loss\n",
        "      train_loss(loss_)\n",
        "      counter = counter + 1\n",
        "      template = '''Done {} step, Loss: {:0.6f}'''\n",
        "      if counter%500==0:\n",
        "        print(template.format(counter, train_loss.result()))\n",
        "\n",
        "    loss_list.append(train_loss.result()) #appending loss after every epoch\n",
        "    ckpt_save_path  = ckpt_manager.save() #checkpointing after every epoch\n",
        "    X=os.path.join(WEIGHTS_PATH,\"weights_epoch_\"+str(k+1))\n",
        "    abae.save_weights(X,save_format=\"h5\")\n",
        "\n",
        "    weights_pathlist.append(X)\n",
        "    \n",
        "    print(\"weights saved after epoch {}\".format(k+1))\n",
        "    print ('Saving checkpoint for iteration {} at {}'.format(k+1, ckpt_save_path))\n",
        "    print(counter, train_loss.result())\n",
        "    train_loss.reset_states()             #resetting loss after every epoch\n",
        "  if return_bestweightspath:\n",
        "    argminimum=np.argmin(loss_list)\n",
        "    return weights_pathlist[argminimum]  \n",
        "  \n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f98eced0f76c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M00at2-AOZ-O"
      },
      "source": [
        "checkpoint_path=\"/content/drive/My Drive/REVIEWS/checkpoints\"\n",
        "abae=model_(embed_outputdim,aspects_k)\n",
        "latest = tf.train.latest_checkpoint(checkpoint_path)\n",
        "ckpt = tf.train.Checkpoint(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), model=abae)\n",
        "ckpt.restore(latest)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN_64DF0PfvC",
        "outputId": "d259176a-0bee-4dc1-ecfc-f5b4896028ba"
      },
      "source": [
        "model_.from_config(MODEL_CONFIG)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.model_ at 0x7fc4b6f72a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3PLKEdNNaKD"
      },
      "source": [
        "inf=model_.from_config(MODEL_CONFIG)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ceid_30DWhtt"
      },
      "source": [
        "for i in generate_dataset().take(2):\n",
        "  k=i"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_51mUR1Mo1Bw"
      },
      "source": [
        "inf(k)\n",
        "inf.load_weights(\"/content/drive/My Drive/REVIEWS/trainedWEIGHTS\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cpyirkTOc_pP",
        "outputId": "e783508b-e75a-4ab1-e395-013f534f0818"
      },
      "source": [
        "predict(k[0][1],\"/content/drive/My Drive/REVIEWS/trainedWEIGHTS\",model_,MODEL_CONFIG)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'belongs to topic_4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADo8mGN5d_M7"
      },
      "source": [
        "testsample=\"price is very reasonable\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3WG_LwIsgGN",
        "outputId": "16367994-f0c7-4c25-d26c-d4228afda073"
      },
      "source": [
        "  tkn=tokenisation_on_traindata(return_tkn=True)\n",
        "  word_countdict=tkn.word_counts \n",
        "  word_countdict.keys\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function OrderedDict.keys>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ikm9OiSd3dN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "257122a3-a0e3-412e-e088-52d3bb45473f"
      },
      "source": [
        "%%writefile /content/drive/MyDrive/REVIEWS/predict.py\n",
        "import emot\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from gensim.models import FastText \n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense,Concatenate,TimeDistributed,Masking,GRU,Input,Dot,Reshape,Softmax\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Using NLTK for PoS tagging and Stopwords removal\n",
        "# Downloading the corresponding packages\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from gensim.models import FastText \n",
        "from random import seed\n",
        "from random import randint\n",
        "from sklearn.cluster import KMeans\n",
        "from config import *\n",
        "from model import model_\n",
        "from preprocess import *\n",
        "from training import *\n",
        "\n",
        "\n",
        "\n",
        "def seq_gen(testsample,maxlen=223)  :\n",
        "  df=pd.DataFrame([testsample],columns=[\"review\"])\n",
        "  df[\"review\"]=df[\"review\"].apply(lambda x:x.strip(\"READ MORE\").lower())\n",
        "  df[\"review\"]=df[\"review\"].apply(convert_emojis)\n",
        "  df[\"review\"]=df[\"review\"].apply(decontractions)\n",
        "  df[\"review\"]=df[\"review\"].apply(removecharacters)\n",
        "  df[\"review\"]=pd.DataFrame(df[\"review\"].apply(lambda x:removestopwords(x,stopword)))\n",
        "  df[\"len\"]=df.review.str.split().apply(len)\n",
        "  df=df.loc[(df[\"len\"]>0) & (df[\"len\"]<maxlen+1)]\n",
        "  tkn=tokenisation_on_traindata(return_tkn=True)\n",
        "  word_countdict=tkn.word_counts  #word count dictionary\n",
        "  df[\"review\"]=pd.DataFrame(df[\"review\"].apply(lambda x:removenewords(x,word_countdict)))\n",
        "  df[\"review\"]=pd.DataFrame(df[\"review\"].apply(lambda x:word_count(x,word_countdict,10)))\n",
        "  \n",
        "  seq_texts=tkn.texts_to_sequences(df['review'].values)\n",
        "  print(seq_texts)\n",
        "  seq_texts=tf.keras.preprocessing.sequence.pad_sequences(seq_texts,\n",
        "                                                         maxlen=maxlen,\n",
        "                                                         padding='post')\n",
        "  return seq_texts\n",
        "\n",
        "def topicpredict(testsample,weight_path,model_,MODEL_CONFIG,maxlen=223):\n",
        "  k=tf.random.uniform(shape=[100,223],minval=1,maxval=38,dtype=tf.int32),tf.random.uniform(shape=[100,20,223],minval=1,maxval=38,dtype=tf.int32)\n",
        "  inf=model_.from_config(MODEL_CONFIG)\n",
        "  inf(k)\n",
        "  inf.load_weights(weight_path)\n",
        "  l=inf.layers\n",
        "  seq_text=seq_gen(testsample,maxlen=maxlen)\n",
        "  sample=seq_text.reshape(1,maxlen)\n",
        "  endsample=l[0](sample)\n",
        "  mask=l[0].compute_mask(sample)\n",
        "  att=l[1](endsample,mask)[0]  \n",
        "  topic_num=np.argmax(l[2](att))+1\n",
        "  return \"belongs to topic_\"+str(topic_num),l[2](att)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/drive/MyDrive/REVIEWS/predict.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQiocvnMmg-h"
      },
      "source": [
        "sample=k[0][1].numpy().reshape(1,223)\n",
        "emb=l[0](sample)\n",
        "mask=l[0].compute_mask(sample)\n",
        "att=l[1](emb,mask)[0]\n",
        "pt=l[2](att)\n",
        "pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "__O93C2aavn_",
        "outputId": "2be88d88-13cf-4f64-f847-506254f59461"
      },
      "source": [
        "topicpredict(\"gaming tho acha hai\",\"/content/drive/My Drive/REVIEWS/trainedWEIGHTS\",model_,MODEL_CONFIG,maxlen=223)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-0f66fd205693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtopicpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gaming tho acha hai\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/content/drive/My Drive/REVIEWS/trainedWEIGHTS\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMODEL_CONFIG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m223\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'topicpredict' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u2st5kyDJFX"
      },
      "source": [
        ""
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z527uyZUJKKM",
        "outputId": "94a7139a-3d61-4210-ab54-9e28254aad0f"
      },
      "source": [
        "l=inf.layers\n",
        "l"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.layers.embeddings.Embedding at 0x7fc4b6f84f50>,\n",
              " <__main__.Attention at 0x7fc4b6f84e90>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7fc4d0d2fad0>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7fc4c625c750>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW8vBigGBM1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cbf3ca5-b2fc-4276-952e-c54feb3204ef"
      },
      "source": [
        "sample=k[0][1].numpy().reshape(1,223)\n",
        "emb=l[0](sample)\n",
        "mask=l[0].compute_mask(sample)\n",
        "att=l[1](emb,mask)[0]\n",
        "pt=l[2](att)\n",
        "pt"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
              "array([[0.07538455, 0.11304139, 0.10993138, 0.104953  , 0.09605172,\n",
              "        0.1072276 , 0.0917433 , 0.08850341, 0.12209129, 0.09107236]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LwktobgByfN",
        "outputId": "9e82e573-6e97-4002-81bb-1d0fb3cbfc48"
      },
      "source": [
        "emdsample=inf.layers[0](sample)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 223, 100), dtype=float32, numpy=\n",
              "array([[[ 0.28434345,  0.3539412 , -0.03884524, ...,  0.08158847,\n",
              "          0.06795241, -0.03691028],\n",
              "        [ 0.09312745,  0.0665113 , -0.01089542, ...,  0.01330447,\n",
              "         -0.25797328,  0.03294387],\n",
              "        [-0.2916876 , -0.02874304,  0.01857883, ...,  0.14095038,\n",
              "         -0.0856531 , -0.15893033],\n",
              "        ...,\n",
              "        [ 0.01035531,  0.01036448,  0.01037198, ...,  0.01034254,\n",
              "         -0.01036166,  0.01037487],\n",
              "        [ 0.01035531,  0.01036448,  0.01037198, ...,  0.01034254,\n",
              "         -0.01036166,  0.01037487],\n",
              "        [ 0.01035531,  0.01036448,  0.01037198, ...,  0.01034254,\n",
              "         -0.01036166,  0.01037487]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5__llp90Y7hQ",
        "outputId": "7e83915c-6d16-46c5-9658-501a63a8e229"
      },
      "source": [
        "att=inf.layers[1](emdsample)[0]\n",
        "att"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 100), dtype=float32, numpy=\n",
              "array([[-0.03784817,  0.04932524,  0.00909207, -0.03855411,  0.03189919,\n",
              "        -0.05768111,  0.28769803, -0.0539141 , -0.2119304 ,  0.13761231,\n",
              "        -0.19599375,  0.15078266,  0.12766425, -0.15458861,  0.05844464,\n",
              "         0.00605304,  0.08107603, -0.13156097, -0.0095018 ,  0.07236446,\n",
              "         0.0761936 ,  0.00411951, -0.18544461, -0.04545902, -0.0270914 ,\n",
              "         0.13593271, -0.22507998,  0.06798376, -0.13280551,  0.01041025,\n",
              "         0.05213068,  0.17390668,  0.202806  ,  0.14431584,  0.14312342,\n",
              "         0.1353922 ,  0.02476707,  0.06251456,  0.12323368, -0.00152758,\n",
              "         0.03922736, -0.18883106,  0.13550267,  0.12107454, -0.04419479,\n",
              "         0.2056097 ,  0.01607932, -0.14294998, -0.0288608 ,  0.11756141,\n",
              "         0.01950871, -0.14338823,  0.07907163, -0.14048326, -0.01465124,\n",
              "         0.0114028 , -0.1232634 , -0.13779134, -0.05030006,  0.05148183,\n",
              "         0.00965279, -0.04693422,  0.05502408, -0.14524607,  0.02292041,\n",
              "        -0.16702779, -0.09032408,  0.02720484,  0.30139408, -0.11645963,\n",
              "        -0.2458573 , -0.38309753, -0.09785175,  0.00813603,  0.25034863,\n",
              "         0.0425318 , -0.02425337,  0.01404067, -0.01353701, -0.33607188,\n",
              "         0.17981254, -0.11140373, -0.03443918,  0.0337606 , -0.05809933,\n",
              "         0.07881187,  0.01351597, -0.1311582 ,  0.16282997,  0.09425178,\n",
              "        -0.23753943, -0.02256316, -0.09242329, -0.03884177,  0.07662644,\n",
              "        -0.03446334, -0.03226683,  0.11157088, -0.1434564 ,  0.03651715]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVg3CrzZahp0",
        "outputId": "6056a9cf-3299-4888-b5ef-83a83ba13c99"
      },
      "source": [
        "inf.layers[2](att)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
              "array([[0.0852352 , 0.13147014, 0.0958802 , 0.11055278, 0.08819757,\n",
              "        0.09580411, 0.08448124, 0.09142829, 0.11823527, 0.09871526]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DUy4oWHB7Ao"
      },
      "source": [
        "test.load_weights(os.path.join(DATA_PATH,\"_newWeights\",str(1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9JNpuaOCCp1",
        "outputId": "1469b732-12b0-4a44-e2fa-0da04410de6a"
      },
      "source": [
        "test(k)[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.010878837>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yjw5lqFKi7G"
      },
      "source": [
        "abae.save_weights(os.path.join(DATA_PATH,\"_newWeights\"),save_format=\"h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwLfrjc3JFkU"
      },
      "source": [
        "#### Loss - plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "IPG-sV2kM6HB",
        "outputId": "04183a83-4062-4147-988d-efdc2300cf2f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(loss_list[1:])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dU9_GOC5W8P",
        "outputId": "ecebe94a-bf77-4bd6-e4b4-efc440c4465e"
      },
      "source": [
        "abae.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model__1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_layer (Embedding)  multiple                  699800    \n",
            "_________________________________________________________________\n",
            "attention_1 (Attention)      multiple                  10000     \n",
            "_________________________________________________________________\n",
            "dim_reduction_layer (Dense)  multiple                  1010      \n",
            "_________________________________________________________________\n",
            "final_dense (Dense)          multiple                  1100      \n",
            "=================================================================\n",
            "Total params: 711,910\n",
            "Trainable params: 711,910\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZL3E3W4M5FT",
        "outputId": "b5a71cc9-86e0-42cb-b6b3-4902279d0e30"
      },
      "source": [
        "for i in abae.weights:\n",
        "  print(i.name, i.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model__1/embedding_layer/embeddings:0 (6998, 100)\n",
            "Variable:0 (100, 100)\n",
            "model__1/dim_reduction_layer/kernel:0 (100, 10)\n",
            "model__1/dim_reduction_layer/bias:0 (10,)\n",
            "model__1/final_dense/kernel:0 (10, 100)\n",
            "model__1/final_dense/bias:0 (100,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2YfYl7XYhm9"
      },
      "source": [
        "from gensim.models import FastText\n",
        "model=FastText.load(\"/content/drive/My Drive/REVIEWS/trained_embeddings\") #reusing already trained model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_DQk1GL4ljg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37650396-0064-4182-aa3f-2af47eeedc38"
      },
      "source": [
        "!python \"/content/drive/MyDrive/REVIEWS/training.py\""
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-16 05:21:36.256373: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx5y77bAUJto",
        "outputId": "bbcf1f0a-282e-467c-9067-0f61e9aa699a"
      },
      "source": [
        "!python \"/content/drive/MyDrive/REVIEWS/predict.py\""
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-16 05:33:52.046366: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Camera is awesome\n",
            "2021-08-16 05:34:06.770771: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-08-16 05:34:06.783645: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-08-16 05:34:06.783697: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (74ab1ff4eadb): /proc/driver/nvidia/version does not exist\n",
            "2021-08-16 05:35:00.570096: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /content/drive/My Drive/REVIEWS/trainedWEIGHTS: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n",
            "[[4, 13]]\n",
            "('belongs to topic_2', <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
            "array([[0.07675852, 0.12149984, 0.10254771, 0.10284691, 0.09783805,\n",
            "        0.11666577, 0.09667478, 0.08279258, 0.1197534 , 0.08262251]],\n",
            "      dtype=float32)>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "TDdfM6jYbuOf",
        "outputId": "908c1f0d-aeb7-4dd1-8821-dc59e45a3ae1"
      },
      "source": [
        "topics=inf.weights[4].numpy()  \n",
        "topic_words={}\n",
        "k=1\n",
        "for i in range(len(topics)):\n",
        "  topic_words[\"topic_\"+str(k)]=np.argsort(model.wv.cosine_similarities(topics[i],model.wv.vectors))[::-1][0:20]\n",
        "  k=k+1\n",
        "pd.DataFrame(topic_words).applymap(lambda x:model.wv.index2word[x])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic_1</th>\n",
              "      <th>topic_2</th>\n",
              "      <th>topic_3</th>\n",
              "      <th>topic_4</th>\n",
              "      <th>topic_5</th>\n",
              "      <th>topic_6</th>\n",
              "      <th>topic_7</th>\n",
              "      <th>topic_8</th>\n",
              "      <th>topic_9</th>\n",
              "      <th>topic_10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rs</td>\n",
              "      <td>played</td>\n",
              "      <td>cameras</td>\n",
              "      <td>notifications</td>\n",
              "      <td>nice</td>\n",
              "      <td>far</td>\n",
              "      <td>body</td>\n",
              "      <td>ke</td>\n",
              "      <td>cancel</td>\n",
              "      <td>s9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>offer</td>\n",
              "      <td>games</td>\n",
              "      <td>depth</td>\n",
              "      <td>app</td>\n",
              "      <td>xilent</td>\n",
              "      <td>think</td>\n",
              "      <td>case</td>\n",
              "      <td>hai</td>\n",
              "      <td>deliver</td>\n",
              "      <td>obviously</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>discount</td>\n",
              "      <td>graphics</td>\n",
              "      <td>images</td>\n",
              "      <td>exit</td>\n",
              "      <td>osm</td>\n",
              "      <td>comparison</td>\n",
              "      <td>thin</td>\n",
              "      <td>liye</td>\n",
              "      <td>delivered</td>\n",
              "      <td>terms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>discounts</td>\n",
              "      <td>runs</td>\n",
              "      <td>portrait</td>\n",
              "      <td>messages</td>\n",
              "      <td>vry</td>\n",
              "      <td>really</td>\n",
              "      <td>design</td>\n",
              "      <td>bhi</td>\n",
              "      <td>order</td>\n",
              "      <td>absolutely</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>price</td>\n",
              "      <td>almost</td>\n",
              "      <td>capture</td>\n",
              "      <td>operations</td>\n",
              "      <td>nyc</td>\n",
              "      <td>aspects</td>\n",
              "      <td>fits</td>\n",
              "      <td>h</td>\n",
              "      <td>informed</td>\n",
              "      <td>s8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>deal</td>\n",
              "      <td>90</td>\n",
              "      <td>captures</td>\n",
              "      <td>apps</td>\n",
              "      <td>sm</td>\n",
              "      <td>actually</td>\n",
              "      <td>grip</td>\n",
              "      <td>hota</td>\n",
              "      <td>address</td>\n",
              "      <td>beast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>discounted</td>\n",
              "      <td>heavy</td>\n",
              "      <td>daylight</td>\n",
              "      <td>application</td>\n",
              "      <td>lovly</td>\n",
              "      <td>trust</td>\n",
              "      <td>slippery</td>\n",
              "      <td>acha</td>\n",
              "      <td>promised</td>\n",
              "      <td>flagships</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>12000</td>\n",
              "      <td>playing</td>\n",
              "      <td>lens</td>\n",
              "      <td>restart</td>\n",
              "      <td>good</td>\n",
              "      <td>one</td>\n",
              "      <td>feels</td>\n",
              "      <td>ki</td>\n",
              "      <td>assured</td>\n",
              "      <td>flagship</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>999</td>\n",
              "      <td>gaming</td>\n",
              "      <td>rear</td>\n",
              "      <td>applications</td>\n",
              "      <td>nais</td>\n",
              "      <td>better</td>\n",
              "      <td>silicon</td>\n",
              "      <td>nhi</td>\n",
              "      <td>asked</td>\n",
              "      <td>barring</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>13000</td>\n",
              "      <td>pubg</td>\n",
              "      <td>wide</td>\n",
              "      <td>whatsapp</td>\n",
              "      <td>super</td>\n",
              "      <td>believe</td>\n",
              "      <td>looks</td>\n",
              "      <td>accha</td>\n",
              "      <td>customer</td>\n",
              "      <td>amazing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>7000</td>\n",
              "      <td>handles</td>\n",
              "      <td>camera</td>\n",
              "      <td>able</td>\n",
              "      <td>swm</td>\n",
              "      <td>compared</td>\n",
              "      <td>handy</td>\n",
              "      <td>jyada</td>\n",
              "      <td>delivery</td>\n",
              "      <td>iphones</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>19000</td>\n",
              "      <td>extreme</td>\n",
              "      <td>front</td>\n",
              "      <td>browsing</td>\n",
              "      <td>gjb</td>\n",
              "      <td>still</td>\n",
              "      <td>slim</td>\n",
              "      <td>ye</td>\n",
              "      <td>cancelled</td>\n",
              "      <td>s10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>billion</td>\n",
              "      <td>medium</td>\n",
              "      <td>pictures</td>\n",
              "      <td>music</td>\n",
              "      <td>profomance</td>\n",
              "      <td>compare</td>\n",
              "      <td>plastic</td>\n",
              "      <td>kam</td>\n",
              "      <td>executives</td>\n",
              "      <td>shifted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>8100</td>\n",
              "      <td>50</td>\n",
              "      <td>shots</td>\n",
              "      <td>function</td>\n",
              "      <td>osom</td>\n",
              "      <td>definitely</td>\n",
              "      <td>cover</td>\n",
              "      <td>bahut</td>\n",
              "      <td>said</td>\n",
              "      <td>loving</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5500</td>\n",
              "      <td>100</td>\n",
              "      <td>aperture</td>\n",
              "      <td>icon</td>\n",
              "      <td>awesome</td>\n",
              "      <td>terms</td>\n",
              "      <td>glass</td>\n",
              "      <td>nahi</td>\n",
              "      <td>bangalore</td>\n",
              "      <td>absolute</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>diwali</td>\n",
              "      <td>lasts</td>\n",
              "      <td>angle</td>\n",
              "      <td>menu</td>\n",
              "      <td>also</td>\n",
              "      <td>plus</td>\n",
              "      <td>curved</td>\n",
              "      <td>ka</td>\n",
              "      <td>ekart</td>\n",
              "      <td>great</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>offers</td>\n",
              "      <td>combat</td>\n",
              "      <td>captured</td>\n",
              "      <td>sometimes</td>\n",
              "      <td>bt</td>\n",
              "      <td>phones</td>\n",
              "      <td>hands</td>\n",
              "      <td>jada</td>\n",
              "      <td>date</td>\n",
              "      <td>aspects</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18999</td>\n",
              "      <td>lag</td>\n",
              "      <td>selfie</td>\n",
              "      <td>browser</td>\n",
              "      <td>betray</td>\n",
              "      <td>awesome</td>\n",
              "      <td>thick</td>\n",
              "      <td>lekin</td>\n",
              "      <td>seller</td>\n",
              "      <td>way</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>rupees</td>\n",
              "      <td>usage</td>\n",
              "      <td>produces</td>\n",
              "      <td>wifi</td>\n",
              "      <td>love</td>\n",
              "      <td>comparing</td>\n",
              "      <td>bigger</td>\n",
              "      <td>mai</td>\n",
              "      <td>sent</td>\n",
              "      <td>top</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>competitive</td>\n",
              "      <td>smooth</td>\n",
              "      <td>saturated</td>\n",
              "      <td>disable</td>\n",
              "      <td>nd</td>\n",
              "      <td>great</td>\n",
              "      <td>beautiful</td>\n",
              "      <td>ko</td>\n",
              "      <td>courier</td>\n",
              "      <td>oneplus</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        topic_1   topic_2    topic_3  ... topic_8     topic_9    topic_10\n",
              "0            rs    played    cameras  ...      ke      cancel          s9\n",
              "1         offer     games      depth  ...     hai     deliver   obviously\n",
              "2      discount  graphics     images  ...    liye   delivered       terms\n",
              "3     discounts      runs   portrait  ...     bhi       order  absolutely\n",
              "4         price    almost    capture  ...       h    informed          s8\n",
              "5          deal        90   captures  ...    hota     address       beast\n",
              "6    discounted     heavy   daylight  ...    acha    promised   flagships\n",
              "7         12000   playing       lens  ...      ki     assured    flagship\n",
              "8           999    gaming       rear  ...     nhi       asked     barring\n",
              "9         13000      pubg       wide  ...   accha    customer     amazing\n",
              "10         7000   handles     camera  ...   jyada    delivery     iphones\n",
              "11        19000   extreme      front  ...      ye   cancelled         s10\n",
              "12      billion    medium   pictures  ...     kam  executives     shifted\n",
              "13         8100        50      shots  ...   bahut        said      loving\n",
              "14         5500       100   aperture  ...    nahi   bangalore    absolute\n",
              "15       diwali     lasts      angle  ...      ka       ekart       great\n",
              "16       offers    combat   captured  ...    jada        date     aspects\n",
              "17        18999       lag     selfie  ...   lekin      seller         way\n",
              "18       rupees     usage   produces  ...     mai        sent         top\n",
              "19  competitive    smooth  saturated  ...      ko     courier     oneplus\n",
              "\n",
              "[20 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq3txHX1yb3_"
      },
      "source": [
        "import json\n",
        "topics_inf={}\n",
        "topics_inf[1]=\"price and sale\"\n",
        "topics_inf[2]=\"gaming experience\"\n",
        "topics_inf[3]=\"camera and pictures\"\n",
        "topics_inf[4]=\"apps and notifications\"\n",
        "topics_inf[5]=\"misspelt and positive words\"\n",
        "topics_inf[6]=\"coparision\"\n",
        "topics_inf[7]=\"physical design and experience\"\n",
        "topics_inf[8]=\"hindi word\"\n",
        "topics_inf[9]=\"delivery\"\n",
        "topics_inf[10]=\"premium phones\"\n",
        "with open(\"/content/drive/My Drive/REVIEWS/topics_inf.json\",\"w+\") as file:\n",
        "  json.dump(topics_inf,file)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JieqInGrydsb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "fldSm4OyyYri",
        "outputId": "7ddb0eba-eae4-4d3b-ae8b-0ccb0a658f87"
      },
      "source": [
        "topics=inf.weights[4].numpy()  \n",
        "topic_words={}\n",
        "k=1\n",
        "for i in range(len(topics)):\n",
        "  topic_words[\"topic_\"+str(k)]=np.argsort(model.wv.cosine_similarities(topics[i],model.wv.vectors))[::-1][0:20]\n",
        "  k=k+1\n",
        "pd.DataFrame(topic_words).applymap(lambda x:model.wv.index2word[x])  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic_1</th>\n",
              "      <th>topic_2</th>\n",
              "      <th>topic_3</th>\n",
              "      <th>topic_4</th>\n",
              "      <th>topic_5</th>\n",
              "      <th>topic_6</th>\n",
              "      <th>topic_7</th>\n",
              "      <th>topic_8</th>\n",
              "      <th>topic_9</th>\n",
              "      <th>topic_10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rs</td>\n",
              "      <td>played</td>\n",
              "      <td>cameras</td>\n",
              "      <td>notifications</td>\n",
              "      <td>nice</td>\n",
              "      <td>far</td>\n",
              "      <td>body</td>\n",
              "      <td>ke</td>\n",
              "      <td>cancel</td>\n",
              "      <td>s9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>offer</td>\n",
              "      <td>games</td>\n",
              "      <td>depth</td>\n",
              "      <td>app</td>\n",
              "      <td>xilent</td>\n",
              "      <td>think</td>\n",
              "      <td>case</td>\n",
              "      <td>hai</td>\n",
              "      <td>deliver</td>\n",
              "      <td>obviously</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>discount</td>\n",
              "      <td>graphics</td>\n",
              "      <td>images</td>\n",
              "      <td>exit</td>\n",
              "      <td>osm</td>\n",
              "      <td>comparison</td>\n",
              "      <td>thin</td>\n",
              "      <td>liye</td>\n",
              "      <td>delivered</td>\n",
              "      <td>terms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>discounts</td>\n",
              "      <td>runs</td>\n",
              "      <td>portrait</td>\n",
              "      <td>messages</td>\n",
              "      <td>vry</td>\n",
              "      <td>really</td>\n",
              "      <td>design</td>\n",
              "      <td>bhi</td>\n",
              "      <td>order</td>\n",
              "      <td>absolutely</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>price</td>\n",
              "      <td>almost</td>\n",
              "      <td>capture</td>\n",
              "      <td>operations</td>\n",
              "      <td>nyc</td>\n",
              "      <td>aspects</td>\n",
              "      <td>fits</td>\n",
              "      <td>h</td>\n",
              "      <td>informed</td>\n",
              "      <td>s8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>deal</td>\n",
              "      <td>90</td>\n",
              "      <td>captures</td>\n",
              "      <td>apps</td>\n",
              "      <td>sm</td>\n",
              "      <td>actually</td>\n",
              "      <td>grip</td>\n",
              "      <td>hota</td>\n",
              "      <td>address</td>\n",
              "      <td>beast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>discounted</td>\n",
              "      <td>heavy</td>\n",
              "      <td>daylight</td>\n",
              "      <td>application</td>\n",
              "      <td>lovly</td>\n",
              "      <td>trust</td>\n",
              "      <td>slippery</td>\n",
              "      <td>acha</td>\n",
              "      <td>promised</td>\n",
              "      <td>flagships</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>12000</td>\n",
              "      <td>playing</td>\n",
              "      <td>lens</td>\n",
              "      <td>restart</td>\n",
              "      <td>good</td>\n",
              "      <td>one</td>\n",
              "      <td>feels</td>\n",
              "      <td>ki</td>\n",
              "      <td>assured</td>\n",
              "      <td>flagship</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>999</td>\n",
              "      <td>gaming</td>\n",
              "      <td>rear</td>\n",
              "      <td>applications</td>\n",
              "      <td>nais</td>\n",
              "      <td>better</td>\n",
              "      <td>silicon</td>\n",
              "      <td>nhi</td>\n",
              "      <td>asked</td>\n",
              "      <td>barring</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>13000</td>\n",
              "      <td>pubg</td>\n",
              "      <td>wide</td>\n",
              "      <td>whatsapp</td>\n",
              "      <td>super</td>\n",
              "      <td>believe</td>\n",
              "      <td>looks</td>\n",
              "      <td>accha</td>\n",
              "      <td>customer</td>\n",
              "      <td>amazing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>7000</td>\n",
              "      <td>handles</td>\n",
              "      <td>camera</td>\n",
              "      <td>able</td>\n",
              "      <td>swm</td>\n",
              "      <td>compared</td>\n",
              "      <td>handy</td>\n",
              "      <td>jyada</td>\n",
              "      <td>delivery</td>\n",
              "      <td>iphones</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>19000</td>\n",
              "      <td>extreme</td>\n",
              "      <td>front</td>\n",
              "      <td>browsing</td>\n",
              "      <td>gjb</td>\n",
              "      <td>still</td>\n",
              "      <td>slim</td>\n",
              "      <td>ye</td>\n",
              "      <td>cancelled</td>\n",
              "      <td>s10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>billion</td>\n",
              "      <td>medium</td>\n",
              "      <td>pictures</td>\n",
              "      <td>music</td>\n",
              "      <td>profomance</td>\n",
              "      <td>compare</td>\n",
              "      <td>plastic</td>\n",
              "      <td>kam</td>\n",
              "      <td>executives</td>\n",
              "      <td>shifted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>8100</td>\n",
              "      <td>50</td>\n",
              "      <td>shots</td>\n",
              "      <td>function</td>\n",
              "      <td>osom</td>\n",
              "      <td>definitely</td>\n",
              "      <td>cover</td>\n",
              "      <td>bahut</td>\n",
              "      <td>said</td>\n",
              "      <td>loving</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5500</td>\n",
              "      <td>100</td>\n",
              "      <td>aperture</td>\n",
              "      <td>icon</td>\n",
              "      <td>awesome</td>\n",
              "      <td>terms</td>\n",
              "      <td>glass</td>\n",
              "      <td>nahi</td>\n",
              "      <td>bangalore</td>\n",
              "      <td>absolute</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>diwali</td>\n",
              "      <td>lasts</td>\n",
              "      <td>angle</td>\n",
              "      <td>menu</td>\n",
              "      <td>also</td>\n",
              "      <td>plus</td>\n",
              "      <td>curved</td>\n",
              "      <td>ka</td>\n",
              "      <td>ekart</td>\n",
              "      <td>great</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>offers</td>\n",
              "      <td>combat</td>\n",
              "      <td>captured</td>\n",
              "      <td>sometimes</td>\n",
              "      <td>bt</td>\n",
              "      <td>phones</td>\n",
              "      <td>hands</td>\n",
              "      <td>jada</td>\n",
              "      <td>date</td>\n",
              "      <td>aspects</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18999</td>\n",
              "      <td>lag</td>\n",
              "      <td>selfie</td>\n",
              "      <td>browser</td>\n",
              "      <td>betray</td>\n",
              "      <td>awesome</td>\n",
              "      <td>thick</td>\n",
              "      <td>lekin</td>\n",
              "      <td>seller</td>\n",
              "      <td>way</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>rupees</td>\n",
              "      <td>usage</td>\n",
              "      <td>produces</td>\n",
              "      <td>wifi</td>\n",
              "      <td>love</td>\n",
              "      <td>comparing</td>\n",
              "      <td>bigger</td>\n",
              "      <td>mai</td>\n",
              "      <td>sent</td>\n",
              "      <td>top</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>competitive</td>\n",
              "      <td>smooth</td>\n",
              "      <td>saturated</td>\n",
              "      <td>disable</td>\n",
              "      <td>nd</td>\n",
              "      <td>great</td>\n",
              "      <td>beautiful</td>\n",
              "      <td>ko</td>\n",
              "      <td>courier</td>\n",
              "      <td>oneplus</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        topic_1   topic_2    topic_3  ... topic_8     topic_9    topic_10\n",
              "0            rs    played    cameras  ...      ke      cancel          s9\n",
              "1         offer     games      depth  ...     hai     deliver   obviously\n",
              "2      discount  graphics     images  ...    liye   delivered       terms\n",
              "3     discounts      runs   portrait  ...     bhi       order  absolutely\n",
              "4         price    almost    capture  ...       h    informed          s8\n",
              "5          deal        90   captures  ...    hota     address       beast\n",
              "6    discounted     heavy   daylight  ...    acha    promised   flagships\n",
              "7         12000   playing       lens  ...      ki     assured    flagship\n",
              "8           999    gaming       rear  ...     nhi       asked     barring\n",
              "9         13000      pubg       wide  ...   accha    customer     amazing\n",
              "10         7000   handles     camera  ...   jyada    delivery     iphones\n",
              "11        19000   extreme      front  ...      ye   cancelled         s10\n",
              "12      billion    medium   pictures  ...     kam  executives     shifted\n",
              "13         8100        50      shots  ...   bahut        said      loving\n",
              "14         5500       100   aperture  ...    nahi   bangalore    absolute\n",
              "15       diwali     lasts      angle  ...      ka       ekart       great\n",
              "16       offers    combat   captured  ...    jada        date     aspects\n",
              "17        18999       lag     selfie  ...   lekin      seller         way\n",
              "18       rupees     usage   produces  ...     mai        sent         top\n",
              "19  competitive    smooth  saturated  ...      ko     courier     oneplus\n",
              "\n",
              "[20 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9H2nJKnJQeO"
      },
      "source": [
        "#### Aspects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "6muA-gX94FNw",
        "outputId": "ba0a4e54-c217-4b58-cc50-2ecb203e7ec2"
      },
      "source": [
        "pd.DataFrame(topic_words).applymap(lambda x:model.wv.index2word[x])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic_1</th>\n",
              "      <th>topic_2</th>\n",
              "      <th>topic_3</th>\n",
              "      <th>topic_4</th>\n",
              "      <th>topic_5</th>\n",
              "      <th>topic_6</th>\n",
              "      <th>topic_7</th>\n",
              "      <th>topic_8</th>\n",
              "      <th>topic_9</th>\n",
              "      <th>topic_10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rs</td>\n",
              "      <td>amoled</td>\n",
              "      <td>deliver</td>\n",
              "      <td>awesome</td>\n",
              "      <td>feels</td>\n",
              "      <td>achha</td>\n",
              "      <td>turn</td>\n",
              "      <td>stunning</td>\n",
              "      <td>nice</td>\n",
              "      <td>fraud</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>billion</td>\n",
              "      <td>stunning</td>\n",
              "      <td>shipping</td>\n",
              "      <td>really</td>\n",
              "      <td>classy</td>\n",
              "      <td>koi</td>\n",
              "      <td>notifications</td>\n",
              "      <td>disappoint</td>\n",
              "      <td>good</td>\n",
              "      <td>seller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>discount</td>\n",
              "      <td>amazing</td>\n",
              "      <td>ekart</td>\n",
              "      <td>phone</td>\n",
              "      <td>grip</td>\n",
              "      <td>mast</td>\n",
              "      <td>app</td>\n",
              "      <td>great</td>\n",
              "      <td>nicr</td>\n",
              "      <td>refused</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>offer</td>\n",
              "      <td>cameras</td>\n",
              "      <td>30th</td>\n",
              "      <td>good</td>\n",
              "      <td>sleek</td>\n",
              "      <td>hai</td>\n",
              "      <td>disable</td>\n",
              "      <td>awesome</td>\n",
              "      <td>awesome</td>\n",
              "      <td>fault</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3500</td>\n",
              "      <td>awesome</td>\n",
              "      <td>ordered</td>\n",
              "      <td>superb</td>\n",
              "      <td>premium</td>\n",
              "      <td>ko</td>\n",
              "      <td>access</td>\n",
              "      <td>really</td>\n",
              "      <td>wesome</td>\n",
              "      <td>sent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>discounts</td>\n",
              "      <td>brilliant</td>\n",
              "      <td>delivered</td>\n",
              "      <td>also</td>\n",
              "      <td>design</td>\n",
              "      <td>acha</td>\n",
              "      <td>application</td>\n",
              "      <td>amazing</td>\n",
              "      <td>greate</td>\n",
              "      <td>executives</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>13500</td>\n",
              "      <td>outstanding</td>\n",
              "      <td>delivery</td>\n",
              "      <td>one</td>\n",
              "      <td>looks</td>\n",
              "      <td>nhi</td>\n",
              "      <td>vibrate</td>\n",
              "      <td>indeed</td>\n",
              "      <td>thanku</td>\n",
              "      <td>accepting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3k</td>\n",
              "      <td>ai</td>\n",
              "      <td>flipkart</td>\n",
              "      <td>great</td>\n",
              "      <td>handy</td>\n",
              "      <td>kharab</td>\n",
              "      <td>irritates</td>\n",
              "      <td>beast</td>\n",
              "      <td>avarge</td>\n",
              "      <td>accept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>13000</td>\n",
              "      <td>decent</td>\n",
              "      <td>saturday</td>\n",
              "      <td>excellent</td>\n",
              "      <td>hold</td>\n",
              "      <td>ek</td>\n",
              "      <td>bluetooth</td>\n",
              "      <td>loving</td>\n",
              "      <td>mast</td>\n",
              "      <td>replacement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>8999</td>\n",
              "      <td>really</td>\n",
              "      <td>oct</td>\n",
              "      <td>battery</td>\n",
              "      <td>slippery</td>\n",
              "      <td>ki</td>\n",
              "      <td>button</td>\n",
              "      <td>outstanding</td>\n",
              "      <td>fantastic</td>\n",
              "      <td>flipkart</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>5k</td>\n",
              "      <td>especially</td>\n",
              "      <td>seller</td>\n",
              "      <td>amazing</td>\n",
              "      <td>sturdy</td>\n",
              "      <td>mai</td>\n",
              "      <td>disabled</td>\n",
              "      <td>truly</td>\n",
              "      <td>wsome</td>\n",
              "      <td>electronics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12000</td>\n",
              "      <td>great</td>\n",
              "      <td>order</td>\n",
              "      <td>fantastic</td>\n",
              "      <td>compact</td>\n",
              "      <td>kam</td>\n",
              "      <td>launcher</td>\n",
              "      <td>incredible</td>\n",
              "      <td>osm</td>\n",
              "      <td>send</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12999</td>\n",
              "      <td>natural</td>\n",
              "      <td>august</td>\n",
              "      <td>actually</td>\n",
              "      <td>stunning</td>\n",
              "      <td>bhi</td>\n",
              "      <td>icon</td>\n",
              "      <td>classy</td>\n",
              "      <td>delevry</td>\n",
              "      <td>customers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2999</td>\n",
              "      <td>makes</td>\n",
              "      <td>12th</td>\n",
              "      <td>design</td>\n",
              "      <td>hands</td>\n",
              "      <td>accha</td>\n",
              "      <td>rings</td>\n",
              "      <td>premium</td>\n",
              "      <td>vry</td>\n",
              "      <td>request</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>11700</td>\n",
              "      <td>stereo</td>\n",
              "      <td>packing</td>\n",
              "      <td>ok</td>\n",
              "      <td>pretty</td>\n",
              "      <td>ye</td>\n",
              "      <td>scroll</td>\n",
              "      <td>killer</td>\n",
              "      <td>also</td>\n",
              "      <td>saying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>8000</td>\n",
              "      <td>crisp</td>\n",
              "      <td>launch</td>\n",
              "      <td>mobile</td>\n",
              "      <td>hand</td>\n",
              "      <td>lekin</td>\n",
              "      <td>notification</td>\n",
              "      <td>beat</td>\n",
              "      <td>exelent</td>\n",
              "      <td>rejected</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>billions</td>\n",
              "      <td>captures</td>\n",
              "      <td>booked</td>\n",
              "      <td>outstanding</td>\n",
              "      <td>metallic</td>\n",
              "      <td>bahut</td>\n",
              "      <td>silent</td>\n",
              "      <td>terms</td>\n",
              "      <td>osom</td>\n",
              "      <td>staff</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>8k</td>\n",
              "      <td>specially</td>\n",
              "      <td>logistics</td>\n",
              "      <td>decent</td>\n",
              "      <td>glossy</td>\n",
              "      <td>ka</td>\n",
              "      <td>shortcut</td>\n",
              "      <td>brilliant</td>\n",
              "      <td>fabulous</td>\n",
              "      <td>genuine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>10000</td>\n",
              "      <td>camera</td>\n",
              "      <td>22nd</td>\n",
              "      <td>disappointed</td>\n",
              "      <td>elegant</td>\n",
              "      <td>kiya</td>\n",
              "      <td>apps</td>\n",
              "      <td>flagship</td>\n",
              "      <td>bettery</td>\n",
              "      <td>said</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>9000</td>\n",
              "      <td>display</td>\n",
              "      <td>dilevry</td>\n",
              "      <td>think</td>\n",
              "      <td>build</td>\n",
              "      <td>nahi</td>\n",
              "      <td>wifi</td>\n",
              "      <td>decent</td>\n",
              "      <td>delevery</td>\n",
              "      <td>return</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      topic_1      topic_2    topic_3  ...      topic_8    topic_9     topic_10\n",
              "0          rs       amoled    deliver  ...     stunning       nice        fraud\n",
              "1     billion     stunning   shipping  ...   disappoint       good       seller\n",
              "2    discount      amazing      ekart  ...        great       nicr      refused\n",
              "3       offer      cameras       30th  ...      awesome    awesome        fault\n",
              "4        3500      awesome    ordered  ...       really     wesome         sent\n",
              "5   discounts    brilliant  delivered  ...      amazing     greate   executives\n",
              "6       13500  outstanding   delivery  ...       indeed     thanku    accepting\n",
              "7          3k           ai   flipkart  ...        beast     avarge       accept\n",
              "8       13000       decent   saturday  ...       loving       mast  replacement\n",
              "9        8999       really        oct  ...  outstanding  fantastic     flipkart\n",
              "10         5k   especially     seller  ...        truly      wsome  electronics\n",
              "11      12000        great      order  ...   incredible        osm         send\n",
              "12      12999      natural     august  ...       classy    delevry    customers\n",
              "13       2999        makes       12th  ...      premium        vry      request\n",
              "14      11700       stereo    packing  ...       killer       also       saying\n",
              "15       8000        crisp     launch  ...         beat    exelent     rejected\n",
              "16   billions     captures     booked  ...        terms       osom        staff\n",
              "17         8k    specially  logistics  ...    brilliant   fabulous      genuine\n",
              "18      10000       camera       22nd  ...     flagship    bettery         said\n",
              "19       9000      display    dilevry  ...       decent   delevery       return\n",
              "\n",
              "[20 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a23a3UYL5L3S",
        "outputId": "50041b23-304b-40e9-a28b-855634986cfd"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(362996, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng1xaE97krIJ",
        "outputId": "1a70c510-348e-458e-a71a-b6f173ab6691"
      },
      "source": [
        "actual=pd.read_pickle(\"/content/drive/My Drive/REVIEWS/reviews.pickle\")\n",
        "actual.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(363572, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMQgP2EbPokE"
      },
      "source": [
        "### Restoring the training of a model from the latest checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Jz-yd8nw6Rv"
      },
      "source": [
        "#data generating\n",
        "from random import seed\n",
        "from random import randint\n",
        "\n",
        "def gendata():\n",
        "  seed(42)\n",
        "  for i in range(0,len(seq_texts)):\n",
        "    lis=[]\n",
        "    lent=[]\n",
        "    while len(lent)<20:\n",
        "      value = randint(0, len(seq_texts)-1)\n",
        "      if value==i:\n",
        "        continue\n",
        "      lis.append(seq_texts[value])\n",
        "      lent.append(value)\n",
        "\n",
        "    yield seq_texts[i],lis\n",
        "datagen=tf.data.Dataset.from_generator(gendata, output_types=(tf.int32,tf.int32))\n",
        "datagen=datagen.repeat(1).shuffle(buffer_size=1024).batch(100).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-mUED4CPkAo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "outputId": "fb05236c-2e92-43f0-db99-577c6a36217d"
      },
      "source": [
        "for i in datagen.take(1):\n",
        "  k=i\n",
        "test=model_(embed_outputdim,aspects_k)\n",
        "_=test(i)\n",
        "test.load_weights(os.path.join(DATA_PATH,\"_newWeights\"))   "
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-6e48b02cc3d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_outputdim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maspects_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"_newWeights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2726\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2728\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2729\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2730\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6895\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6896\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6897\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6898\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: NameError: name 'seq_texts' is not defined\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 645, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 961, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-38-9e8cce4a9756>\", line 7, in gendata\n    for i in range(0,len(seq_texts)):\n\nNameError: name 'seq_texts' is not defined\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMhOsV2rR3Pv",
        "outputId": "9170b515-c595-4819-bb3b-3bd0a8936230"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.model_ at 0x7f357d99c650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_cYS1J5oI3D",
        "outputId": "c252a58b-6846-42b4-a25f-0fbfc7da5356"
      },
      "source": [
        "#model initialisation\n",
        "#abae=model_(embed_inputdim,embed_outputdim,trained_weights,aspects_k,dense,inputlength)\n",
        "# optimiser\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "@tf.function\n",
        "def train_step(input):\n",
        "    with tf.GradientTape() as tape:\n",
        "        #forward propagation\n",
        "        loss = test(input)[0]\n",
        "        #print(loss)\n",
        "\n",
        "    #getting gradients\n",
        "    gradients = tape.gradient(loss, test.trainable_variables)\n",
        "    #applying gradients\n",
        "    optimizer.apply_gradients(zip(gradients, test.trainable_variables))\n",
        "\n",
        "    return loss, gradients\n",
        "#no_iterations=1147*5          #epochs\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "\n",
        "checkpoint_path = \"/content/drive/My Drive/abae_logs/checkpoints/abae/train\"\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "ckpt = tf.train.Checkpoint(optimizer=optimizer, model=test)\n",
        "\n",
        "latest = tf.train.latest_checkpoint(checkpoint_path)\n",
        "ckpt.restore(latest)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
        "\n",
        "\n",
        "##check point to save\n",
        "#checkpoint_path = \"/content/drive/My Drive/abae_logs/checkpoints/abae/train_v2\"\n",
        "#ckpt = tf.train.Checkpoint(optimizer=optimizer, model=test)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
        "\n",
        "\n",
        "it=0\n",
        "loss_list=[]\n",
        "for k in range(0,1):\n",
        "  counter = 0\n",
        "  for input in datagen:\n",
        "\n",
        "      loss_, gradients = train_step(input)\n",
        "      #adding loss to train loss\n",
        "      train_loss(loss_)\n",
        "      counter = counter + 1\n",
        "      template = '''Done {} step, Loss: {:0.6f}'''\n",
        "\n",
        "    \n",
        "      if counter%100==0:\n",
        "        print(template.format(counter, train_loss.result()))\n",
        "        \n",
        "  loss_list.append(train_loss.result())\n",
        "  ckpt_save_path  = ckpt_manager.save()\n",
        "  print ('Saving checkpoint for iteration {} at {}'.format(k+1, ckpt_save_path))\n",
        "  print(counter, train_loss.result())\n",
        "  train_loss.reset_states()\n",
        "  \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done 100 step, Loss: 0.008561\n",
            "Done 200 step, Loss: 0.008550\n",
            "Done 300 step, Loss: 0.008563\n",
            "Done 400 step, Loss: 0.008538\n",
            "Done 500 step, Loss: 0.008546\n",
            "Done 600 step, Loss: 0.008532\n",
            "Done 700 step, Loss: 0.008539\n",
            "Done 800 step, Loss: 0.008539\n",
            "Done 900 step, Loss: 0.008542\n",
            "Done 1000 step, Loss: 0.008534\n",
            "Done 1100 step, Loss: 0.008536\n",
            "Done 1200 step, Loss: 0.008532\n",
            "Done 1300 step, Loss: 0.008531\n",
            "Done 1400 step, Loss: 0.008532\n",
            "Done 1500 step, Loss: 0.008530\n",
            "Done 1600 step, Loss: 0.008528\n",
            "Done 1700 step, Loss: 0.008530\n",
            "Done 1800 step, Loss: 0.008530\n",
            "Done 1900 step, Loss: 0.008526\n",
            "Done 2000 step, Loss: 0.008526\n",
            "Done 2100 step, Loss: 0.008525\n",
            "Done 2200 step, Loss: 0.008523\n",
            "Done 2300 step, Loss: 0.008524\n",
            "Done 2400 step, Loss: 0.008524\n",
            "Done 2500 step, Loss: 0.008521\n",
            "Done 2600 step, Loss: 0.008521\n",
            "Done 2700 step, Loss: 0.008522\n",
            "Done 2800 step, Loss: 0.008518\n",
            "Done 2900 step, Loss: 0.008518\n",
            "Done 3000 step, Loss: 0.008517\n",
            "Done 3100 step, Loss: 0.008516\n",
            "Done 3200 step, Loss: 0.008515\n",
            "Done 3300 step, Loss: 0.008514\n",
            "Done 3400 step, Loss: 0.008512\n",
            "Done 3500 step, Loss: 0.008512\n",
            "Done 3600 step, Loss: 0.008512\n",
            "Saving checkpoint for iteration 1 at /content/drive/My Drive/abae_logs/checkpoints/abae/train/ckpt-17\n",
            "3630 tf.Tensor(0.008512795, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "id": "duouubyyMHzd",
        "outputId": "dafa19f2-8ec3-4d05-bd12-fd93b2a0cd20"
      },
      "source": [
        "topics=test.weights[4].numpy()\n",
        "topic_words={}\n",
        "k=1\n",
        "for i in range(len(topics)):\n",
        "  topic_words[\"Aspect_\"+str(k)]=np.argsort(model.wv.cosine_similarities(topics[i],model.wv.vectors))[::-1][0:20]\n",
        "  k=k+1\n",
        "pd.DataFrame(topic_words).applymap(lambda x:model.wv.index2word[x])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Aspect_1</th>\n",
              "      <th>Aspect_2</th>\n",
              "      <th>Aspect_3</th>\n",
              "      <th>Aspect_4</th>\n",
              "      <th>Aspect_5</th>\n",
              "      <th>Aspect_6</th>\n",
              "      <th>Aspect_7</th>\n",
              "      <th>Aspect_8</th>\n",
              "      <th>Aspect_9</th>\n",
              "      <th>Aspect_10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rs</td>\n",
              "      <td>amoled</td>\n",
              "      <td>shipping</td>\n",
              "      <td>awesome</td>\n",
              "      <td>feels</td>\n",
              "      <td>achha</td>\n",
              "      <td>turn</td>\n",
              "      <td>disappoint</td>\n",
              "      <td>nice</td>\n",
              "      <td>fraud</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>billion</td>\n",
              "      <td>stunning</td>\n",
              "      <td>deliver</td>\n",
              "      <td>really</td>\n",
              "      <td>classy</td>\n",
              "      <td>koi</td>\n",
              "      <td>notifications</td>\n",
              "      <td>stunning</td>\n",
              "      <td>good</td>\n",
              "      <td>seller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>discount</td>\n",
              "      <td>amazing</td>\n",
              "      <td>30th</td>\n",
              "      <td>good</td>\n",
              "      <td>grip</td>\n",
              "      <td>mast</td>\n",
              "      <td>access</td>\n",
              "      <td>great</td>\n",
              "      <td>nicr</td>\n",
              "      <td>refused</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>offer</td>\n",
              "      <td>cameras</td>\n",
              "      <td>ekart</td>\n",
              "      <td>phone</td>\n",
              "      <td>sleek</td>\n",
              "      <td>acha</td>\n",
              "      <td>app</td>\n",
              "      <td>awesome</td>\n",
              "      <td>awesome</td>\n",
              "      <td>fault</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3500</td>\n",
              "      <td>awesome</td>\n",
              "      <td>ordered</td>\n",
              "      <td>superb</td>\n",
              "      <td>premium</td>\n",
              "      <td>kharab</td>\n",
              "      <td>disable</td>\n",
              "      <td>really</td>\n",
              "      <td>greate</td>\n",
              "      <td>executives</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>13000</td>\n",
              "      <td>brilliant</td>\n",
              "      <td>delivered</td>\n",
              "      <td>great</td>\n",
              "      <td>design</td>\n",
              "      <td>hai</td>\n",
              "      <td>vibrate</td>\n",
              "      <td>indeed</td>\n",
              "      <td>wesome</td>\n",
              "      <td>accepting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>8999</td>\n",
              "      <td>outstanding</td>\n",
              "      <td>delivery</td>\n",
              "      <td>also</td>\n",
              "      <td>looks</td>\n",
              "      <td>ko</td>\n",
              "      <td>irritates</td>\n",
              "      <td>amazing</td>\n",
              "      <td>mast</td>\n",
              "      <td>sent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3k</td>\n",
              "      <td>decent</td>\n",
              "      <td>flipkart</td>\n",
              "      <td>excellent</td>\n",
              "      <td>handy</td>\n",
              "      <td>nhi</td>\n",
              "      <td>button</td>\n",
              "      <td>beast</td>\n",
              "      <td>avarge</td>\n",
              "      <td>accept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>discounts</td>\n",
              "      <td>ai</td>\n",
              "      <td>saturday</td>\n",
              "      <td>one</td>\n",
              "      <td>hold</td>\n",
              "      <td>ek</td>\n",
              "      <td>application</td>\n",
              "      <td>loving</td>\n",
              "      <td>thanku</td>\n",
              "      <td>flipkart</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>13500</td>\n",
              "      <td>especially</td>\n",
              "      <td>august</td>\n",
              "      <td>amazing</td>\n",
              "      <td>hands</td>\n",
              "      <td>ki</td>\n",
              "      <td>icon</td>\n",
              "      <td>outstanding</td>\n",
              "      <td>fantastic</td>\n",
              "      <td>replacement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>5k</td>\n",
              "      <td>really</td>\n",
              "      <td>oct</td>\n",
              "      <td>fantastic</td>\n",
              "      <td>slippery</td>\n",
              "      <td>mai</td>\n",
              "      <td>rings</td>\n",
              "      <td>classy</td>\n",
              "      <td>osm</td>\n",
              "      <td>electronics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12999</td>\n",
              "      <td>great</td>\n",
              "      <td>seller</td>\n",
              "      <td>battery</td>\n",
              "      <td>compact</td>\n",
              "      <td>kam</td>\n",
              "      <td>bluetooth</td>\n",
              "      <td>incredible</td>\n",
              "      <td>wsome</td>\n",
              "      <td>customers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12000</td>\n",
              "      <td>makes</td>\n",
              "      <td>order</td>\n",
              "      <td>actually</td>\n",
              "      <td>sturdy</td>\n",
              "      <td>ye</td>\n",
              "      <td>silent</td>\n",
              "      <td>premium</td>\n",
              "      <td>delevry</td>\n",
              "      <td>send</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2999</td>\n",
              "      <td>natural</td>\n",
              "      <td>12th</td>\n",
              "      <td>ok</td>\n",
              "      <td>stunning</td>\n",
              "      <td>accha</td>\n",
              "      <td>launcher</td>\n",
              "      <td>killer</td>\n",
              "      <td>vry</td>\n",
              "      <td>saying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>11700</td>\n",
              "      <td>stereo</td>\n",
              "      <td>launch</td>\n",
              "      <td>design</td>\n",
              "      <td>pretty</td>\n",
              "      <td>bhi</td>\n",
              "      <td>notification</td>\n",
              "      <td>truly</td>\n",
              "      <td>exelent</td>\n",
              "      <td>staff</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>billions</td>\n",
              "      <td>specially</td>\n",
              "      <td>packing</td>\n",
              "      <td>disappointed</td>\n",
              "      <td>hand</td>\n",
              "      <td>lekin</td>\n",
              "      <td>scroll</td>\n",
              "      <td>beat</td>\n",
              "      <td>also</td>\n",
              "      <td>request</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>8000</td>\n",
              "      <td>crisp</td>\n",
              "      <td>booked</td>\n",
              "      <td>decent</td>\n",
              "      <td>metallic</td>\n",
              "      <td>bahut</td>\n",
              "      <td>disabled</td>\n",
              "      <td>brilliant</td>\n",
              "      <td>fabulous</td>\n",
              "      <td>genuine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>8k</td>\n",
              "      <td>captures</td>\n",
              "      <td>dilevry</td>\n",
              "      <td>outstanding</td>\n",
              "      <td>glossy</td>\n",
              "      <td>kiya</td>\n",
              "      <td>vibration</td>\n",
              "      <td>terms</td>\n",
              "      <td>osom</td>\n",
              "      <td>rejected</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>10000</td>\n",
              "      <td>camera</td>\n",
              "      <td>26th</td>\n",
              "      <td>think</td>\n",
              "      <td>lightweight</td>\n",
              "      <td>nahi</td>\n",
              "      <td>shortcut</td>\n",
              "      <td>decent</td>\n",
              "      <td>bettery</td>\n",
              "      <td>said</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>deal</td>\n",
              "      <td>display</td>\n",
              "      <td>22nd</td>\n",
              "      <td>disappointing</td>\n",
              "      <td>elegant</td>\n",
              "      <td>ka</td>\n",
              "      <td>tap</td>\n",
              "      <td>flagship</td>\n",
              "      <td>ok</td>\n",
              "      <td>fake</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Aspect_1     Aspect_2   Aspect_3  ...     Aspect_8   Aspect_9    Aspect_10\n",
              "0          rs       amoled   shipping  ...   disappoint       nice        fraud\n",
              "1     billion     stunning    deliver  ...     stunning       good       seller\n",
              "2    discount      amazing       30th  ...        great       nicr      refused\n",
              "3       offer      cameras      ekart  ...      awesome    awesome        fault\n",
              "4        3500      awesome    ordered  ...       really     greate   executives\n",
              "5       13000    brilliant  delivered  ...       indeed     wesome    accepting\n",
              "6        8999  outstanding   delivery  ...      amazing       mast         sent\n",
              "7          3k       decent   flipkart  ...        beast     avarge       accept\n",
              "8   discounts           ai   saturday  ...       loving     thanku     flipkart\n",
              "9       13500   especially     august  ...  outstanding  fantastic  replacement\n",
              "10         5k       really        oct  ...       classy        osm  electronics\n",
              "11      12999        great     seller  ...   incredible      wsome    customers\n",
              "12      12000        makes      order  ...      premium    delevry         send\n",
              "13       2999      natural       12th  ...       killer        vry       saying\n",
              "14      11700       stereo     launch  ...        truly    exelent        staff\n",
              "15   billions    specially    packing  ...         beat       also      request\n",
              "16       8000        crisp     booked  ...    brilliant   fabulous      genuine\n",
              "17         8k     captures    dilevry  ...        terms       osom     rejected\n",
              "18      10000       camera       26th  ...       decent    bettery         said\n",
              "19       deal      display       22nd  ...     flagship         ok         fake\n",
              "\n",
              "[20 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R492LZn1S-4e"
      },
      "source": [
        "### Aspects Inference\n",
        "#### From above collection of words, we can conclude some of the aspects as below ---\n",
        "Aspect1 - Price\n",
        "\n",
        "Aspect2 - Camera and the picture display\n",
        "\n",
        "Aspect3 - Delivery\n",
        "\n",
        "Aspect4 - Positive intent words\n",
        "\n",
        "Aspect5 - Physical experience (or features) of mobile\n",
        "\n",
        "Aspect6 - Collection of Hindi Language words \n",
        "\n",
        "Aspect7 - Accessing different applications\n",
        "\n",
        "Aspect8 - Extreme positive intent about product\n",
        "\n",
        "Aspect9 - Incorrectly spelled positive intent about product\n",
        "\n",
        "Aspect10- Bad CustomerService experience"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjkIO1GYS01U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}