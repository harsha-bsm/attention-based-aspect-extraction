{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "WORKINGFILE_modularising_Atten_based_Aspect_Extraction_POC_.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoEKsqQceLKm",
        "outputId": "2409be65-0b9c-4cc5-df91-f9c710d0a40b"
      },
      "source": [
        "# Mounting files on google drive which will ease file accessing \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/') "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5NHWDZ9pFfB",
        "outputId": "979cc15f-ba6a-4450-9821-bc8e6facc801"
      },
      "source": [
        "# Launching tensorflow 2 and invoking GPU if available on Google Collab\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXxnYe7yi0zp",
        "outputId": "40219dec-eca7-4bd0-df7e-cb31da06947e"
      },
      "source": [
        "!python \"/content/drive/MyDrive/REVIEWS/preprocess.py\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:tensorflow:From /content/drive/MyDrive/REVIEWS/preprocess.py:220: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use output_signature instead\n",
            "2021-08-19 10:55:11.804078: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-08-19 10:55:11.804149: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (2199ed675841): /proc/driver/nvidia/version does not exist\n",
            "2021-08-19 10:55:12.206239: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyElg5aO6N4-",
        "outputId": "f4f908c0-a650-4dd1-84db-8305c40fc5ee"
      },
      "source": [
        "import pandas as pd\n",
        "pd.read_pickle(\"/content/drive/MyDrive/REVIEWS/reviews.pickle\")[\"review\"].values"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Good READ MORE', 'Flashlight is very very bad READ MORE',\n",
              "       'battery is too bad . READ MORE', ...,\n",
              "       'Excellent product under the price of just 6k Worth every penny READ MORE',\n",
              "       'Nyc products Basic user good mobile Worth for money READ MORE',\n",
              "       'Good honor 9s READ MORE'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XrnZ2wYeK63",
        "outputId": "ea805cc3-b248-4033-9974-eef448bc606a"
      },
      "source": [
        "!pip install emot"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emot\n",
            "  Downloading emot-3.1-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 34.3 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 20 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 40 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 19 kB/s \n",
            "\u001b[?25hInstalling collected packages: emot\n",
            "Successfully installed emot-3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmN7qyB0EqeI",
        "outputId": "d3ef92f5-fb4d-44aa-be56-9b2c8101b838"
      },
      "source": [
        "!python \"/content/drive/MyDrive/REVIEWS/predict.py\" "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Nyc products Basic user good mobile Worth for money\n",
            "[[236, 139, 590, 143, 1, 6, 58, 14]]\n",
            "2021-08-19 11:50:33.315949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 11:50:33.323608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 11:50:33.324475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 11:50:33.325869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 11:50:33.326661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 11:50:33.327411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 11:50:33.795063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 11:50:33.795931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 11:50:33.796746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 11:50:33.797470: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 11:50:33.797532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10699 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\n",
            "2021-08-19 11:51:19.912496: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom/WeightsV5/weights_epoch_15: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n",
            "('belongs to topics in descending order_[1, 2, 9, 3, 4]', array([0.5806582 , 0.565051  , 0.5184304 , 0.5135377 , 0.49679542,\n",
            "       0.4777314 , 0.44736084, 0.4988363 , 0.53907937, 0.4190252 ],\n",
            "      dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWdpHTR3nOrm",
        "outputId": "849a1f98-d523-44d4-aebf-f9dfe30f7af0"
      },
      "source": [
        "!python \"/content/drive/MyDrive/REVIEWS/training.py\" "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:tensorflow:From /content/drive/MyDrive/REVIEWS/preprocess.py:220: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use output_signature instead\n",
            "2021-08-19 11:49:41.102706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 11:49:41.111047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 11:49:41.111812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 11:49:41.113373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 11:49:41.114140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 11:49:41.114928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 11:49:41.667593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 11:49:41.668474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 11:49:41.669320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 11:49:41.669991: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 11:49:41.670059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10699 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/REVIEWS/training.py\", line 76, in <module>\n",
            "  File \"/content/drive/MyDrive/REVIEWS/training.py\", line 28, in training\n",
            "    abae=model_(embed_outputdim,aspects_k)\n",
            "  File \"/content/drive/MyDrive/REVIEWS/model.py\", line 73, in __init__\n",
            "    self.embed_inputdim=len(embedding_layerinit()[1].wv.vocab)\n",
            "  File \"/content/drive/MyDrive/REVIEWS/model.py\", line 24, in embedding_layerinit\n",
            "    model=training_vocab(embed_dim=embed_outputdim,negative_sampling=5,min_count=1,window=10,iter=250,sg=1,train_again=vocabtrain_again,return_model=True)\n",
            "  File \"/content/drive/MyDrive/REVIEWS/preprocess.py\", line 126, in training_vocab\n",
            "    tkn.fit_on_texts(df['review'].values)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/keras_preprocessing/text.py\", line 225, in fit_on_texts\n",
            "    self.split)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/keras_preprocessing/text.py\", line 59, in text_to_word_sequence\n",
            "    translate_map = maketrans(translate_dict)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fuK8XEXbkkiU",
        "outputId": "1fcd335c-ed95-41d7-ff5a-115bc7a9e974"
      },
      "source": [
        "#data generating\n",
        "import os\n",
        "import io\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "import emot\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from gensim.models import FastText \n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense,Concatenate,TimeDistributed,Masking,GRU,Input,Dot,Reshape,Softmax\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from gensim.models import FastText \n",
        "from random import seed\n",
        "from random import randint\n",
        "from sklearn.cluster import KMeans\n",
        "DATA_PATH=\"/content/drive/My Drive/REVIEWS/\"\n",
        "raw_file=\"reviews.pickle\"\n",
        "preprocessed_file=\"preprocessed_df.pickle\"\n",
        "trained_embeddings=\"trained_embeddings\"\n",
        "padded_seqfile=\"padded_sequences.pickle\"\n",
        "tokenfile=\"token.pickle\"\n",
        "train_again=True\n",
        "vocabtrain_again=False\n",
        "stopword=stopwords.words(fileids=\"english\")\n",
        "embed_outputdim=100\n",
        "aspects_k=10 #number of aspects\n",
        "buffer_size=1024\n",
        "batch_size=100\n",
        "negative_samples=20\n",
        "WEIGHTS_PATH=\"/content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom/WeightsV5\"\n",
        "weight_path=\"/content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom/WeightsV5/weights_epoch_15\"\n",
        "CHECKPOINTS_PATH=\"/content/drive/My Drive/REVIEWS/checkpointsV5\"\n",
        "lr=0.001\n",
        "iterations=15\n",
        "return_bestweightspath=True\n",
        "lamda=0.5\n",
        "MODEL_CONFIG = {'embed_outputdim': embed_outputdim,\n",
        "                  'aspects_k' : aspects_k}\n",
        "with io.open(os.path.join(DATA_PATH,\"padded_sequences.pickle\"),\"rb\") as file:\n",
        "  seq_texts=pickle.load(file)\n",
        "from random import seed\n",
        "from random import randint\n",
        "\n",
        "def embedding_layerinit():\n",
        "  model=training_vocab(embed_dim=embed_outputdim,negative_sampling=5,min_count=1,window=10,iter=250,sg=1,train_again=vocabtrain_again,return_model=True)\n",
        "  trained_weights=np.vstack((np.zeros((1,100)),model.wv.vectors))\n",
        "  return trained_weights,model\n",
        "\n",
        "def inputlength():\n",
        "  seq=textinputsequence_padding(padding=\"post\",padded_seqagain=train_again,return_paddedsequences=True)\n",
        "  return seq.shape[1]\n",
        "\n",
        "\n",
        "def convert_emojis(review):\n",
        "  for x in review:\n",
        "    if x in emot.emo_unicode.UNICODE_EMOJI.keys():\n",
        "      review=review.replace(x,\"\")\n",
        "  return review\n",
        "\n",
        "#Converting all the extra spaces into single space. This will help while splitting the data in the future.\n",
        "def removecharacters(review):\n",
        "  review= re.sub('[^A-Za-z0-9]+',' ',review)  #anything exept numbers and alphabets, replace them with space\n",
        "  review=re.sub(r\"\\n\",\" \",review)  #new lines into space\n",
        "  review=re.sub(r\"\\t\",\" \",review)  #tabs into space\n",
        "  review=re.sub(r\"\\v\",\" \",review)  #vertical tab into space\n",
        "  review=re.sub(r\"\\s\",\" \",review)   #all extra spaces into single space\n",
        "  return review.lower()\n",
        "  \n",
        "def removenewords(review,wordcountdict):\n",
        "  return \" \".join([word for word in review.split(\" \") if  word in list(wordcountdict.keys())])\n",
        "\n",
        "def removestopwords(review,stopword):\n",
        "  return \" \".join([word for word in review.split(\" \") if not word in stopword])\n",
        "\n",
        "def word_count(review,wordcountdict,min_word_repeat):\n",
        "  return  \" \".join([word for word in review.split() if wordcountdict[word]>min_word_repeat] )\n",
        "\n",
        "\n",
        "\n",
        "def text_processing(stopword=stopword,min_word_repeat=10,sent_len_percentile=99.9,dump_aspickle=True,return_df=False):\n",
        "  #stop_words=set(stopwords.words('english'))\n",
        "  raw_path=os.path.join(DATA_PATH,raw_file)\n",
        "  preprocessed_path=os.path.join(DATA_PATH,preprocessed_file)\n",
        "  if os.path.isfile(preprocessed_path):\n",
        "    preprocessed_df=pd.read_pickle(preprocessed_path)\n",
        "  else:\n",
        "    df = pd.read_pickle(raw_path)\n",
        "    df[\"review\"]=df[\"review\"].apply(lambda x:x.strip(\"READ MORE\").lower())\n",
        "    df[\"review\"]=df[\"review\"].apply(convert_emojis)\n",
        "    df[\"review\"]=df[\"review\"].apply(decontractions)\n",
        "    df[\"review\"]=df[\"review\"].apply(removecharacters)\n",
        "    df[\"review\"]=pd.DataFrame(df[\"review\"].apply(lambda x:removestopwords(x,stopword)))\n",
        "    df[\"len\"]=df.review.str.split().apply(len)\n",
        "    l=np.percentile(df.len,sent_len_percentile)\n",
        "    df=df.loc[(df[\"len\"]>0) & (df[\"len\"]<l+1)]\n",
        "    tkn = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n') #tensorflow tokenising\n",
        "    tkn.fit_on_texts(df['review'].values)\n",
        "    word_countdict=tkn.word_counts  #word count dictionary\n",
        "    df[\"review\"]=pd.DataFrame(df[\"review\"].apply(lambda x:word_count(x,word_countdict,min_word_repeat)))\n",
        "    df[\"len\"]=df.review.str.split(\" \").apply(len)\n",
        "    if  dump_aspickle:\n",
        "      with open(preprocessed_path,\"wb\") as file:\n",
        "        pickle.dump(df.loc[df[\"len\"]>0],file)\n",
        "    preprocessed_df=df.loc[df[\"len\"]>0]\n",
        "  if return_df:\n",
        "    return preprocessed_df\n",
        "\n",
        "\n",
        "def tokenisation_on_traindata(return_tkn=True):\n",
        "  if os.path.isfile(os.path.join(DATA_PATH,preprocessed_file)):\n",
        "    train=pd.read_pickle(os.path.join(DATA_PATH,preprocessed_file))\n",
        "  else:\n",
        "    train=text_processing(stopword=stopword,min_word_repeat=10,sent_len_percentile=99.9,dump_aspickle=True,return_df=True)\n",
        "  tkn = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n') #tensorflow tokenising\n",
        "  tkn.fit_on_texts(train['review'].values)\n",
        "  with io.open(os.path.join(DATA_PATH,\"token.pickle\"),\"wb\") as file:\n",
        "    pickle.dump(tkn,file)\n",
        "  if return_tkn:\n",
        "    return tkn\n",
        "\n",
        "\n",
        "def training_vocab(embed_dim=100,negative_sampling=5,min_count=1,window=10,iter=250,sg=1,train_again=vocabtrain_again,return_model=True):\n",
        "  preprocessed_path=os.path.join(DATA_PATH,preprocessed_file)\n",
        "  if os.path.isfile(preprocessed_path):\n",
        "    df=pd.read_pickle(preprocessed_path)\n",
        "  else:\n",
        "    df=text_processing(stopword=set(stopwords.words('english')),return_df=True)\n",
        "  tkn = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "  tkn.fit_on_texts(df['review'].values)\n",
        "  word_countdict=tkn.word_counts\n",
        "  seq_texts=tkn.texts_to_sequences(df['review'].values)  #converting tokenised reviews into sequence of\n",
        "# integers with each integer corresponding to one word in the corpus\n",
        "  text=pd.Series(df['review'].values).apply(lambda x: x.split())  #spliting the reviews in a format suitable to  feed \n",
        "#into Fasttext for trainig vocab \n",
        "\n",
        "  trained_embeddings_path=os.path.join(DATA_PATH,trained_embeddings)\n",
        "  if os.path.isfile(trained_embeddings_path):\n",
        "    if train_again:\n",
        "      model = FastText(size=embed_dim, negative=negative_sampling, min_count=min_count,window=window,iter=iter,sg=sg) \n",
        "      model.build_vocab(text) \n",
        "      gensim_fasttext = model.train(sentences=text, \n",
        "                           sg=sg, ##skipgram\n",
        "                           epochs=iter, ##no of iterations\n",
        "                           size=embed_dim, ##dimentions of word embedding\n",
        "                           seed=1,\n",
        "                           total_examples=model.corpus_count)\n",
        "      model.save(trained_embeddings_path)\n",
        "    else:\n",
        "      model=FastText.load(trained_embeddings_path)\n",
        "  else:\n",
        "    model = FastText(size=embed_dim, negative=negative_sampling, min_count=min_count,window=window,iter=iter,sg=sg) \n",
        "    model.build_vocab(text)\n",
        "    gensim_fasttext = model.train(sentences=text, \n",
        "                           sg=sg, ##skipgram\n",
        "                           epochs=iter, ##no of iterations\n",
        "                           size=embed_dim, ##dimentions of word embedding\n",
        "                           seed=1,\n",
        "                           total_examples=model.corpus_count)\n",
        "    model.save(trained_embeddings_path)\n",
        "  \n",
        "  \n",
        "  if return_model:\n",
        "    return model\n",
        "\n",
        "def textinputsequence_padding(padding=\"post\",padded_seqagain=train_again,return_paddedsequences=True):\n",
        "  preprocessed_path=os.path.join(DATA_PATH,preprocessed_file)\n",
        "  padded_seqpath=os.path.join(DATA_PATH,padded_seqfile)\n",
        "  if os.path.isfile(preprocessed_path):\n",
        "    df=pd.read_pickle(preprocessed_path)\n",
        "  else:\n",
        "    df=text_processing(stopword=set(stopwords.words('english')),return_df=True)\n",
        "  if os.path.isfile(padded_seqpath):\n",
        "\n",
        "    if padded_seqagain:\n",
        "      tkn = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "      tkn.fit_on_texts(df['review'].values)\n",
        "      word_countdict=tkn.word_counts\n",
        "      seq_texts=tkn.texts_to_sequences(df['review'].values)\n",
        "      seq_texts=tf.keras.preprocessing.sequence.pad_sequences(seq_texts,\n",
        "                                                         maxlen=max(df.len),\n",
        "                                                         padding=padding)\n",
        "      with open(os.path.join(DATA_PATH,\"padded_sequences.pickle\"),\"wb\") as file:\n",
        "        pickle.dump(seq_texts,file)\n",
        "    else:\n",
        "      with open(padded_seqpath,\"rb\") as file:\n",
        "        seq_texts=pickle.load(file)\n",
        "  else:\n",
        "    tkn = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "    tkn.fit_on_texts(df['review'].values)\n",
        "    word_countdict=tkn.word_counts\n",
        "    seq_texts=tkn.texts_to_sequences(df['review'].values)\n",
        "    seq_texts=tf.keras.preprocessing.sequence.pad_sequences(seq_texts,\n",
        "                                                         maxlen=max(df.len),\n",
        "                                                         padding=padding)\n",
        "    with open(os.path.join(DATA_PATH,\"padded_sequences.pickle\"),\"wb\") as file:\n",
        "      pickle.dump(seq_texts,file)\n",
        "    with open(os.path.join(DATA_PATH,\"maxlen.pickle\"),\"wb\") as file:\n",
        "      pickle.dump(seq_texts.shape[1],file)\n",
        "\n",
        "  if return_paddedsequences:\n",
        "    return seq_texts\n",
        "\n",
        "\n",
        "def generate_dataset(buffer_size=buffer_size,batch_size=batch_size,negative_samples=negative_samples):\n",
        "  if  os.path.isfile(os.path.join(DATA_PATH,padded_seqfile)):\n",
        "    with io.open(os.path.join(DATA_PATH,\"padded_sequences.pickle\"),\"rb\") as file:\n",
        "      seq_texts=pickle.load(file)\n",
        "  else:\n",
        "    seq_texts=textinputsequence_padding(padding=\"post\",padded_seqagain=train_again,return_paddedsequences=True)\n",
        "\n",
        "  def gendata():\n",
        "    seed(42)\n",
        "    for i in range(0,len(seq_texts)):\n",
        "      lis=[]\n",
        "      lent=[]\n",
        "      while len(lent)<negative_samples:\n",
        "        value = randint(0, len(seq_texts)-1)\n",
        "        if value==i:\n",
        "          continue\n",
        "        lis.append(seq_texts[value])\n",
        "        lent.append(value)\n",
        "      yield seq_texts[i],lis\n",
        "  dataset=tf.data.Dataset.from_generator(gendata, output_types=(tf.int32,tf.int32))\n",
        "  dataset=dataset.repeat(1).shuffle(buffer_size=buffer_size).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return dataset\n",
        "def aspectlayer_weightinit():\n",
        "  trained_weights=embedding_layerinit()[0]\n",
        "  kmeans = KMeans(n_clusters=aspects_k,random_state=0,max_iter=500,n_jobs=-1).fit(trained_weights) #clustering the trained wordembeddings into 10 clusters\n",
        "  init=tf.constant_initializer(kmeans.cluster_centers_) #used for initialing the weights of final dense layer\n",
        "  return init\n",
        "\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      #self.embed_outputdim=embed_outputdim\n",
        "      self.soft = tf.keras.layers.Softmax(axis=-2,name=\"softmax_att\") #softmax layer\n",
        "      self.dot=tf.keras.layers.Dot(axes=(-1,-1),name=\"dot_att\")       #dot layer\n",
        "      self.w = tf.Variable(\n",
        "            initial_value=tf.random_normal_initializer()(shape=(embed_outputdim,embed_outputdim), dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )              #weights that captures essense between the word embedding and global context vector(or average of all the word embedding of the sentence)\n",
        "  \n",
        "    def call(self,embed_output, mask=None):\n",
        "\n",
        "      ys = tf.reduce_mean(embed_output,axis=-2) #average of all word embeddings of the sentence\n",
        "      ys=tf.expand_dims(ys,axis=-2)\n",
        "      eW=tf.matmul(embed_output, self.w)\n",
        "      eW = eW * tf.expand_dims(tf.cast(mask,tf.float32),-1) #maskpropagation. Preventing masked elements into calculations\n",
        "      f=self.dot([eW, ys])\n",
        "      f = f+tf.expand_dims(tf.cast(tf.math.equal(mask, False), f.dtype)*-1e9,-1) #multiplying all the masked elements by -1e9 so that the softmax step do not impact the vectors\n",
        "      f=self.soft(f)\n",
        "      zs=tf.math.reduce_sum(f*embed_output,axis=-2) #zs is aspect embedding space after attention mechanisms on words of the sentence\n",
        "                                                    # f - softmax - gives info about unimportant words in the sentence for extracting aspects \n",
        "      \n",
        "      return zs,f\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "      return cls(**config)\n",
        "\n",
        "\n",
        "# custom model\n",
        "\n",
        "class model_(tf.keras.Model):\n",
        "  def __init__(self,embed_outputdim,aspects_k):\n",
        "    super().__init__()\n",
        "    self.embed_inputdim=len(embedding_layerinit()[1].wv.vocab)\n",
        "    self.inputlength=inputlength()\n",
        "    self.embed_outputdim=embed_outputdim\n",
        "    self.trained_weights=embedding_layerinit()[0]\n",
        "    self.embedding=Embedding(input_dim=self.embed_inputdim+1,output_dim=self.embed_outputdim,mask_zero=True,\n",
        "                             input_length=self.inputlength,weights=[self.trained_weights],name=\"embedding_layer\",trainable=True) #embedding layer. Zero masking\n",
        "    self.attention=Attention()\n",
        "    self.aspects_k=aspects_k #number of aspects\n",
        "    self.init=aspectlayer_weightinit()\n",
        "    self.k = tf.keras.layers.Dense(aspects_k,name=\"dim_reduction_layer\",activation=\"sigmoid\")\n",
        "    self.dense= self.trained_weights.shape[1]\n",
        "    self.final=tf.keras.layers.Dense(self.dense,name=\"final_dense\",kernel_initializer=self.init) #weights are initialised with embedding clusters\n",
        "  def call(self,input):\n",
        "    e=self.embedding(input[0])\n",
        "    mask = self.embedding.compute_mask(input[0]) #computing mask so that this this can be used while propagating mask for subsequent layers\n",
        "    zs=self.attention(e, mask = mask)[0] #aspect vector\n",
        "    pt=self.k(zs)  #dimensionality vector\n",
        "    rs=self.final(pt) #reconstructed vector\n",
        "\n",
        "    # building regulariser to be used in the loss. [(T*transpose(T))-I]\n",
        "    reg=tf.tensordot(tf.linalg.normalize(self.final.weights[0],axis=1)[1],tf.linalg.normalize(self.final.weights[0],axis=1)[1],axes=[[1],[1]])\n",
        "    reg=reg-tf.ones([self.aspects_k,self.aspects_k])\n",
        "    reg=tf.norm(reg, ord='euclidean', axis=None, keepdims=None, name=None)\n",
        "    # calculating loss\n",
        "    r=tf.expand_dims(rs,-2)\n",
        "    f=tf.tensordot(rs,zs,[[0,1],[0,1]])\n",
        "    a=self.embedding(input[1])\n",
        "    a=tf.reduce_mean(a,axis=-2)\n",
        "    loss=tf.reduce_sum(tf.nn.relu(1-tf.reduce_sum(tf.tensordot(a,r,[[0,2],[0,2]]))+f))+lamda*reg #this is the loss which is to be minimised\n",
        "    return loss,pt,rs \n",
        "    \n",
        "    \n",
        "  def get_config(self):\n",
        "    config = {\n",
        "                  'embed_outputdim': self.embed_outputdim,\n",
        "                  'aspects_k' : self.aspects_k}\n",
        "    return config\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)\n",
        "def gendata():\n",
        "  seed(42)\n",
        "  for i in range(0,len(seq_texts)):\n",
        "    lis=[]\n",
        "    lent=[]\n",
        "    while len(lent)<20:\n",
        "      value = randint(0, len(seq_texts)-1)\n",
        "      if value==i:\n",
        "        continue\n",
        "      lis.append(seq_texts[value])\n",
        "      lent.append(value)\n",
        "\n",
        "    yield seq_texts[i],lis\n",
        "datagen=tf.data.Dataset.from_generator(gendata, output_types=(tf.int32,tf.int32))\n",
        "datagen=datagen.repeat(1).shuffle(buffer_size=1024).batch(100).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "import io\n",
        "for i in datagen.take(1):\n",
        "  k=i\n",
        "test=model_(embed_outputdim,aspects_k)\n",
        "_=test(i)\n",
        "test.load_weights(weight_path)\n",
        "topics=test.weights[4].numpy()\n",
        "topic_words={}\n",
        "k=1\n",
        "from gensim.models import FastText\n",
        "model=FastText.load(\"/content/drive/My Drive/REVIEWS/trained_embeddings\") #reusing already trained model\n",
        "for i in range(len(topics)):\n",
        "  topic_words[\"Aspect_\"+str(k)]=np.argsort(model.wv.cosine_similarities(topics[i],model.wv.vectors))[::-1][0:50]\n",
        "  k=k+1\n",
        "pd.DataFrame(topic_words).applymap(lambda x:model.wv.index2word[x])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Aspect_1</th>\n",
              "      <th>Aspect_2</th>\n",
              "      <th>Aspect_3</th>\n",
              "      <th>Aspect_4</th>\n",
              "      <th>Aspect_5</th>\n",
              "      <th>Aspect_6</th>\n",
              "      <th>Aspect_7</th>\n",
              "      <th>Aspect_8</th>\n",
              "      <th>Aspect_9</th>\n",
              "      <th>Aspect_10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rs</td>\n",
              "      <td>playing</td>\n",
              "      <td>cameras</td>\n",
              "      <td>notifications</td>\n",
              "      <td>nice</td>\n",
              "      <td>far</td>\n",
              "      <td>thin</td>\n",
              "      <td>ke</td>\n",
              "      <td>delivered</td>\n",
              "      <td>amazing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>offer</td>\n",
              "      <td>played</td>\n",
              "      <td>captures</td>\n",
              "      <td>messages</td>\n",
              "      <td>good</td>\n",
              "      <td>really</td>\n",
              "      <td>body</td>\n",
              "      <td>hai</td>\n",
              "      <td>deliver</td>\n",
              "      <td>loving</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>price</td>\n",
              "      <td>gaming</td>\n",
              "      <td>camera</td>\n",
              "      <td>app</td>\n",
              "      <td>osm</td>\n",
              "      <td>think</td>\n",
              "      <td>handy</td>\n",
              "      <td>liye</td>\n",
              "      <td>order</td>\n",
              "      <td>terms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>discount</td>\n",
              "      <td>runs</td>\n",
              "      <td>images</td>\n",
              "      <td>exit</td>\n",
              "      <td>nyc</td>\n",
              "      <td>trust</td>\n",
              "      <td>fits</td>\n",
              "      <td>bhi</td>\n",
              "      <td>assured</td>\n",
              "      <td>obviously</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>deal</td>\n",
              "      <td>games</td>\n",
              "      <td>shots</td>\n",
              "      <td>apps</td>\n",
              "      <td>xilent</td>\n",
              "      <td>actually</td>\n",
              "      <td>feels</td>\n",
              "      <td>h</td>\n",
              "      <td>cancel</td>\n",
              "      <td>great</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>bought</td>\n",
              "      <td>medium</td>\n",
              "      <td>depth</td>\n",
              "      <td>disable</td>\n",
              "      <td>happy</td>\n",
              "      <td>aspects</td>\n",
              "      <td>slim</td>\n",
              "      <td>hota</td>\n",
              "      <td>delivery</td>\n",
              "      <td>s9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>billion</td>\n",
              "      <td>heavy</td>\n",
              "      <td>portrait</td>\n",
              "      <td>whatsapp</td>\n",
              "      <td>awesome</td>\n",
              "      <td>comparison</td>\n",
              "      <td>beautiful</td>\n",
              "      <td>nhi</td>\n",
              "      <td>promised</td>\n",
              "      <td>absolutely</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>sale</td>\n",
              "      <td>performance</td>\n",
              "      <td>pictures</td>\n",
              "      <td>function</td>\n",
              "      <td>super</td>\n",
              "      <td>believe</td>\n",
              "      <td>case</td>\n",
              "      <td>jyada</td>\n",
              "      <td>ekart</td>\n",
              "      <td>brilliant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>diwali</td>\n",
              "      <td>pubg</td>\n",
              "      <td>wide</td>\n",
              "      <td>restart</td>\n",
              "      <td>love</td>\n",
              "      <td>one</td>\n",
              "      <td>bigger</td>\n",
              "      <td>ki</td>\n",
              "      <td>seller</td>\n",
              "      <td>beast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>12000</td>\n",
              "      <td>extreme</td>\n",
              "      <td>rear</td>\n",
              "      <td>menu</td>\n",
              "      <td>also</td>\n",
              "      <td>compared</td>\n",
              "      <td>compact</td>\n",
              "      <td>acha</td>\n",
              "      <td>item</td>\n",
              "      <td>outstanding</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>13000</td>\n",
              "      <td>lag</td>\n",
              "      <td>daylight</td>\n",
              "      <td>dialer</td>\n",
              "      <td>vry</td>\n",
              "      <td>better</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>accha</td>\n",
              "      <td>date</td>\n",
              "      <td>awesome</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>offers</td>\n",
              "      <td>graphics</td>\n",
              "      <td>front</td>\n",
              "      <td>application</td>\n",
              "      <td>amazing</td>\n",
              "      <td>definitely</td>\n",
              "      <td>grip</td>\n",
              "      <td>kam</td>\n",
              "      <td>packing</td>\n",
              "      <td>flagship</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>7000</td>\n",
              "      <td>handles</td>\n",
              "      <td>selfies</td>\n",
              "      <td>wifi</td>\n",
              "      <td>gjb</td>\n",
              "      <td>terms</td>\n",
              "      <td>slippery</td>\n",
              "      <td>ka</td>\n",
              "      <td>customer</td>\n",
              "      <td>loved</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>offered</td>\n",
              "      <td>min</td>\n",
              "      <td>macro</td>\n",
              "      <td>browser</td>\n",
              "      <td>osom</td>\n",
              "      <td>going</td>\n",
              "      <td>sleek</td>\n",
              "      <td>bahut</td>\n",
              "      <td>bangalore</td>\n",
              "      <td>absolute</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>republic</td>\n",
              "      <td>multitasking</td>\n",
              "      <td>capture</td>\n",
              "      <td>operations</td>\n",
              "      <td>aswam</td>\n",
              "      <td>compare</td>\n",
              "      <td>hands</td>\n",
              "      <td>mai</td>\n",
              "      <td>flipkart</td>\n",
              "      <td>masterpiece</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>discounted</td>\n",
              "      <td>smooth</td>\n",
              "      <td>angle</td>\n",
              "      <td>sometimes</td>\n",
              "      <td>sm</td>\n",
              "      <td>loving</td>\n",
              "      <td>classy</td>\n",
              "      <td>mast</td>\n",
              "      <td>address</td>\n",
              "      <td>really</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>discounts</td>\n",
              "      <td>lags</td>\n",
              "      <td>aperture</td>\n",
              "      <td>applications</td>\n",
              "      <td>loved</td>\n",
              "      <td>awesome</td>\n",
              "      <td>looks</td>\n",
              "      <td>ye</td>\n",
              "      <td>courier</td>\n",
              "      <td>aspects</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>brought</td>\n",
              "      <td>processor</td>\n",
              "      <td>photography</td>\n",
              "      <td>unlocking</td>\n",
              "      <td>superb</td>\n",
              "      <td>absolutely</td>\n",
              "      <td>design</td>\n",
              "      <td>nahi</td>\n",
              "      <td>asked</td>\n",
              "      <td>s8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>999</td>\n",
              "      <td>browsing</td>\n",
              "      <td>selfie</td>\n",
              "      <td>disconnect</td>\n",
              "      <td>tq</td>\n",
              "      <td>plus</td>\n",
              "      <td>bulky</td>\n",
              "      <td>ko</td>\n",
              "      <td>informed</td>\n",
              "      <td>iphones</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>bbd</td>\n",
              "      <td>charges</td>\n",
              "      <td>ois</td>\n",
              "      <td>access</td>\n",
              "      <td>wesome</td>\n",
              "      <td>amazing</td>\n",
              "      <td>thick</td>\n",
              "      <td>jada</td>\n",
              "      <td>cancelled</td>\n",
              "      <td>flagships</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>5500</td>\n",
              "      <td>100</td>\n",
              "      <td>shot</td>\n",
              "      <td>icon</td>\n",
              "      <td>ppo</td>\n",
              "      <td>earlier</td>\n",
              "      <td>silicon</td>\n",
              "      <td>lekin</td>\n",
              "      <td>executives</td>\n",
              "      <td>iphone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>competitive</td>\n",
              "      <td>continuous</td>\n",
              "      <td>lens</td>\n",
              "      <td>lock</td>\n",
              "      <td>perfect</td>\n",
              "      <td>series</td>\n",
              "      <td>durable</td>\n",
              "      <td>koi</td>\n",
              "      <td>23rd</td>\n",
              "      <td>incredible</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>19000</td>\n",
              "      <td>pretty</td>\n",
              "      <td>brilliant</td>\n",
              "      <td>occasionally</td>\n",
              "      <td>thnxx</td>\n",
              "      <td>phones</td>\n",
              "      <td>plastic</td>\n",
              "      <td>thoda</td>\n",
              "      <td>sent</td>\n",
              "      <td>powerful</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>got</td>\n",
              "      <td>game</td>\n",
              "      <td>night</td>\n",
              "      <td>contacts</td>\n",
              "      <td>a3s</td>\n",
              "      <td>bought</td>\n",
              "      <td>perfect</td>\n",
              "      <td>liya</td>\n",
              "      <td>responsible</td>\n",
              "      <td>stunning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>rupees</td>\n",
              "      <td>45</td>\n",
              "      <td>effect</td>\n",
              "      <td>launcher</td>\n",
              "      <td>wsm</td>\n",
              "      <td>worthy</td>\n",
              "      <td>cover</td>\n",
              "      <td>baat</td>\n",
              "      <td>care</td>\n",
              "      <td>mazing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>24999</td>\n",
              "      <td>battery</td>\n",
              "      <td>especially</td>\n",
              "      <td>download</td>\n",
              "      <td>nyz</td>\n",
              "      <td>since</td>\n",
              "      <td>hand</td>\n",
              "      <td>hu</td>\n",
              "      <td>ordered</td>\n",
              "      <td>handy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>amazing</td>\n",
              "      <td>55</td>\n",
              "      <td>bokeh</td>\n",
              "      <td>able</td>\n",
              "      <td>gd</td>\n",
              "      <td>go</td>\n",
              "      <td>glass</td>\n",
              "      <td>kya</td>\n",
              "      <td>said</td>\n",
              "      <td>wonderful</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>9990</td>\n",
              "      <td>moderate</td>\n",
              "      <td>zoom</td>\n",
              "      <td>button</td>\n",
              "      <td>lovly</td>\n",
              "      <td>still</td>\n",
              "      <td>weight</td>\n",
              "      <td>bhut</td>\n",
              "      <td>refused</td>\n",
              "      <td>way</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>purchased</td>\n",
              "      <td>gpu</td>\n",
              "      <td>saturated</td>\n",
              "      <td>internet</td>\n",
              "      <td>bast</td>\n",
              "      <td>comparing</td>\n",
              "      <td>solid</td>\n",
              "      <td>na</td>\n",
              "      <td>received</td>\n",
              "      <td>fulfills</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>14500</td>\n",
              "      <td>lasts</td>\n",
              "      <td>captured</td>\n",
              "      <td>browsing</td>\n",
              "      <td>suprb</td>\n",
              "      <td>love</td>\n",
              "      <td>hold</td>\n",
              "      <td>aur</td>\n",
              "      <td>service</td>\n",
              "      <td>among</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>daughter</td>\n",
              "      <td>charge</td>\n",
              "      <td>bright</td>\n",
              "      <td>connection</td>\n",
              "      <td>nais</td>\n",
              "      <td>might</td>\n",
              "      <td>size</td>\n",
              "      <td>isme</td>\n",
              "      <td>ecom</td>\n",
              "      <td>truly</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>5400</td>\n",
              "      <td>normal</td>\n",
              "      <td>light</td>\n",
              "      <td>hotstar</td>\n",
              "      <td>avarge</td>\n",
              "      <td>much</td>\n",
              "      <td>sturdy</td>\n",
              "      <td>bhai</td>\n",
              "      <td>staff</td>\n",
              "      <td>beat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>12990</td>\n",
              "      <td>awesome</td>\n",
              "      <td>saturation</td>\n",
              "      <td>facebook</td>\n",
              "      <td>delevery</td>\n",
              "      <td>great</td>\n",
              "      <td>heavier</td>\n",
              "      <td>ho</td>\n",
              "      <td>shipment</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>festive</td>\n",
              "      <td>quite</td>\n",
              "      <td>clicks</td>\n",
              "      <td>connected</td>\n",
              "      <td>valuable</td>\n",
              "      <td>similar</td>\n",
              "      <td>matt</td>\n",
              "      <td>itna</td>\n",
              "      <td>boy</td>\n",
              "      <td>compact</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>35k</td>\n",
              "      <td>amazing</td>\n",
              "      <td>photos</td>\n",
              "      <td>notification</td>\n",
              "      <td>mazing</td>\n",
              "      <td>previously</td>\n",
              "      <td>panel</td>\n",
              "      <td>kiya</td>\n",
              "      <td>reached</td>\n",
              "      <td>a13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>3k</td>\n",
              "      <td>hour</td>\n",
              "      <td>stereo</td>\n",
              "      <td>folders</td>\n",
              "      <td>exelent</td>\n",
              "      <td>buying</td>\n",
              "      <td>curved</td>\n",
              "      <td>sab</td>\n",
              "      <td>hub</td>\n",
              "      <td>beautiful</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>22500</td>\n",
              "      <td>continuously</td>\n",
              "      <td>motion</td>\n",
              "      <td>configure</td>\n",
              "      <td>dilevary</td>\n",
              "      <td>disappoint</td>\n",
              "      <td>gorgeous</td>\n",
              "      <td>ghatiya</td>\n",
              "      <td>pandemic</td>\n",
              "      <td>top</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>sell</td>\n",
              "      <td>approx</td>\n",
              "      <td>ai</td>\n",
              "      <td>accessing</td>\n",
              "      <td>mast</td>\n",
              "      <td>feel</td>\n",
              "      <td>protects</td>\n",
              "      <td>itne</td>\n",
              "      <td>commitment</td>\n",
              "      <td>xr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>18999</td>\n",
              "      <td>90</td>\n",
              "      <td>steady</td>\n",
              "      <td>shortcut</td>\n",
              "      <td>amaizing</td>\n",
              "      <td>like</td>\n",
              "      <td>aluminium</td>\n",
              "      <td>sabse</td>\n",
              "      <td>complaint</td>\n",
              "      <td>perfect</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>pricing</td>\n",
              "      <td>50</td>\n",
              "      <td>crisp</td>\n",
              "      <td>interface</td>\n",
              "      <td>gud</td>\n",
              "      <td>however</td>\n",
              "      <td>fit</td>\n",
              "      <td>gya</td>\n",
              "      <td>parcel</td>\n",
              "      <td>underrated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>prices</td>\n",
              "      <td>6</td>\n",
              "      <td>image</td>\n",
              "      <td>truecaller</td>\n",
              "      <td>delevary</td>\n",
              "      <td>rather</td>\n",
              "      <td>build</td>\n",
              "      <td>achha</td>\n",
              "      <td>elivered</td>\n",
              "      <td>course</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>8499</td>\n",
              "      <td>40</td>\n",
              "      <td>capturing</td>\n",
              "      <td>music</td>\n",
              "      <td>exilent</td>\n",
              "      <td>best</td>\n",
              "      <td>metallic</td>\n",
              "      <td>kar</td>\n",
              "      <td>replacement</td>\n",
              "      <td>innovation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>5k</td>\n",
              "      <td>hrs</td>\n",
              "      <td>ultrawide</td>\n",
              "      <td>fine</td>\n",
              "      <td>awsm</td>\n",
              "      <td>recently</td>\n",
              "      <td>small</td>\n",
              "      <td>jata</td>\n",
              "      <td>gift</td>\n",
              "      <td>s10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>4999</td>\n",
              "      <td>monster</td>\n",
              "      <td>cam</td>\n",
              "      <td>key</td>\n",
              "      <td>sprb</td>\n",
              "      <td>performance</td>\n",
              "      <td>back</td>\n",
              "      <td>yar</td>\n",
              "      <td>dilivery</td>\n",
              "      <td>best</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>bigbillion</td>\n",
              "      <td>really</td>\n",
              "      <td>impressive</td>\n",
              "      <td>home</td>\n",
              "      <td>wonderfull</td>\n",
              "      <td>features</td>\n",
              "      <td>ergonomic</td>\n",
              "      <td>raha</td>\n",
              "      <td>courteous</td>\n",
              "      <td>performance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>31k</td>\n",
              "      <td>percent</td>\n",
              "      <td>mode</td>\n",
              "      <td>volume</td>\n",
              "      <td>tx</td>\n",
              "      <td>find</td>\n",
              "      <td>pretty</td>\n",
              "      <td>ek</td>\n",
              "      <td>booked</td>\n",
              "      <td>superb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>paid</td>\n",
              "      <td>charging</td>\n",
              "      <td>reproduction</td>\n",
              "      <td>icons</td>\n",
              "      <td>really</td>\n",
              "      <td>way</td>\n",
              "      <td>finish</td>\n",
              "      <td>chahiye</td>\n",
              "      <td>receive</td>\n",
              "      <td>affordable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>awesome</td>\n",
              "      <td>smoothly</td>\n",
              "      <td>decent</td>\n",
              "      <td>default</td>\n",
              "      <td>beautiful</td>\n",
              "      <td>phone</td>\n",
              "      <td>scratches</td>\n",
              "      <td>badiya</td>\n",
              "      <td>us</td>\n",
              "      <td>ultimate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>best</td>\n",
              "      <td>5hrs</td>\n",
              "      <td>lighting</td>\n",
              "      <td>alarm</td>\n",
              "      <td>thanks</td>\n",
              "      <td>s9</td>\n",
              "      <td>hides</td>\n",
              "      <td>jo</td>\n",
              "      <td>customers</td>\n",
              "      <td>xceptional</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>2999</td>\n",
              "      <td>display</td>\n",
              "      <td>display</td>\n",
              "      <td>inbuilt</td>\n",
              "      <td>supper</td>\n",
              "      <td>model</td>\n",
              "      <td>fragile</td>\n",
              "      <td>kr</td>\n",
              "      <td>accept</td>\n",
              "      <td>phone</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Aspect_1      Aspect_2      Aspect_3  ... Aspect_8     Aspect_9    Aspect_10\n",
              "0            rs       playing       cameras  ...       ke    delivered      amazing\n",
              "1         offer        played      captures  ...      hai      deliver       loving\n",
              "2         price        gaming        camera  ...     liye        order        terms\n",
              "3      discount          runs        images  ...      bhi      assured    obviously\n",
              "4          deal         games         shots  ...        h       cancel        great\n",
              "5        bought        medium         depth  ...     hota     delivery           s9\n",
              "6       billion         heavy      portrait  ...      nhi     promised   absolutely\n",
              "7          sale   performance      pictures  ...    jyada        ekart    brilliant\n",
              "8        diwali          pubg          wide  ...       ki       seller        beast\n",
              "9         12000       extreme          rear  ...     acha         item  outstanding\n",
              "10        13000           lag      daylight  ...    accha         date      awesome\n",
              "11       offers      graphics         front  ...      kam      packing     flagship\n",
              "12         7000       handles       selfies  ...       ka     customer        loved\n",
              "13      offered           min         macro  ...    bahut    bangalore     absolute\n",
              "14     republic  multitasking       capture  ...      mai     flipkart  masterpiece\n",
              "15   discounted        smooth         angle  ...     mast      address       really\n",
              "16    discounts          lags      aperture  ...       ye      courier      aspects\n",
              "17      brought     processor   photography  ...     nahi        asked           s8\n",
              "18          999      browsing        selfie  ...       ko     informed      iphones\n",
              "19          bbd       charges           ois  ...     jada    cancelled    flagships\n",
              "20         5500           100          shot  ...    lekin   executives       iphone\n",
              "21  competitive    continuous          lens  ...      koi         23rd   incredible\n",
              "22        19000        pretty     brilliant  ...    thoda         sent     powerful\n",
              "23          got          game         night  ...     liya  responsible     stunning\n",
              "24       rupees            45        effect  ...     baat         care       mazing\n",
              "25        24999       battery    especially  ...       hu      ordered        handy\n",
              "26      amazing            55         bokeh  ...      kya         said    wonderful\n",
              "27         9990      moderate          zoom  ...     bhut      refused          way\n",
              "28    purchased           gpu     saturated  ...       na     received     fulfills\n",
              "29        14500         lasts      captured  ...      aur      service        among\n",
              "30     daughter        charge        bright  ...     isme         ecom        truly\n",
              "31         5400        normal         light  ...     bhai        staff         beat\n",
              "32        12990       awesome    saturation  ...       ho     shipment         love\n",
              "33      festive         quite        clicks  ...     itna          boy      compact\n",
              "34          35k       amazing        photos  ...     kiya      reached          a13\n",
              "35           3k          hour        stereo  ...      sab          hub    beautiful\n",
              "36        22500  continuously        motion  ...  ghatiya     pandemic          top\n",
              "37         sell        approx            ai  ...     itne   commitment           xr\n",
              "38        18999            90        steady  ...    sabse    complaint      perfect\n",
              "39      pricing            50         crisp  ...      gya       parcel   underrated\n",
              "40       prices             6         image  ...    achha     elivered       course\n",
              "41         8499            40     capturing  ...      kar  replacement   innovation\n",
              "42           5k           hrs     ultrawide  ...     jata         gift          s10\n",
              "43         4999       monster           cam  ...      yar     dilivery         best\n",
              "44   bigbillion        really    impressive  ...     raha    courteous  performance\n",
              "45          31k       percent          mode  ...       ek       booked       superb\n",
              "46         paid      charging  reproduction  ...  chahiye      receive   affordable\n",
              "47      awesome      smoothly        decent  ...   badiya           us     ultimate\n",
              "48         best          5hrs      lighting  ...       jo    customers   xceptional\n",
              "49         2999       display       display  ...       kr       accept        phone\n",
              "\n",
              "[50 rows x 10 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssg5yNdiqbEJ"
      },
      "source": [
        "pd.DataFrame(topic_words).applymap(lambda x:model.wv.index2word[x]).to_csv(os.path.join(DATA_PATH,\"V5_topics.csv\")) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVkhti0fJo3i"
      },
      "source": [
        "for i in datagen.take(10):\n",
        "  k=i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwG5SPhbD8lD",
        "outputId": "10ed63a6-3139-4fac-eb2e-220e755f53ae"
      },
      "source": [
        "l=test.layers   \n",
        "l[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.Attention at 0x7f402d661d50>"
            ]
          },
          "execution_count": 7,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSogm41UJYPn"
      },
      "source": [
        "k=k[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuTBwQrDKb7E"
      },
      "source": [
        "k=k.numpy().reshape(1,223)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRtavvC-5f5j"
      },
      "source": [
        "endsample=l[0](k)\n",
        "mask=l[0].compute_mask(k)\n",
        "att=l[1](endsample,mask)[0]  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlme3YR7QiUs"
      },
      "source": [
        "boole=(np.sort(l[2](att)[0].numpy())[::-1]>0.5)*1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3buD8LWP3Pr",
        "outputId": "bcb67561-94ed-4504-a220-7d608b5c65af"
      },
      "source": [
        "final=list((np.argsort(l[2](att)[0].numpy())[::-1]+1)*boole)\n",
        "final=[x for x in final if x!=0]\n",
        "final"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2, 9, 4, 6]"
            ]
          },
          "execution_count": 14,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqsxaee4YohW",
        "outputId": "f97d5a44-e559-403a-df01-d394bd9bbcab"
      },
      "source": [
        "[x for x in final if x!=0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2, 9, 4, 6]"
            ]
          },
          "execution_count": 13,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sROtDp_2N4JL",
        "outputId": "db1fdb71-7e88-4365-df78-4bcdd6984be0"
      },
      "source": [
        "np.argwhere(np.argsort(l[2](att)[0].numpy()>0.5).reshape(-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 8])"
            ]
          },
          "execution_count": 53,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbhcKkvhLR75",
        "outputId": "9c97e25a-746e-4245-89b7-636c1b19a355"
      },
      "source": [
        "l[2](att).numpy()>0.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ True,  True, False, False, False, False, False, False,  True,\n",
              "        False]])"
            ]
          },
          "execution_count": 49,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GVZZl9sLSGw"
      },
      "source": [
        "topic_num=np.argmax(l[2](att))+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbhxmeeloBfD",
        "outputId": "ff88fd93-8e6a-48fb-80af-362506ba3d76"
      },
      "source": [
        "!pip install emot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting emot\n",
            "  Downloading emot-3.1-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 20 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 40 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 18 kB/s \n",
            "\u001b[?25hInstalling collected packages: emot\n",
            "Successfully installed emot-3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh2_g0FMtoZv",
        "outputId": "5269c078-85af-4422-b789-ab2a1ac4df8d"
      },
      "source": [
        "%%writefile /content/drive/MyDrive/REVIEWS/app.py\n",
        "from flask import Flask\n",
        "from flask import request\n",
        "from flask import jsonify\n",
        "from predict import *\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/correct', methods=['GET'])\n",
        "def aspect():\n",
        "    try:\n",
        "        text = request.args.get('text')\n",
        "        results = inf_predict.topicpredict(text)\n",
        "    except Exception as error:\n",
        "        results = 'Unexpected error: {}'.format(error)\n",
        "    out = {'input_sentence':text, 'correct_sent':results}\n",
        "    out = jsonify(out)\n",
        "    return out\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/drive/MyDrive/REVIEWS/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V96G5klzdvdN",
        "outputId": "b7268fdf-e967-4240-e98b-548e0e040a68"
      },
      "source": [
        "%%writefile /content/drive/MyDrive/REVIEWS/predict.py\n",
        "import emot\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from gensim.models import FastText \n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense,Concatenate,TimeDistributed,Masking,GRU,Input,Dot,Reshape,Softmax\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Using NLTK for PoS tagging and Stopwords removal\n",
        "# Downloading the corresponding packages\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from gensim.models import FastText \n",
        "from random import seed\n",
        "from random import randint\n",
        "from sklearn.cluster import KMeans\n",
        "from config import *\n",
        "from model import model_\n",
        "from preprocess import *\n",
        "from training import *\n",
        "\n",
        "\n",
        "\n",
        "with io.open(os.path.join(DATA_PATH,padded_seqfile),\"rb\") as file:\n",
        "  maxlen=pickle.load(file).shape[1]\n",
        "topicpredict(testsample,weight_path,model_,MODEL_CONFIG,maxlen=223):  \n",
        "  df=pd.DataFrame([testsample],columns=[\"review\"])\n",
        "  df[\"review\"]=df[\"review\"].apply(lambda x:x.strip(\"READ MORE\").lower())\n",
        "  df[\"review\"]=df[\"review\"].apply(convert_emojis)\n",
        "  df[\"review\"]=df[\"review\"].apply(decontractions)\n",
        "  df[\"review\"]=df[\"review\"].apply(removecharacters)\n",
        "  df[\"review\"]=pd.DataFrame(df[\"review\"].apply(lambda x:removestopwords(x,stopword)))\n",
        "  df[\"len\"]=df.review.str.split().apply(len)\n",
        "  df=df.loc[(df[\"len\"]>0) & (df[\"len\"]<maxlen+1)]\n",
        "  tkn=tokenisation_on_traindata(return_tkn=True)\n",
        "  word_countdict=tkn.word_counts  #word count dictionary\n",
        "  df[\"review\"]=pd.DataFrame(df[\"review\"].apply(lambda x:removenewords(x,word_countdict)))\n",
        "  df[\"review\"]=pd.DataFrame(df[\"review\"].apply(lambda x:word_count(x,word_countdict,10)))\n",
        "  \n",
        "  seq_texts=tkn.texts_to_sequences(df['review'].values)\n",
        "  print(seq_texts)\n",
        "  seq_texts=tf.keras.preprocessing.sequence.pad_sequences(seq_texts,\n",
        "                                                         maxlen=maxlen,\n",
        "                                                         padding='post')\n",
        "\n",
        "\n",
        "  k=tf.random.uniform(shape=[100,223],minval=1,maxval=38,dtype=tf.int32),tf.random.uniform(shape=[100,20,223],minval=1,maxval=38,dtype=tf.int32)\n",
        "  inf=model_.from_config(MODEL_CONFIG)\n",
        "  inf(k)\n",
        "  inf.load_weights(weight_path)\n",
        "  l=inf.layers\n",
        "  sample=seq_texts.reshape(1,maxlen)\n",
        "  endsample=l[0](sample)\n",
        "  mask=l[0].compute_mask(sample)\n",
        "  att=l[1](endsample,mask)[0]  \n",
        "  topic_num=np.argmax(l[2](att))+1\n",
        "  return \"belongs to topic_\"+str(topic_num),l[2](att)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/drive/MyDrive/REVIEWS/predict.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "yJ6w8UfZssTh",
        "outputId": "84fa1940-7eae-4b10-b348-43e1b4d9bf38"
      },
      "source": [
        "#%%writefile /content/drive/MyDrive/REVIEWS/preprocess.py\n",
        "# importing necessary modules\n",
        "!pip install emot\n",
        "import emot\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import seaborn as sns\n",
        "import pandas as pd\n",
        "from gensim.models import FastText \n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense,Concatenate,TimeDistributed,Masking,GRU,Input,Dot,Reshape,Softmax\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Using NLTK for PoS tagging and Stopwords removal\n",
        "# Downloading the corresponding packages\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from gensim.models import FastText \n",
        "from random import seed\n",
        "from random import randint\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "\n",
        "#  removing some of the common contractions.\n",
        "def decontractions(phrase):\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "def convert_emojis(review):\n",
        "  for x in review:\n",
        "    if x in emot.emo_unicode.UNICODE_EMOJI.keys():\n",
        "      review=review.replace(x,\"\")\n",
        "  return review\n",
        "\n",
        "#Converting all the extra spaces into single space. This will help while splitting the data in the future.\n",
        "def removecharacters(review):\n",
        "  review= re.sub('[^A-Za-z0-9]+',' ',review)  #anything exept numbers and alphabets, replace them with space\n",
        "  review=re.sub(r\"\\n\",\" \",review)  #new lines into space\n",
        "  review=re.sub(r\"\\t\",\" \",review)  #tabs into space\n",
        "  review=re.sub(r\"\\v\",\" \",review)  #vertical tab into space\n",
        "  review=re.sub(r\"\\s\",\" \",review)   #all extra spaces into single space\n",
        "  return review.lower()\n",
        "\n",
        "def removenewords(review,wordcountdict):\n",
        "  return \" \".join([word for word in review.split(\" \") if  word in list(wordcountdict.keys())])\n",
        "\n",
        "def removestopwords(review,stopword):\n",
        "  return \" \".join([word for word in review.split(\" \") if not word in stopword])\n",
        "\n",
        "def word_count(review,wordcountdict,min_word_repeat):\n",
        "  return  \" \".join([word for word in review.split() if wordcountdict[word]>min_word_repeat] )\n",
        "\n",
        "\n",
        "\n",
        "def text_processing(stopword=stopword,min_word_repeat=10,sent_len_percentile=99.9,dump_aspickle=True,return_df=False):\n",
        "  #stop_words=set(stopwords.words('english'))\n",
        "  raw_path=os.path.join(DATA_PATH,raw_file)\n",
        "  preprocessed_path=os.path.join(DATA_PATH,preprocessed_file)\n",
        "  if os.path.isfile(preprocessed_path):\n",
        "    preprocessed_df=pd.read_pickle(preprocessed_path)\n",
        "  else:\n",
        "    df = pd.read_pickle(raw_path)\n",
        "    df[\"review\"]=df[\"review\"].apply(lambda x:x.strip(\"READ MORE\").lower())\n",
        "    df[\"review\"]=df[\"review\"].apply(convert_emojis)\n",
        "    df[\"review\"]=df[\"review\"].apply(decontractions)\n",
        "    df[\"review\"]=df[\"review\"].apply(removecharacters)\n",
        "    df[\"review\"]=pd.DataFrame(df[\"review\"].apply(lambda x:removestopwords(x,stopword)))\n",
        "    df[\"len\"]=df.review.str.split().apply(len)\n",
        "    l=np.percentile(df.len,sent_len_percentile)\n",
        "    df=df.loc[(df[\"len\"]>0) & (df[\"len\"]<l+1)]\n",
        "    tkn = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n') #tensorflow tokenising\n",
        "    tkn.fit_on_texts(df['review'].values)\n",
        "    word_countdict=tkn.word_counts  #word count dictionary\n",
        "    df[\"review\"]=pd.DataFrame(df[\"review\"].apply(lambda x:word_count(x,word_countdict,min_word_repeat)))\n",
        "    df[\"len\"]=df.review.str.split(\" \").apply(len)\n",
        "    if  dump_aspickle:\n",
        "      with io.open(preprocessed_path,\"wb\") as file:\n",
        "        pickle.dump(df.loc[df[\"len\"]>0],file)\n",
        "    preprocessed_df=df.loc[df[\"len\"]>0]\n",
        "  if return_df:\n",
        "    return preprocessed_df\n",
        "\n",
        "\n",
        "def tokenisation_on_traindata(return_tkn=True):\n",
        "  if os.path.isfile(os.path.join(DATA_PATH,preprocessed_file)):\n",
        "    train=pd.read_pickle(os.path.join(DATA_PATH,preprocessed_file))\n",
        "  else:\n",
        "    train=text_processing(stopword=stopword,min_word_repeat=10,sent_len_percentile=99.9,dump_aspickle=True,return_df=True)\n",
        "  tkn = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n') #tensorflow tokenising\n",
        "  tkn.fit_on_texts(train['review'].values)\n",
        "  with io.open(os.path.join(DATA_PATH,\"token.pickle\"),\"wb\") as file:\n",
        "    pickle.dump(tkn,file)\n",
        "  if return_tkn:\n",
        "    return tkn\n",
        "\n",
        "\n",
        "def training_vocab(embed_dim=100,negative_sampling=5,min_count=1,window=10,iter=250,sg=1,train_again=vocabtrain_again,return_model=True):\n",
        "  preprocessed_path=os.path.join(DATA_PATH,preprocessed_file)\n",
        "  if os.path.isfile(preprocessed_path):\n",
        "    df=pd.read_pickle(preprocessed_path)\n",
        "  else:\n",
        "    df=text_processing(stopword=set(stopwords.words('english')),return_df=True)\n",
        "  tkn = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "  tkn.fit_on_texts(df['review'].values)\n",
        "  word_countdict=tkn.word_counts\n",
        "  seq_texts=tkn.texts_to_sequences(df['review'].values)  #converting tokenised reviews into sequence of\n",
        "# integers with each integer corresponding to one word in the corpus\n",
        "  text=pd.Series(df['review'].values).apply(lambda x: x.split())  #spliting the reviews in a format suitable to  feed \n",
        "#into Fasttext for trainig vocab \n",
        "\n",
        "  trained_embeddings_path=os.path.join(DATA_PATH,trained_embeddings)\n",
        "  if os.path.isfile(trained_embeddings_path):\n",
        "    if train_again:\n",
        "      model = FastText(size=embed_dim, negative=negative_sampling, min_count=min_count,window=window,iter=iter,sg=sg) \n",
        "      model.build_vocab(text) \n",
        "      gensim_fasttext = model.train(sentences=text, \n",
        "                           sg=sg, ##skipgram\n",
        "                           epochs=iter, ##no of iterations\n",
        "                           size=embed_dim, ##dimentions of word embedding\n",
        "                           seed=1,\n",
        "                           total_examples=model.corpus_count)\n",
        "      model.save(trained_embeddings_path)\n",
        "    else:\n",
        "      model=FastText.load(trained_embeddings_path)\n",
        "  else:\n",
        "    model = FastText(size=embed_dim, negative=negative_sampling, min_count=min_count,window=window,iter=iter,sg=sg) \n",
        "    model.build_vocab(text)\n",
        "    gensim_fasttext = model.train(sentences=text, \n",
        "                           sg=sg, ##skipgram\n",
        "                           epochs=iter, ##no of iterations\n",
        "                           size=embed_dim, ##dimentions of word embedding\n",
        "                           seed=1,\n",
        "                           total_examples=model.corpus_count)\n",
        "    model.save(trained_embeddings_path)\n",
        "  \n",
        "  \n",
        "  if return_model:\n",
        "    return model\n",
        "\n",
        "def textinputsequence_padding(padding=\"post\",padded_seqagain=train_again,return_paddedsequences=True):\n",
        "  preprocessed_path=os.path.join(DATA_PATH,preprocessed_file)\n",
        "  padded_seqpath=os.path.join(DATA_PATH,padded_seqfile)\n",
        "  if os.path.isfile(preprocessed_path):\n",
        "    df=pd.read_pickle(preprocessed_path)\n",
        "  else:\n",
        "    df=text_processing(stopword=set(stopwords.words('english')),return_df=True)\n",
        "  if os.path.isfile(padded_seqpath):\n",
        "\n",
        "    if padded_seqagain:\n",
        "      tkn = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "      tkn.fit_on_texts(df['review'].values)\n",
        "      word_countdict=tkn.word_counts\n",
        "      seq_texts=tkn.texts_to_sequences(df['review'].values)\n",
        "      seq_texts=tf.keras.preprocessing.sequence.pad_sequences(seq_texts,\n",
        "                                                         maxlen=max(df.len),\n",
        "                                                         padding=padding)\n",
        "      with io.open(os.path.join(DATA_PATH,\"padded_sequences.pickle\"),\"wb\") as file:\n",
        "        pickle.dump(seq_texts,file)\n",
        "    else:\n",
        "      with io.open(padded_seqpath,\"rb\") as file:\n",
        "        seq_texts=pickle.load(file)\n",
        "  else:\n",
        "    tkn = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "    tkn.fit_on_texts(df['review'].values)\n",
        "    word_countdict=tkn.word_counts\n",
        "    seq_texts=tkn.texts_to_sequences(df['review'].values)\n",
        "    seq_texts=tf.keras.preprocessing.sequence.pad_sequences(seq_texts,\n",
        "                                                         maxlen=max(df.len),\n",
        "                                                         padding=padding)\n",
        "    with io.open(os.path.join(DATA_PATH,\"padded_seqfile\"),\"wb\") as file:\n",
        "      pickle.dump(seq_texts,file)\n",
        "\n",
        "  if return_paddedsequences:\n",
        "    return seq_texts\n",
        "\n",
        "\n",
        "def generate_dataset(buffer_size=buffer_size,batch_size=batch_size,negative_samples=negative_samples):\n",
        "  if  os.path.isfile(os.path.join(DATA_PATH,padded_seqfile)):\n",
        "    with io.open(os.path.join(DATA_PATH,\"padded_sequences.pickle\"),\"rb\") as file:\n",
        "      seq_texts=pickle.load(file)\n",
        "  else:\n",
        "    seq_texts=textinputsequence_padding(padding=\"post\",padded_seqagain=train_again,return_paddedsequences=True)\n",
        "\n",
        "  def gendata():\n",
        "    seed(42)\n",
        "    for i in range(0,len(seq_texts)):\n",
        "      lis=[]\n",
        "      lent=[]\n",
        "      while len(lent)<negative_samples:\n",
        "        value = randint(0, len(seq_texts)-1)\n",
        "        if value==i:\n",
        "          continue\n",
        "        lis.append(seq_texts[value])\n",
        "        lent.append(value)\n",
        "      yield seq_texts[i],lis\n",
        "  dataset=tf.data.Dataset.from_generator(gendata, output_types=(tf.int32,tf.int32))\n",
        "  dataset=dataset.repeat(1).shuffle(buffer_size=buffer_size).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting emot\n",
            "  Downloading emot-3.1-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 20 kB 32.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 40 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 51 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 26 kB/s \n",
            "\u001b[?25hInstalling collected packages: emot\n",
            "Successfully installed emot-3.1\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f4fd212d8176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mtext_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_word_repeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent_len_percentile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m99.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdump_aspickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m   \u001b[0;31m#stop_words=set(stopwords.words('english'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0mraw_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mraw_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'stopword' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bixEGnbIIa3M"
      },
      "source": [
        "#### Building custom attention layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp8YNv4UFCba"
      },
      "source": [
        "#%%writefile /content/drive/MyDrive/REVIEWS/model.py\n",
        "\n",
        "\n",
        "\n",
        "# importing necessary modules\n",
        "\n",
        "import emot\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import seaborn as sns\n",
        "import pandas as pd\n",
        "from gensim.models import FastText \n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense,Concatenate,TimeDistributed,Masking,GRU,Input,Dot,Reshape,Softmax\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Using NLTK for PoS tagging and Stopwords removal\n",
        "# Downloading the corresponding packages\n",
        "import pickle\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from gensim.models import FastText \n",
        "from random import seed\n",
        "from random import randint\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "embed_outputdim=100\n",
        "aspects_k=10 #number of aspects\n",
        "\n",
        "def embedding_layerinit():\n",
        "  model=training_vocab(embed_dim=embed_outputdim,negative_sampling=5,min_count=1,window=10,iter=250,sg=1,train_again=vocabtrain_again,return_model=True)\n",
        "  trained_weights=np.vstack((np.zeros((1,100)),model.wv.vectors))\n",
        "  return trained_weights,model\n",
        "\n",
        "def inputlength():\n",
        "  seq=textinputsequence_padding(padding=\"post\",padded_seqagain=train_again,return_paddedsequences=True)\n",
        "  return seq.shape[1]\n",
        "\n",
        "\n",
        "def aspectlayer_weightinit():\n",
        "  trained_weights=embedding_layerinit()[0]\n",
        "  kmeans = KMeans(n_clusters=aspects_k,random_state=0,max_iter=500,n_jobs=-1).fit(trained_weights) #clustering the trained wordembeddings into 10 clusters\n",
        "  init=tf.constant_initializer(kmeans.cluster_centers_) #used for initialing the weights of final dense layer\n",
        "  return init\n",
        "\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      #self.embed_outputdim=embed_outputdim\n",
        "      self.soft = tf.keras.layers.Softmax(axis=-2,name=\"softmax_att\") #softmax layer\n",
        "      self.dot=tf.keras.layers.Dot(axes=(-1,-1),name=\"dot_att\")       #dot layer\n",
        "      self.w = tf.Variable(\n",
        "            initial_value=tf.random_normal_initializer()(shape=(embed_outputdim,embed_outputdim), dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )              #weights that captures essense between the word embedding and global context vector(or average of all the word embedding of the sentence)\n",
        "  \n",
        "    def call(self,embed_output, mask=None):\n",
        "\n",
        "      ys = tf.reduce_mean(embed_output,axis=-2) #average of all word embeddings of the sentence\n",
        "      ys=tf.expand_dims(ys,axis=-2)\n",
        "      eW=tf.matmul(embed_output, self.w)\n",
        "      eW = eW * tf.expand_dims(tf.cast(mask,tf.float32),-1) #maskpropagation. Preventing masked elements into calculations\n",
        "      f=self.dot([eW, ys])\n",
        "      f = f+tf.expand_dims(tf.cast(tf.math.equal(mask, False), f.dtype)*-1e9,-1) #multiplying all the masked elements by -1e9 so that the softmax step do not impact the vectors\n",
        "      f=self.soft(f)\n",
        "      zs=tf.math.reduce_sum(f*embed_output,axis=-2) #zs is aspect embedding space after attention mechanisms on words of the sentence\n",
        "                                                    # f - softmax - gives info about unimportant words in the sentence for extracting aspects \n",
        "      \n",
        "      return zs,f\n",
        "\n",
        "\n",
        "# custom model\n",
        "\n",
        "class model_(tf.keras.Model):\n",
        "  def __init__(self,embed_outputdim,aspects_k):\n",
        "    super().__init__()\n",
        "    self.embed_inputdim=len(embedding_layerinit()[1].wv.vocab)\n",
        "    self.inputlength=inputlength()\n",
        "    self.embed_outputdim=embed_outputdim\n",
        "    self.trained_weights=embedding_layerinit()[0]\n",
        "    self.embedding=Embedding(input_dim=self.embed_inputdim+1,output_dim=self.embed_outputdim,mask_zero=True,\n",
        "                             input_length=self.inputlength,weights=[self.trained_weights],name=\"embedding_layer\",trainable=True) #embedding layer. Zero masking\n",
        "    self.attention=Attention()\n",
        "    self.aspects_k=aspects_k #number of aspects\n",
        "    self.init=aspectlayer_weightinit()\n",
        "    self.k = tf.keras.layers.Dense(aspects_k,name=\"dim_reduction_layer\",activation=\"softmax\")\n",
        "    self.dense= self.trained_weights.shape[1]\n",
        "    self.final=tf.keras.layers.Dense(self.dense,name=\"final_dense\",kernel_initializer=self.init) #weights are initialised with embedding clusters\n",
        "  def call(self,input):\n",
        "    e=self.embedding(input[0])\n",
        "    mask = self.embedding.compute_mask(input[0]) #computing mask so that this this can be used while propagating mask for subsequent layers\n",
        "    zs=self.attention(e, mask = mask)[0] #aspect vector\n",
        "    pt=self.k(zs)  #dimensionality vector\n",
        "    rs=self.final(pt) #reconstructed vector\n",
        "\n",
        "    # building regulariser to be used in the loss. [(T*transpose(T))-I]\n",
        "    reg=tf.tensordot(tf.linalg.normalize(self.final.weights[0],axis=1)[1],tf.linalg.normalize(self.final.weights[0],axis=1)[1],axes=[[1],[1]])\n",
        "    reg=reg-tf.ones([self.aspects_k,self.aspects_k])\n",
        "    reg=tf.norm(reg, ord='euclidean', axis=None, keepdims=None, name=None)\n",
        "    # calculating loss\n",
        "    r=tf.expand_dims(rs,-2)\n",
        "    f=tf.tensordot(rs,zs,[[0,1],[0,1]])\n",
        "    a=self.embedding(input[1])\n",
        "    a=tf.reduce_mean(a,axis=-2)\n",
        "    loss=tf.reduce_sum(tf.nn.relu(1-tf.reduce_sum(tf.tensordot(a,r,[[0,2],[0,2]]))+f))+1*reg #this is the loss which is to be minimised\n",
        "    return loss,pt,rs \n",
        "    \n",
        "    \n",
        "  def get_config(self):\n",
        "    config = {\n",
        "                  'embed_outputdim': self.embed_outputdim,\n",
        "                  'aspects_k' : self.aspects_k}\n",
        "    return config\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6JtRvyYcArt"
      },
      "source": [
        "#%%writefile /content/drive/MyDrive/REVIEWS/config.py\n",
        "from nltk.corpus import stopwords\n",
        "DATA_PATH=\"/content/drive/My Drive/REVIEWS/\"\n",
        "raw_file=\"reviews.pickle\"\n",
        "preprocessed_file=\"preprocessed_df.pickle\"\n",
        "trained_embeddings=\"trained_embeddings\"\n",
        "padded_seqfile=\"padded_sequences.pickle\"\n",
        "train_again=True\n",
        "vocabtrain_again=False\n",
        "stopword=stopwords.words(fileids=\"english\")\n",
        "embed_outputdim=100\n",
        "aspects_k=10 #number of aspects\n",
        "buffer_size=1024\n",
        "batch_size=100\n",
        "negative_samples=20\n",
        "WEIGHTS_PATH=\"/content/drive/My Drive/REVIEWS/Weights\"\n",
        "CHECKPOINTS_PATH=\"/content/drive/My Drive/REVIEWS/checkpoints\"\n",
        "lr=0.001\n",
        "iterations=15\n",
        "return_bestweightspath=True\n",
        "\n",
        "MODEL_CONFIG = {'embed_outputdim': embed_outputdim,\n",
        "                  'aspects_k' : aspects_k}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuqiB0fqI0B-"
      },
      "source": [
        "#### Building tensorflow dataset from generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVmKcEt1SXWN"
      },
      "source": [
        "def gendata():\n",
        "    seed(42)\n",
        "    for i in range(0,len(seq_texts)):\n",
        "      lis=[]\n",
        "      lent=[]\n",
        "      while len(lent)<negative_samples:\n",
        "        value = randint(0, len(seq_texts)-1)\n",
        "        if value==i:\n",
        "          continue\n",
        "        lis.append(seq_texts[value])\n",
        "        lent.append(value)\n",
        "      yield seq_texts[i],lis\n",
        "dataset=tf.data.Dataset.from_generator(gendata, output_types=(tf.int32,tf.int32))\n",
        "dataset=dataset.repeat(1).shuffle(buffer_size=1024).batch(100).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GwvIM_P4n-w"
      },
      "source": [
        "    with open(os.path.join(DATA_PATH,\"padded_sequences.pickle\"),\"rb\") as file:\n",
        "      seq_texts=pickle.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72dSrdejULaE"
      },
      "source": [
        "dataset=generate_dataset(buffer_size=1024,batch_size=100,negative_samples=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dt9__VhhTSP"
      },
      "source": [
        "import os\n",
        "os.makedirs(os.path.join(WEIGHTS_PATH,\"check.h5\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkrZfTM0I_S3"
      },
      "source": [
        "#### Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "BI7-IMtxE_vQ",
        "outputId": "4a573338-1893-4978-9783-ebaf6f4f09ec"
      },
      "source": [
        "\n",
        "#%%writefile /content/drive/MyDrive/REVIEWS/training.py\n",
        "import emot\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from gensim.models import FastText \n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense,Concatenate,TimeDistributed,Masking,GRU,Input,Dot,Reshape,Softmax\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Using NLTK for PoS tagging and Stopwords removal\n",
        "# Downloading the corresponding packages\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from gensim.models import FastText \n",
        "from random import seed\n",
        "from random import randint\n",
        "from sklearn.cluster import KMeans\n",
        "#from config import *\n",
        "#from model import model_\n",
        "\n",
        "WEIGHTS_PATH=\"/content/drive/My Drive/REVIEWS/Weights/\"\n",
        "CHECKPOINTS_PATH=\"/content/drive/My Drive/REVIEWS/checkpoints\"\n",
        "\n",
        "lr=0.001\n",
        "iterations=1\n",
        "return_bestweightspath=True \n",
        "\n",
        "def training(WEIGHTS_PATH,CHECKPOINTS_PATH,model_,lr=lr,iterations=iterations,return_bestweightspath=return_bestweightspath):\n",
        "  abae=model_(embed_outputdim,aspects_k)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "  @tf.function\n",
        "  def train_step(input):\n",
        "      with tf.GradientTape() as tape:\n",
        "        loss = abae(input)[0]\n",
        "      gradients = tape.gradient(loss, abae.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(gradients, abae.trainable_variables))\n",
        "      return loss, gradients\n",
        "  train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "##check point to save\n",
        "  ckpt = tf.train.Checkpoint(optimizer=optimizer, model=abae)\n",
        "  ckpt_manager = tf.train.CheckpointManager(ckpt, CHECKPOINTS_PATH, max_to_keep=3)\n",
        "  \n",
        "  loss_list=[]\n",
        "  weights_pathlist=[]\n",
        "  for k in range(0,iterations): # k - number of iterations\n",
        "    counter = 0\n",
        "\n",
        "  # navigating through each batch\n",
        "    for input in dataset:\n",
        "      loss_, gradients = train_step(input)\n",
        "      #adding loss to train loss\n",
        "      train_loss(loss_)\n",
        "      counter = counter + 1\n",
        "      template = '''Done {} step, Loss: {:0.6f}'''\n",
        "      if counter%500==0:\n",
        "        print(template.format(counter, train_loss.result()))\n",
        "\n",
        "    loss_list.append(train_loss.result()) #appending loss after every epoch\n",
        "    ckpt_save_path  = ckpt_manager.save() #checkpointing after every epoch\n",
        "    X=os.path.join(WEIGHTS_PATH,\"weights_epoch_\"+str(k+1))\n",
        "    abae.save_weights(X,save_format=\"h5\")\n",
        "\n",
        "    weights_pathlist.append(X)\n",
        "    \n",
        "    print(\"weights saved after epoch {}\".format(k+1))\n",
        "    print ('Saving checkpoint for iteration {} at {}'.format(k+1, ckpt_save_path))\n",
        "    print(counter, train_loss.result())\n",
        "    train_loss.reset_states()             #resetting loss after every epoch\n",
        "  if return_bestweightspath:\n",
        "    argminimum=np.argmin(loss_list)\n",
        "    return weights_pathlist[argminimum]  \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a7b432a32db8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#%%writefile /content/drive/MyDrive/REVIEWS/training.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0memot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'emot'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M00at2-AOZ-O"
      },
      "source": [
        "checkpoint_path=\"/content/drive/My Drive/REVIEWS/checkpoints\"\n",
        "abae=model_(embed_outputdim,aspects_k)\n",
        "latest = tf.train.latest_checkpoint(checkpoint_path)\n",
        "ckpt = tf.train.Checkpoint(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), model=abae)\n",
        "ckpt.restore(latest)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN_64DF0PfvC",
        "outputId": "d259176a-0bee-4dc1-ecfc-f5b4896028ba"
      },
      "source": [
        "model_.from_config(MODEL_CONFIG)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.model_ at 0x7fc4b6f72a50>"
            ]
          },
          "execution_count": 14,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3PLKEdNNaKD"
      },
      "source": [
        "inf=model_.from_config(MODEL_CONFIG)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ceid_30DWhtt"
      },
      "source": [
        "for i in generate_dataset().take(2):\n",
        "  k=i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_51mUR1Mo1Bw"
      },
      "source": [
        "inf(k)\n",
        "inf.load_weights(\"/content/drive/My Drive/REVIEWS/trainedWEIGHTS\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cpyirkTOc_pP",
        "outputId": "e783508b-e75a-4ab1-e395-013f534f0818"
      },
      "source": [
        "predict(k[0][1],\"/content/drive/My Drive/REVIEWS/trainedWEIGHTS\",model_,MODEL_CONFIG)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'belongs to topic_4'"
            ]
          },
          "execution_count": 50,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADo8mGN5d_M7"
      },
      "source": [
        "testsample=\"price is very reasonable\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3WG_LwIsgGN",
        "outputId": "16367994-f0c7-4c25-d26c-d4228afda073"
      },
      "source": [
        "  tkn=tokenisation_on_traindata(return_tkn=True)\n",
        "  word_countdict=tkn.word_counts \n",
        "  word_countdict.keys\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function OrderedDict.keys>"
            ]
          },
          "execution_count": 115,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ikm9OiSd3dN",
        "outputId": "424091b9-577f-4127-c924-74a2e1b39837"
      },
      "source": [
        "%%writefile /content/drive/MyDrive/REVIEWS/predict.py\n",
        "import emot\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from gensim.models import FastText \n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense,Concatenate,TimeDistributed,Masking,GRU,Input,Dot,Reshape,Softmax\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Using NLTK for PoS tagging and Stopwords removal\n",
        "# Downloading the corresponding packages\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from gensim.models import FastText \n",
        "from random import seed\n",
        "from random import randint\n",
        "from sklearn.cluster import KMeans\n",
        "from config import *\n",
        "from model import model_\n",
        "from preprocess import *\n",
        "from training import *\n",
        "\n",
        "\n",
        "\n",
        "def seq_gen(testsample,maxlen=223)  :\n",
        "  df=pd.DataFrame([testsample],columns=[\"review\"])\n",
        "  df[\"review\"]=df[\"review\"].apply(lambda x:x.strip(\"READ MORE\").lower())\n",
        "  df[\"review\"]=df[\"review\"].apply(convert_emojis)\n",
        "  df[\"review\"]=df[\"review\"].apply(decontractions)\n",
        "  df[\"review\"]=df[\"review\"].apply(removecharacters)\n",
        "  df[\"review\"]=pd.DataFrame(df[\"review\"].apply(lambda x:removestopwords(x,stopword)))\n",
        "  df[\"len\"]=df.review.str.split().apply(len)\n",
        "  df=df.loc[(df[\"len\"]>0) & (df[\"len\"]<maxlen+1)]\n",
        "  tkn=tokenisation_on_traindata(return_tkn=True)\n",
        "  word_countdict=tkn.word_counts  #word count dictionary\n",
        "  df[\"review\"]=pd.DataFrame(df[\"review\"].apply(lambda x:removenewords(x,word_countdict)))\n",
        "  df[\"review\"]=pd.DataFrame(df[\"review\"].apply(lambda x:word_count(x,word_countdict,10)))\n",
        "  \n",
        "  seq_texts=tkn.texts_to_sequences(df['review'].values)\n",
        "  print(seq_texts)\n",
        "  seq_texts=tf.keras.preprocessing.sequence.pad_sequences(seq_texts,\n",
        "                                                         maxlen=maxlen,\n",
        "                                                         padding='post')\n",
        "  return seq_texts\n",
        "\n",
        "def topicpredict(testsample,weight_path,model_,MODEL_CONFIG,maxlen=223):\n",
        "  k=tf.random.uniform(shape=[100,223],minval=1,maxval=38,dtype=tf.int32),tf.random.uniform(shape=[100,20,223],minval=1,maxval=38,dtype=tf.int32)\n",
        "  inf=model_.from_config(MODEL_CONFIG)\n",
        "  inf(k)\n",
        "  inf.load_weights(weight_path)\n",
        "  l=inf.layers\n",
        "  seq_text=seq_gen(testsample,maxlen=maxlen)\n",
        "  sample=seq_text.reshape(1,maxlen)\n",
        "  endsample=l[0](sample)\n",
        "  mask=l[0].compute_mask(sample)\n",
        "  att=l[1](endsample,mask)[0]  \n",
        "  topic_num=np.argmax(l[2](att))+1\n",
        "  return \"belongs to topic_\"+str(topic_num),l[2](att)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/drive/MyDrive/REVIEWS/predict.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQiocvnMmg-h"
      },
      "source": [
        "sample=k[0][1].numpy().reshape(1,223)\n",
        "emb=l[0](sample)\n",
        "mask=l[0].compute_mask(sample)\n",
        "att=l[1](emb,mask)[0]\n",
        "pt=l[2](att)\n",
        "pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "__O93C2aavn_",
        "outputId": "2be88d88-13cf-4f64-f847-506254f59461"
      },
      "source": [
        "topicpredict(\"gaming tho acha hai\",\"/content/drive/My Drive/REVIEWS/trainedWEIGHTS\",model_,MODEL_CONFIG,maxlen=223)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-0f66fd205693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtopicpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gaming tho acha hai\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/content/drive/My Drive/REVIEWS/trainedWEIGHTS\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMODEL_CONFIG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m223\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'topicpredict' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u2st5kyDJFX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z527uyZUJKKM",
        "outputId": "94a7139a-3d61-4210-ab54-9e28254aad0f"
      },
      "source": [
        "l=inf.layers\n",
        "l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.layers.embeddings.Embedding at 0x7fc4b6f84f50>,\n",
              " <__main__.Attention at 0x7fc4b6f84e90>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7fc4d0d2fad0>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7fc4c625c750>]"
            ]
          },
          "execution_count": 130,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW8vBigGBM1c",
        "outputId": "8cbf3ca5-b2fc-4276-952e-c54feb3204ef"
      },
      "source": [
        "sample=k[0][1].numpy().reshape(1,223)\n",
        "emb=l[0](sample)\n",
        "mask=l[0].compute_mask(sample)\n",
        "att=l[1](emb,mask)[0]\n",
        "pt=l[2](att)\n",
        "pt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
              "array([[0.07538455, 0.11304139, 0.10993138, 0.104953  , 0.09605172,\n",
              "        0.1072276 , 0.0917433 , 0.08850341, 0.12209129, 0.09107236]],\n",
              "      dtype=float32)>"
            ]
          },
          "execution_count": 146,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LwktobgByfN",
        "outputId": "9e82e573-6e97-4002-81bb-1d0fb3cbfc48"
      },
      "source": [
        "emdsample=inf.layers[0](sample)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 223, 100), dtype=float32, numpy=\n",
              "array([[[ 0.28434345,  0.3539412 , -0.03884524, ...,  0.08158847,\n",
              "          0.06795241, -0.03691028],\n",
              "        [ 0.09312745,  0.0665113 , -0.01089542, ...,  0.01330447,\n",
              "         -0.25797328,  0.03294387],\n",
              "        [-0.2916876 , -0.02874304,  0.01857883, ...,  0.14095038,\n",
              "         -0.0856531 , -0.15893033],\n",
              "        ...,\n",
              "        [ 0.01035531,  0.01036448,  0.01037198, ...,  0.01034254,\n",
              "         -0.01036166,  0.01037487],\n",
              "        [ 0.01035531,  0.01036448,  0.01037198, ...,  0.01034254,\n",
              "         -0.01036166,  0.01037487],\n",
              "        [ 0.01035531,  0.01036448,  0.01037198, ...,  0.01034254,\n",
              "         -0.01036166,  0.01037487]]], dtype=float32)>"
            ]
          },
          "execution_count": 35,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5__llp90Y7hQ",
        "outputId": "7e83915c-6d16-46c5-9658-501a63a8e229"
      },
      "source": [
        "att=inf.layers[1](emdsample)[0]\n",
        "att"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 100), dtype=float32, numpy=\n",
              "array([[-0.03784817,  0.04932524,  0.00909207, -0.03855411,  0.03189919,\n",
              "        -0.05768111,  0.28769803, -0.0539141 , -0.2119304 ,  0.13761231,\n",
              "        -0.19599375,  0.15078266,  0.12766425, -0.15458861,  0.05844464,\n",
              "         0.00605304,  0.08107603, -0.13156097, -0.0095018 ,  0.07236446,\n",
              "         0.0761936 ,  0.00411951, -0.18544461, -0.04545902, -0.0270914 ,\n",
              "         0.13593271, -0.22507998,  0.06798376, -0.13280551,  0.01041025,\n",
              "         0.05213068,  0.17390668,  0.202806  ,  0.14431584,  0.14312342,\n",
              "         0.1353922 ,  0.02476707,  0.06251456,  0.12323368, -0.00152758,\n",
              "         0.03922736, -0.18883106,  0.13550267,  0.12107454, -0.04419479,\n",
              "         0.2056097 ,  0.01607932, -0.14294998, -0.0288608 ,  0.11756141,\n",
              "         0.01950871, -0.14338823,  0.07907163, -0.14048326, -0.01465124,\n",
              "         0.0114028 , -0.1232634 , -0.13779134, -0.05030006,  0.05148183,\n",
              "         0.00965279, -0.04693422,  0.05502408, -0.14524607,  0.02292041,\n",
              "        -0.16702779, -0.09032408,  0.02720484,  0.30139408, -0.11645963,\n",
              "        -0.2458573 , -0.38309753, -0.09785175,  0.00813603,  0.25034863,\n",
              "         0.0425318 , -0.02425337,  0.01404067, -0.01353701, -0.33607188,\n",
              "         0.17981254, -0.11140373, -0.03443918,  0.0337606 , -0.05809933,\n",
              "         0.07881187,  0.01351597, -0.1311582 ,  0.16282997,  0.09425178,\n",
              "        -0.23753943, -0.02256316, -0.09242329, -0.03884177,  0.07662644,\n",
              "        -0.03446334, -0.03226683,  0.11157088, -0.1434564 ,  0.03651715]],\n",
              "      dtype=float32)>"
            ]
          },
          "execution_count": 40,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVg3CrzZahp0",
        "outputId": "6056a9cf-3299-4888-b5ef-83a83ba13c99"
      },
      "source": [
        "inf.layers[2](att)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
              "array([[0.0852352 , 0.13147014, 0.0958802 , 0.11055278, 0.08819757,\n",
              "        0.09580411, 0.08448124, 0.09142829, 0.11823527, 0.09871526]],\n",
              "      dtype=float32)>"
            ]
          },
          "execution_count": 101,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DUy4oWHB7Ao"
      },
      "source": [
        "test.load_weights(os.path.join(DATA_PATH,\"_newWeights\",str(1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9JNpuaOCCp1",
        "outputId": "1469b732-12b0-4a44-e2fa-0da04410de6a"
      },
      "source": [
        "test(k)[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.010878837>"
            ]
          },
          "execution_count": 72,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yjw5lqFKi7G"
      },
      "source": [
        "abae.save_weights(os.path.join(DATA_PATH,\"_newWeights\"),save_format=\"h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwLfrjc3JFkU"
      },
      "source": [
        "#### Loss - plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "IPG-sV2kM6HB",
        "outputId": "04183a83-4062-4147-988d-efdc2300cf2f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(loss_list[1:])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dU9_GOC5W8P",
        "outputId": "ecebe94a-bf77-4bd6-e4b4-efc440c4465e"
      },
      "source": [
        "abae.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model__1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_layer (Embedding)  multiple                  699800    \n",
            "_________________________________________________________________\n",
            "attention_1 (Attention)      multiple                  10000     \n",
            "_________________________________________________________________\n",
            "dim_reduction_layer (Dense)  multiple                  1010      \n",
            "_________________________________________________________________\n",
            "final_dense (Dense)          multiple                  1100      \n",
            "=================================================================\n",
            "Total params: 711,910\n",
            "Trainable params: 711,910\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZL3E3W4M5FT",
        "outputId": "b5a71cc9-86e0-42cb-b6b3-4902279d0e30"
      },
      "source": [
        "for i in abae.weights:\n",
        "  print(i.name, i.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model__1/embedding_layer/embeddings:0 (6998, 100)\n",
            "Variable:0 (100, 100)\n",
            "model__1/dim_reduction_layer/kernel:0 (100, 10)\n",
            "model__1/dim_reduction_layer/bias:0 (10,)\n",
            "model__1/final_dense/kernel:0 (10, 100)\n",
            "model__1/final_dense/bias:0 (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2YfYl7XYhm9"
      },
      "source": [
        "from gensim.models import FastText\n",
        "model=FastText.load(\"/content/drive/My Drive/REVIEWS/trained_embeddings\") #reusing already trained model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_DQk1GL4ljg",
        "outputId": "37650396-0064-4182-aa3f-2af47eeedc38"
      },
      "source": [
        "!python \"/content/drive/MyDrive/REVIEWS/training.py\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021-08-16 05:21:36.256373: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx5y77bAUJto",
        "outputId": "b20f0659-bfc4-45eb-abcc-53de5958b41e"
      },
      "source": [
        "!python \"/content/drive/MyDrive/REVIEWS/predict.py\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021-08-16 12:01:30.684009: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "battery back up is very good\n",
            "2021-08-16 12:01:40.623134: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-08-16 12:01:40.636830: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-08-16 12:01:40.636893: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (74ab1ff4eadb): /proc/driver/nvidia/version does not exist\n",
            "2021-08-16 12:02:26.758377: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom/weights: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n",
            "[[7, 47, 1]]\n",
            "('topic number - 6', <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
            "array([[0.07620194, 0.1129895 , 0.09598941, 0.10272404, 0.08342132,\n",
            "        0.1216599 , 0.09646794, 0.08961868, 0.11447812, 0.10644924]],\n",
            "      dtype=float32)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "TDdfM6jYbuOf",
        "outputId": "365a8f8b-1af2-4b11-b523-ab96ce3e92b9"
      },
      "source": [
        "topics=inf.weights[4].numpy()  \n",
        "topic_words={}\n",
        "k=1\n",
        "for i in range(len(topics)):\n",
        "  topic_words[\"topic_\"+str(k)]=np.argsort(model.wv.cosine_similarities(topics[i],model.wv.vectors))[::-1][0:20]\n",
        "  k=k+1\n",
        "pd.DataFrame(topic_words).applymap(lambda x:model.wv.index2word[x])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-0a27adbd08b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtopics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtopic_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtopic_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"topic_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'inf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq3txHX1yb3_"
      },
      "source": [
        "import json\n",
        "topics_inf={}\n",
        "topics_inf[1]=\"price and sale\"\n",
        "topics_inf[2]=\"processors\"\n",
        "topics_inf[3]=\"camera and pictures\"\n",
        "topics_inf[4]=\"apps and notifications\"\n",
        "topics_inf[5]=\"misspelt and positive words\"\n",
        "topics_inf[6]=\"coparision\"\n",
        "topics_inf[7]=\"physical design and experience\"\n",
        "topics_inf[8]=\"hindi word\"\n",
        "topics_inf[9]=\"delivery\"\n",
        "topics_inf[10]=\"premium phones\"\n",
        "with open(\"/content/drive/My Drive/REVIEWS/topics_inf.json\",\"w+\") as file:\n",
        "  json.dump(topics_inf,file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JieqInGrydsb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "fldSm4OyyYri",
        "outputId": "7ddb0eba-eae4-4d3b-ae8b-0ccb0a658f87"
      },
      "source": [
        "topics=inf.weights[4].numpy()  \n",
        "topic_words={}\n",
        "k=1\n",
        "for i in range(len(topics)):\n",
        "  topic_words[\"topic_\"+str(k)]=np.argsort(model.wv.cosine_similarities(topics[i],model.wv.vectors))[::-1][0:20]\n",
        "  k=k+1\n",
        "pd.DataFrame(topic_words).applymap(lambda x:model.wv.index2word[x])  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic_1</th>\n",
              "      <th>topic_2</th>\n",
              "      <th>topic_3</th>\n",
              "      <th>topic_4</th>\n",
              "      <th>topic_5</th>\n",
              "      <th>topic_6</th>\n",
              "      <th>topic_7</th>\n",
              "      <th>topic_8</th>\n",
              "      <th>topic_9</th>\n",
              "      <th>topic_10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rs</td>\n",
              "      <td>played</td>\n",
              "      <td>cameras</td>\n",
              "      <td>notifications</td>\n",
              "      <td>nice</td>\n",
              "      <td>far</td>\n",
              "      <td>body</td>\n",
              "      <td>ke</td>\n",
              "      <td>cancel</td>\n",
              "      <td>s9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>offer</td>\n",
              "      <td>games</td>\n",
              "      <td>depth</td>\n",
              "      <td>app</td>\n",
              "      <td>xilent</td>\n",
              "      <td>think</td>\n",
              "      <td>case</td>\n",
              "      <td>hai</td>\n",
              "      <td>deliver</td>\n",
              "      <td>obviously</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>discount</td>\n",
              "      <td>graphics</td>\n",
              "      <td>images</td>\n",
              "      <td>exit</td>\n",
              "      <td>osm</td>\n",
              "      <td>comparison</td>\n",
              "      <td>thin</td>\n",
              "      <td>liye</td>\n",
              "      <td>delivered</td>\n",
              "      <td>terms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>discounts</td>\n",
              "      <td>runs</td>\n",
              "      <td>portrait</td>\n",
              "      <td>messages</td>\n",
              "      <td>vry</td>\n",
              "      <td>really</td>\n",
              "      <td>design</td>\n",
              "      <td>bhi</td>\n",
              "      <td>order</td>\n",
              "      <td>absolutely</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>price</td>\n",
              "      <td>almost</td>\n",
              "      <td>capture</td>\n",
              "      <td>operations</td>\n",
              "      <td>nyc</td>\n",
              "      <td>aspects</td>\n",
              "      <td>fits</td>\n",
              "      <td>h</td>\n",
              "      <td>informed</td>\n",
              "      <td>s8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>deal</td>\n",
              "      <td>90</td>\n",
              "      <td>captures</td>\n",
              "      <td>apps</td>\n",
              "      <td>sm</td>\n",
              "      <td>actually</td>\n",
              "      <td>grip</td>\n",
              "      <td>hota</td>\n",
              "      <td>address</td>\n",
              "      <td>beast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>discounted</td>\n",
              "      <td>heavy</td>\n",
              "      <td>daylight</td>\n",
              "      <td>application</td>\n",
              "      <td>lovly</td>\n",
              "      <td>trust</td>\n",
              "      <td>slippery</td>\n",
              "      <td>acha</td>\n",
              "      <td>promised</td>\n",
              "      <td>flagships</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>12000</td>\n",
              "      <td>playing</td>\n",
              "      <td>lens</td>\n",
              "      <td>restart</td>\n",
              "      <td>good</td>\n",
              "      <td>one</td>\n",
              "      <td>feels</td>\n",
              "      <td>ki</td>\n",
              "      <td>assured</td>\n",
              "      <td>flagship</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>999</td>\n",
              "      <td>gaming</td>\n",
              "      <td>rear</td>\n",
              "      <td>applications</td>\n",
              "      <td>nais</td>\n",
              "      <td>better</td>\n",
              "      <td>silicon</td>\n",
              "      <td>nhi</td>\n",
              "      <td>asked</td>\n",
              "      <td>barring</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>13000</td>\n",
              "      <td>pubg</td>\n",
              "      <td>wide</td>\n",
              "      <td>whatsapp</td>\n",
              "      <td>super</td>\n",
              "      <td>believe</td>\n",
              "      <td>looks</td>\n",
              "      <td>accha</td>\n",
              "      <td>customer</td>\n",
              "      <td>amazing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>7000</td>\n",
              "      <td>handles</td>\n",
              "      <td>camera</td>\n",
              "      <td>able</td>\n",
              "      <td>swm</td>\n",
              "      <td>compared</td>\n",
              "      <td>handy</td>\n",
              "      <td>jyada</td>\n",
              "      <td>delivery</td>\n",
              "      <td>iphones</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>19000</td>\n",
              "      <td>extreme</td>\n",
              "      <td>front</td>\n",
              "      <td>browsing</td>\n",
              "      <td>gjb</td>\n",
              "      <td>still</td>\n",
              "      <td>slim</td>\n",
              "      <td>ye</td>\n",
              "      <td>cancelled</td>\n",
              "      <td>s10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>billion</td>\n",
              "      <td>medium</td>\n",
              "      <td>pictures</td>\n",
              "      <td>music</td>\n",
              "      <td>profomance</td>\n",
              "      <td>compare</td>\n",
              "      <td>plastic</td>\n",
              "      <td>kam</td>\n",
              "      <td>executives</td>\n",
              "      <td>shifted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>8100</td>\n",
              "      <td>50</td>\n",
              "      <td>shots</td>\n",
              "      <td>function</td>\n",
              "      <td>osom</td>\n",
              "      <td>definitely</td>\n",
              "      <td>cover</td>\n",
              "      <td>bahut</td>\n",
              "      <td>said</td>\n",
              "      <td>loving</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5500</td>\n",
              "      <td>100</td>\n",
              "      <td>aperture</td>\n",
              "      <td>icon</td>\n",
              "      <td>awesome</td>\n",
              "      <td>terms</td>\n",
              "      <td>glass</td>\n",
              "      <td>nahi</td>\n",
              "      <td>bangalore</td>\n",
              "      <td>absolute</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>diwali</td>\n",
              "      <td>lasts</td>\n",
              "      <td>angle</td>\n",
              "      <td>menu</td>\n",
              "      <td>also</td>\n",
              "      <td>plus</td>\n",
              "      <td>curved</td>\n",
              "      <td>ka</td>\n",
              "      <td>ekart</td>\n",
              "      <td>great</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>offers</td>\n",
              "      <td>combat</td>\n",
              "      <td>captured</td>\n",
              "      <td>sometimes</td>\n",
              "      <td>bt</td>\n",
              "      <td>phones</td>\n",
              "      <td>hands</td>\n",
              "      <td>jada</td>\n",
              "      <td>date</td>\n",
              "      <td>aspects</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18999</td>\n",
              "      <td>lag</td>\n",
              "      <td>selfie</td>\n",
              "      <td>browser</td>\n",
              "      <td>betray</td>\n",
              "      <td>awesome</td>\n",
              "      <td>thick</td>\n",
              "      <td>lekin</td>\n",
              "      <td>seller</td>\n",
              "      <td>way</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>rupees</td>\n",
              "      <td>usage</td>\n",
              "      <td>produces</td>\n",
              "      <td>wifi</td>\n",
              "      <td>love</td>\n",
              "      <td>comparing</td>\n",
              "      <td>bigger</td>\n",
              "      <td>mai</td>\n",
              "      <td>sent</td>\n",
              "      <td>top</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>competitive</td>\n",
              "      <td>smooth</td>\n",
              "      <td>saturated</td>\n",
              "      <td>disable</td>\n",
              "      <td>nd</td>\n",
              "      <td>great</td>\n",
              "      <td>beautiful</td>\n",
              "      <td>ko</td>\n",
              "      <td>courier</td>\n",
              "      <td>oneplus</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        topic_1   topic_2    topic_3  ... topic_8     topic_9    topic_10\n",
              "0            rs    played    cameras  ...      ke      cancel          s9\n",
              "1         offer     games      depth  ...     hai     deliver   obviously\n",
              "2      discount  graphics     images  ...    liye   delivered       terms\n",
              "3     discounts      runs   portrait  ...     bhi       order  absolutely\n",
              "4         price    almost    capture  ...       h    informed          s8\n",
              "5          deal        90   captures  ...    hota     address       beast\n",
              "6    discounted     heavy   daylight  ...    acha    promised   flagships\n",
              "7         12000   playing       lens  ...      ki     assured    flagship\n",
              "8           999    gaming       rear  ...     nhi       asked     barring\n",
              "9         13000      pubg       wide  ...   accha    customer     amazing\n",
              "10         7000   handles     camera  ...   jyada    delivery     iphones\n",
              "11        19000   extreme      front  ...      ye   cancelled         s10\n",
              "12      billion    medium   pictures  ...     kam  executives     shifted\n",
              "13         8100        50      shots  ...   bahut        said      loving\n",
              "14         5500       100   aperture  ...    nahi   bangalore    absolute\n",
              "15       diwali     lasts      angle  ...      ka       ekart       great\n",
              "16       offers    combat   captured  ...    jada        date     aspects\n",
              "17        18999       lag     selfie  ...   lekin      seller         way\n",
              "18       rupees     usage   produces  ...     mai        sent         top\n",
              "19  competitive    smooth  saturated  ...      ko     courier     oneplus\n",
              "\n",
              "[20 rows x 10 columns]"
            ]
          },
          "execution_count": 107,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9H2nJKnJQeO"
      },
      "source": [
        "#### Aspects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMQgP2EbPokE"
      },
      "source": [
        "### Restoring the training of a model from the latest checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Jz-yd8nw6Rv"
      },
      "source": [
        "#data generating\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "DATA_PATH=\"/content/drive/My Drive/REVIEWS/\"\n",
        "raw_file=\"reviews.pickle\"\n",
        "preprocessed_file=\"preprocessed_df.pickle\"\n",
        "trained_embeddings=\"trained_embeddings\"\n",
        "padded_seqfile=\"padded_sequences.pickle\"\n",
        "tokenfile=\"token.pickle\"\n",
        "train_again=True\n",
        "vocabtrain_again=False\n",
        "stopword=stopwords.words(fileids=\"english\")\n",
        "embed_outputdim=100\n",
        "aspects_k=10 #number of aspects\n",
        "buffer_size=1024\n",
        "batch_size=100\n",
        "negative_samples=20\n",
        "WEIGHTS_PATH=\"/content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom/WeightsV2\"\n",
        "weight_path=\"/content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom/WeightsV2/weights_epoch_12\"\n",
        "CHECKPOINTS_PATH=\"/content/drive/My Drive/REVIEWS/checkpointsV2\"\n",
        "lr=0.001\n",
        "iterations=15\n",
        "return_bestweightspath=True\n",
        "lamda=0.5\n",
        "MODEL_CONFIG = {'embed_outputdim': embed_outputdim,\n",
        "                  'aspects_k' : aspects_k}\n",
        "with open(os.path.join(DATA_PATH,\"padded_sequences.pickle\"),\"rb\") as file:\n",
        "  seq_texts=pickle.load(file)\n",
        "from random import seed\n",
        "from random import randint\n",
        "\n",
        "def gendata():\n",
        "  seed(42)\n",
        "  for i in range(0,len(seq_texts)):\n",
        "    lis=[]\n",
        "    lent=[]\n",
        "    while len(lent)<20:\n",
        "      value = randint(0, len(seq_texts)-1)\n",
        "      if value==i:\n",
        "        continue\n",
        "      lis.append(seq_texts[value])\n",
        "      lent.append(value)\n",
        "\n",
        "    yield seq_texts[i],lis\n",
        "datagen=tf.data.Dataset.from_generator(gendata, output_types=(tf.int32,tf.int32))\n",
        "datagen=datagen.repeat(1).shuffle(buffer_size=1024).batch(100).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "import io\n",
        "for i in datagen.take(1):\n",
        "  k=i\n",
        "test=model_(embed_outputdim,aspects_k)\n",
        "_=test(i)\n",
        "test.load_weights(WEIGHTS_PATH)\n",
        "topics=test.weights[4].numpy()\n",
        "topic_words={}\n",
        "k=1\n",
        "for i in range(len(topics)):\n",
        "  topic_words[\"Aspect_\"+str(k)]=np.argsort(model.wv.cosine_similarities(topics[i],model.wv.vectors))[::-1][0:30]\n",
        "  k=k+1\n",
        "pd.DataFrame(topic_words).applymap(lambda x:model.wv.index2word[x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8LH1XnJ5BN8"
      },
      "source": [
        "WEIGHTS_PATH=\"/content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom/WeightsV2/weights_epoch_12\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNLfvqZwfz6a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-mUED4CPkAo"
      },
      "source": [
        "import io\n",
        "for i in datagen.take(1):\n",
        "  k=i\n",
        "test=model_(embed_outputdim,aspects_k)\n",
        "_=test(i)\n",
        "test.load_weights(WEIGHTS_PATH)\n",
        "topics=test.weights[4].numpy()\n",
        "topic_words={}\n",
        "k=1\n",
        "for i in range(len(topics)):\n",
        "  topic_words[\"Aspect_\"+str(k)]=np.argsort(model.wv.cosine_similarities(topics[i],model.wv.vectors))[::-1][0:30]\n",
        "  k=k+1\n",
        "pd.DataFrame(topic_words).applymap(lambda x:model.wv.index2word[x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTSGDNLFsqXw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_cYS1J5oI3D",
        "outputId": "24b1f360-32de-472e-8eb0-5d7ec0ce8332"
      },
      "source": [
        "#model initialisation\n",
        "#abae=model_(embed_inputdim,embed_outputdim,trained_weights,aspects_k,dense,inputlength)\n",
        "# optimiser\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "@tf.function\n",
        "def train_step(input):\n",
        "    with tf.GradientTape() as tape:\n",
        "        #forward propagation\n",
        "        loss = test(input)[0]\n",
        "        #print(loss)\n",
        "\n",
        "    #getting gradients\n",
        "    gradients = tape.gradient(loss, test.trainable_variables)\n",
        "    #applying gradients\n",
        "    optimizer.apply_gradients(zip(gradients, test.trainable_variables))\n",
        "\n",
        "    return loss, gradients\n",
        "#no_iterations=1147*5          #epochs\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "\n",
        "checkpoint_path = \"/content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom\"\n",
        "\n",
        "latest = tf.train.latest_checkpoint(checkpoint_path)\n",
        "ckpt = tf.train.Checkpoint(optimizer=optimizer, model=test)\n",
        "ckpt.restore(latest)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
        "\n",
        "#ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
        "\n",
        "\n",
        "##check point to save\n",
        "#checkpoint_path = \"/content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom\"\n",
        "#ckpt = tf.train.Checkpoint(optimizer=optimizer, model=test)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
        "\n",
        "\n",
        "it=0\n",
        "loss_list=[]\n",
        "for k in range(0,10):\n",
        "  counter = 0\n",
        "  for input in datagen:\n",
        "\n",
        "      loss_, gradients = train_step(input)\n",
        "      #adding loss to train loss\n",
        "      train_loss(loss_)\n",
        "      counter = counter + 1\n",
        "      template = '''Done {} step, Loss: {:0.6f}'''\n",
        "\n",
        "    \n",
        "      if counter%100==0:\n",
        "        print(template.format(counter, train_loss.result()))\n",
        "        \n",
        "  loss_list.append(train_loss.result())\n",
        "  test.save_weights(\"/content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom/weights\",save_format=\"h5\")\n",
        "  ckpt_save_path  = ckpt_manager.save()\n",
        "  print ('Saving checkpoint for iteration {} at {}'.format(k+1, ckpt_save_path))\n",
        "  print(counter, train_loss.result())\n",
        "  train_loss.reset_states()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done 100 step, Loss: 0.001681\n",
            "Done 200 step, Loss: 0.001739\n",
            "Done 300 step, Loss: 0.001915\n",
            "Done 400 step, Loss: 0.002092\n",
            "Done 500 step, Loss: 0.002253\n",
            "Done 600 step, Loss: 0.002400\n",
            "Done 700 step, Loss: 0.002532\n",
            "Done 800 step, Loss: 0.002652\n",
            "Done 900 step, Loss: 0.002762\n",
            "Done 1000 step, Loss: 0.002862\n",
            "Done 1100 step, Loss: 0.002954\n",
            "Done 1200 step, Loss: 0.003038\n",
            "Done 1300 step, Loss: 0.003117\n",
            "Done 1400 step, Loss: 0.003189\n",
            "Done 1500 step, Loss: 0.003257\n",
            "Done 1600 step, Loss: 0.003319\n",
            "Done 1700 step, Loss: 0.003378\n",
            "Done 1800 step, Loss: 0.003433\n",
            "Done 1900 step, Loss: 0.003484\n",
            "Done 2000 step, Loss: 0.003532\n",
            "Done 2100 step, Loss: 0.003577\n",
            "Done 2200 step, Loss: 0.003619\n",
            "Done 2300 step, Loss: 0.003659\n",
            "Done 2400 step, Loss: 0.003697\n",
            "Done 2500 step, Loss: 0.003732\n",
            "Done 2600 step, Loss: 0.003766\n",
            "Done 2700 step, Loss: 0.003798\n",
            "Done 2800 step, Loss: 0.003828\n",
            "Done 2900 step, Loss: 0.003857\n",
            "Done 3000 step, Loss: 0.003884\n",
            "Done 3100 step, Loss: 0.003910\n",
            "Done 3200 step, Loss: 0.003934\n",
            "Done 3300 step, Loss: 0.003957\n",
            "Done 3400 step, Loss: 0.003980\n",
            "Done 3500 step, Loss: 0.004001\n",
            "Done 3600 step, Loss: 0.004021\n",
            "Saving checkpoint for iteration 1 at /content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom/ckpt-13\n",
            "3631 tf.Tensor(0.0040271506, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.004761\n",
            "Done 200 step, Loss: 0.004732\n",
            "Done 300 step, Loss: 0.004747\n",
            "Done 400 step, Loss: 0.004743\n",
            "Done 500 step, Loss: 0.004751\n",
            "Done 600 step, Loss: 0.004750\n",
            "Done 700 step, Loss: 0.004749\n",
            "Done 800 step, Loss: 0.004755\n",
            "Done 900 step, Loss: 0.004754\n",
            "Done 1000 step, Loss: 0.004753\n",
            "Done 1100 step, Loss: 0.004758\n",
            "Done 1200 step, Loss: 0.004758\n",
            "Done 1300 step, Loss: 0.004761\n",
            "Done 1400 step, Loss: 0.004761\n",
            "Done 1500 step, Loss: 0.004762\n",
            "Done 1600 step, Loss: 0.004761\n",
            "Done 1700 step, Loss: 0.004764\n",
            "Done 1800 step, Loss: 0.004765\n",
            "Done 1900 step, Loss: 0.004766\n",
            "Done 2000 step, Loss: 0.004766\n",
            "Done 2100 step, Loss: 0.004768\n",
            "Done 2200 step, Loss: 0.004768\n",
            "Done 2300 step, Loss: 0.004768\n",
            "Done 2400 step, Loss: 0.004767\n",
            "Done 2500 step, Loss: 0.004769\n",
            "Done 2600 step, Loss: 0.004768\n",
            "Done 2700 step, Loss: 0.004771\n",
            "Done 2800 step, Loss: 0.004770\n",
            "Done 2900 step, Loss: 0.004771\n",
            "Done 3000 step, Loss: 0.004772\n",
            "Done 3100 step, Loss: 0.004772\n",
            "Done 3200 step, Loss: 0.004771\n",
            "Done 3300 step, Loss: 0.004772\n",
            "Done 3400 step, Loss: 0.004772\n",
            "Done 3500 step, Loss: 0.004773\n",
            "Done 3600 step, Loss: 0.004773\n",
            "Saving checkpoint for iteration 2 at /content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom/ckpt-14\n",
            "3631 tf.Tensor(0.0047727195, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.004779\n",
            "Done 200 step, Loss: 0.004780\n",
            "Done 300 step, Loss: 0.004780\n",
            "Done 400 step, Loss: 0.004779\n",
            "Done 500 step, Loss: 0.004779\n",
            "Done 600 step, Loss: 0.004779\n",
            "Done 700 step, Loss: 0.004779\n",
            "Done 800 step, Loss: 0.004779\n",
            "Done 900 step, Loss: 0.004778\n",
            "Done 1000 step, Loss: 0.004778\n",
            "Done 1100 step, Loss: 0.004779\n",
            "Done 1200 step, Loss: 0.004778\n",
            "Done 1300 step, Loss: 0.004778\n",
            "Done 1400 step, Loss: 0.004778\n",
            "Done 1500 step, Loss: 0.004778\n",
            "Done 1600 step, Loss: 0.004778\n",
            "Done 1700 step, Loss: 0.004778\n",
            "Done 1800 step, Loss: 0.004778\n",
            "Done 1900 step, Loss: 0.004775\n",
            "Done 2000 step, Loss: 0.004775\n",
            "Done 2100 step, Loss: 0.004776\n",
            "Done 2200 step, Loss: 0.004776\n",
            "Done 2300 step, Loss: 0.004776\n",
            "Done 2400 step, Loss: 0.004776\n",
            "Done 2500 step, Loss: 0.004776\n",
            "Done 2600 step, Loss: 0.004776\n",
            "Done 2700 step, Loss: 0.004776\n",
            "Done 2800 step, Loss: 0.004776\n",
            "Done 2900 step, Loss: 0.004775\n",
            "Done 3000 step, Loss: 0.004775\n",
            "Done 3100 step, Loss: 0.004775\n",
            "Done 3200 step, Loss: 0.004775\n",
            "Done 3300 step, Loss: 0.004775\n",
            "Done 3400 step, Loss: 0.004775\n",
            "Done 3500 step, Loss: 0.004775\n",
            "Done 3600 step, Loss: 0.004775\n",
            "Saving checkpoint for iteration 3 at /content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom/ckpt-15\n",
            "3631 tf.Tensor(0.0047736294, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.004835\n",
            "Done 200 step, Loss: 0.004805\n",
            "Done 300 step, Loss: 0.004795\n",
            "Done 400 step, Loss: 0.004788\n",
            "Done 500 step, Loss: 0.004785\n",
            "Done 600 step, Loss: 0.004783\n",
            "Done 700 step, Loss: 0.004781\n",
            "Done 800 step, Loss: 0.004779\n",
            "Done 900 step, Loss: 0.004778\n",
            "Done 1000 step, Loss: 0.004777\n",
            "Done 1100 step, Loss: 0.004776\n",
            "Done 1200 step, Loss: 0.004775\n",
            "Done 1300 step, Loss: 0.004774\n",
            "Done 1400 step, Loss: 0.004801\n",
            "Done 1500 step, Loss: 0.005051\n",
            "Done 1600 step, Loss: 0.005280\n",
            "Done 1700 step, Loss: 0.005485\n",
            "Done 1800 step, Loss: 0.005663\n",
            "Done 1900 step, Loss: 0.005822\n",
            "Done 2000 step, Loss: 0.005965\n",
            "Done 2100 step, Loss: 0.006097\n",
            "Done 2200 step, Loss: 0.006212\n",
            "Done 2300 step, Loss: 0.006320\n",
            "Done 2400 step, Loss: 0.006417\n",
            "Done 2500 step, Loss: 0.006508\n",
            "Done 2600 step, Loss: 0.006592\n",
            "Done 2700 step, Loss: 0.006669\n",
            "Done 2800 step, Loss: 0.006740\n",
            "Done 2900 step, Loss: 0.006808\n",
            "Done 3000 step, Loss: 0.006871\n",
            "Done 3100 step, Loss: 0.006926\n",
            "Done 3200 step, Loss: 0.006981\n",
            "Done 3300 step, Loss: 0.007031\n",
            "Done 3400 step, Loss: 0.007079\n",
            "Done 3500 step, Loss: 0.007125\n",
            "Done 3600 step, Loss: 0.007168\n",
            "Saving checkpoint for iteration 4 at /content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom/ckpt-16\n",
            "3631 tf.Tensor(0.0071797273, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.008722\n",
            "Done 200 step, Loss: 0.008638\n",
            "Done 300 step, Loss: 0.008660\n",
            "Done 400 step, Loss: 0.008662\n",
            "Done 500 step, Loss: 0.008657\n",
            "Done 600 step, Loss: 0.008655\n",
            "Done 700 step, Loss: 0.008657\n",
            "Done 800 step, Loss: 0.008650\n",
            "Done 900 step, Loss: 0.008649\n",
            "Done 1000 step, Loss: 0.008652\n",
            "Done 1100 step, Loss: 0.008644\n",
            "Done 1200 step, Loss: 0.008647\n",
            "Done 1300 step, Loss: 0.008646\n",
            "Done 1400 step, Loss: 0.008647\n",
            "Done 1500 step, Loss: 0.008645\n",
            "Done 1600 step, Loss: 0.008646\n",
            "Done 1700 step, Loss: 0.008640\n",
            "Done 1800 step, Loss: 0.008642\n",
            "Done 1900 step, Loss: 0.008642\n",
            "Done 2000 step, Loss: 0.008640\n",
            "Done 2100 step, Loss: 0.008640\n",
            "Done 2200 step, Loss: 0.008641\n",
            "Done 2300 step, Loss: 0.008636\n",
            "Done 2400 step, Loss: 0.008637\n",
            "Done 2500 step, Loss: 0.008637\n",
            "Done 2600 step, Loss: 0.008637\n",
            "Done 2700 step, Loss: 0.008635\n",
            "Done 2800 step, Loss: 0.008636\n",
            "Done 2900 step, Loss: 0.008632\n",
            "Done 3000 step, Loss: 0.008633\n",
            "Done 3100 step, Loss: 0.008633\n",
            "Done 3200 step, Loss: 0.008632\n",
            "Done 3300 step, Loss: 0.008631\n",
            "Done 3400 step, Loss: 0.008631\n",
            "Done 3500 step, Loss: 0.008628\n",
            "Done 3600 step, Loss: 0.008629\n",
            "Saving checkpoint for iteration 5 at /content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom/ckpt-17\n",
            "3631 tf.Tensor(0.008628689, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.008602\n",
            "Done 200 step, Loss: 0.008600\n",
            "Done 300 step, Loss: 0.008603\n",
            "Done 400 step, Loss: 0.008608\n",
            "Done 500 step, Loss: 0.008596\n",
            "Done 600 step, Loss: 0.008603\n",
            "Done 700 step, Loss: 0.008594\n",
            "Done 800 step, Loss: 0.008594\n",
            "Done 900 step, Loss: 0.008598\n",
            "Done 1000 step, Loss: 0.008596\n",
            "Done 1100 step, Loss: 0.008595\n",
            "Done 1200 step, Loss: 0.008595\n",
            "Done 1300 step, Loss: 0.008597\n",
            "Done 1400 step, Loss: 0.008592\n",
            "Done 1500 step, Loss: 0.008594\n",
            "Done 1600 step, Loss: 0.008590\n",
            "Done 1700 step, Loss: 0.008589\n",
            "Done 1800 step, Loss: 0.008591\n",
            "Done 1900 step, Loss: 0.008591\n",
            "Done 2000 step, Loss: 0.008589\n",
            "Done 2100 step, Loss: 0.008589\n",
            "Done 2200 step, Loss: 0.008589\n",
            "Done 2300 step, Loss: 0.008586\n",
            "Done 2400 step, Loss: 0.008587\n",
            "Done 2500 step, Loss: 0.008587\n",
            "Done 2600 step, Loss: 0.008585\n",
            "Done 2700 step, Loss: 0.008585\n",
            "Done 2800 step, Loss: 0.008586\n",
            "Done 2900 step, Loss: 0.008582\n",
            "Done 3000 step, Loss: 0.008583\n",
            "Done 3100 step, Loss: 0.008583\n",
            "Done 3200 step, Loss: 0.008581\n",
            "Done 3300 step, Loss: 0.008582\n",
            "Done 3400 step, Loss: 0.008582\n",
            "Done 3500 step, Loss: 0.008579\n",
            "Done 3600 step, Loss: 0.008579\n",
            "Saving checkpoint for iteration 6 at /content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom/ckpt-18\n",
            "3631 tf.Tensor(0.00857704, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.008615\n",
            "Done 200 step, Loss: 0.008596\n",
            "Done 300 step, Loss: 0.008572\n",
            "Done 400 step, Loss: 0.008561\n",
            "Done 500 step, Loss: 0.008571\n",
            "Done 600 step, Loss: 0.008553\n",
            "Done 700 step, Loss: 0.008559\n",
            "Done 800 step, Loss: 0.008561\n",
            "Done 900 step, Loss: 0.008551\n",
            "Done 1000 step, Loss: 0.008552\n",
            "Done 1100 step, Loss: 0.008557\n",
            "Done 1200 step, Loss: 0.008546\n",
            "Done 1300 step, Loss: 0.008551\n",
            "Done 1400 step, Loss: 0.008551\n",
            "Done 1500 step, Loss: 0.008551\n",
            "Done 1600 step, Loss: 0.008548\n",
            "Done 1700 step, Loss: 0.008549\n",
            "Done 1800 step, Loss: 0.008546\n",
            "Done 1900 step, Loss: 0.008546\n",
            "Done 2000 step, Loss: 0.008546\n",
            "Done 2100 step, Loss: 0.008544\n",
            "Done 2200 step, Loss: 0.008543\n",
            "Done 2300 step, Loss: 0.008544\n",
            "Done 2400 step, Loss: 0.008540\n",
            "Done 2500 step, Loss: 0.008540\n",
            "Done 2600 step, Loss: 0.008540\n",
            "Done 2700 step, Loss: 0.008537\n",
            "Done 2800 step, Loss: 0.008539\n",
            "Done 2900 step, Loss: 0.008539\n",
            "Done 3000 step, Loss: 0.008537\n",
            "Done 3100 step, Loss: 0.008537\n",
            "Done 3200 step, Loss: 0.008537\n",
            "Done 3300 step, Loss: 0.008534\n",
            "Done 3400 step, Loss: 0.008535\n",
            "Done 3500 step, Loss: 0.008535\n",
            "Done 3600 step, Loss: 0.008533\n",
            "Saving checkpoint for iteration 7 at /content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom/ckpt-19\n",
            "3631 tf.Tensor(0.008532668, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.008543\n",
            "Done 200 step, Loss: 0.008528\n",
            "Done 300 step, Loss: 0.008519\n",
            "Done 400 step, Loss: 0.008521\n",
            "Done 500 step, Loss: 0.008523\n",
            "Done 600 step, Loss: 0.008510\n",
            "Done 700 step, Loss: 0.008512\n",
            "Done 800 step, Loss: 0.008507\n",
            "Done 900 step, Loss: 0.008507\n",
            "Done 1000 step, Loss: 0.008509\n",
            "Done 1100 step, Loss: 0.008508\n",
            "Done 1200 step, Loss: 0.008508\n",
            "Done 1300 step, Loss: 0.008509\n",
            "Done 1400 step, Loss: 0.008509\n",
            "Done 1500 step, Loss: 0.008504\n",
            "Done 1600 step, Loss: 0.008505\n",
            "Done 1700 step, Loss: 0.008505\n",
            "Done 1800 step, Loss: 0.008503\n",
            "Done 1900 step, Loss: 0.008503\n",
            "Done 2000 step, Loss: 0.008504\n",
            "Done 2100 step, Loss: 0.008501\n",
            "Done 2200 step, Loss: 0.008502\n",
            "Done 2300 step, Loss: 0.008499\n",
            "Done 2400 step, Loss: 0.008499\n",
            "Done 2500 step, Loss: 0.008499\n",
            "Done 2600 step, Loss: 0.008499\n",
            "Done 2700 step, Loss: 0.008497\n",
            "Done 2800 step, Loss: 0.008498\n",
            "Done 2900 step, Loss: 0.008499\n",
            "Done 3000 step, Loss: 0.008495\n",
            "Done 3100 step, Loss: 0.008496\n",
            "Done 3200 step, Loss: 0.008495\n",
            "Done 3300 step, Loss: 0.008494\n",
            "Done 3400 step, Loss: 0.008494\n",
            "Done 3500 step, Loss: 0.008494\n",
            "Done 3600 step, Loss: 0.008492\n",
            "Saving checkpoint for iteration 8 at /content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom/ckpt-20\n",
            "3631 tf.Tensor(0.008493145, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.008393\n",
            "Done 200 step, Loss: 0.008446\n",
            "Done 300 step, Loss: 0.008457\n",
            "Done 400 step, Loss: 0.008442\n",
            "Done 500 step, Loss: 0.008459\n",
            "Done 600 step, Loss: 0.008463\n",
            "Done 700 step, Loss: 0.008455\n",
            "Done 800 step, Loss: 0.008457\n",
            "Done 900 step, Loss: 0.008464\n",
            "Done 1000 step, Loss: 0.008453\n",
            "Done 1100 step, Loss: 0.008458\n",
            "Done 1200 step, Loss: 0.008460\n",
            "Done 1300 step, Loss: 0.008453\n",
            "Done 1400 step, Loss: 0.008457\n",
            "Done 1500 step, Loss: 0.008458\n",
            "Done 1600 step, Loss: 0.008456\n",
            "Done 1700 step, Loss: 0.008456\n",
            "Done 1800 step, Loss: 0.008457\n",
            "Done 1900 step, Loss: 0.008453\n",
            "Done 2000 step, Loss: 0.008454\n",
            "Done 2100 step, Loss: 0.008454\n",
            "Done 2200 step, Loss: 0.008453\n",
            "Done 2300 step, Loss: 0.008452\n",
            "Done 2400 step, Loss: 0.008452\n",
            "Done 2500 step, Loss: 0.008449\n",
            "Done 2600 step, Loss: 0.008450\n",
            "Done 2700 step, Loss: 0.008451\n",
            "Done 2800 step, Loss: 0.008449\n",
            "Done 2900 step, Loss: 0.008449\n",
            "Done 3000 step, Loss: 0.008449\n",
            "Done 3100 step, Loss: 0.008446\n",
            "Done 3200 step, Loss: 0.008446\n",
            "Done 3300 step, Loss: 0.008447\n",
            "Done 3400 step, Loss: 0.008444\n",
            "Done 3500 step, Loss: 0.008444\n",
            "Done 3600 step, Loss: 0.008444\n",
            "Saving checkpoint for iteration 9 at /content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom/ckpt-21\n",
            "3631 tf.Tensor(0.008444394, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.008386\n",
            "Done 200 step, Loss: 0.008407\n",
            "Done 300 step, Loss: 0.008425\n",
            "Done 400 step, Loss: 0.008405\n",
            "Done 500 step, Loss: 0.008415\n",
            "Done 600 step, Loss: 0.008403\n",
            "Done 700 step, Loss: 0.008406\n",
            "Done 800 step, Loss: 0.008411\n",
            "Done 900 step, Loss: 0.008412\n",
            "Done 1000 step, Loss: 0.008409\n",
            "Done 1100 step, Loss: 0.008410\n",
            "Done 1200 step, Loss: 0.008412\n",
            "Done 1300 step, Loss: 0.008407\n",
            "Done 1400 step, Loss: 0.008409\n",
            "Done 1500 step, Loss: 0.008406\n",
            "Done 1600 step, Loss: 0.008405\n",
            "Done 1700 step, Loss: 0.008407\n",
            "Done 1800 step, Loss: 0.008407\n",
            "Done 1900 step, Loss: 0.008405\n",
            "Done 2000 step, Loss: 0.008406\n",
            "Done 2100 step, Loss: 0.008406\n",
            "Done 2200 step, Loss: 0.008402\n",
            "Done 2300 step, Loss: 0.008403\n",
            "Done 2400 step, Loss: 0.008404\n",
            "Done 2500 step, Loss: 0.008401\n",
            "Done 2600 step, Loss: 0.008401\n",
            "Done 2700 step, Loss: 0.008400\n",
            "Done 2800 step, Loss: 0.008399\n",
            "Done 2900 step, Loss: 0.008399\n",
            "Done 3000 step, Loss: 0.008399\n",
            "Done 3100 step, Loss: 0.008397\n",
            "Done 3200 step, Loss: 0.008397\n",
            "Done 3300 step, Loss: 0.008397\n",
            "Done 3400 step, Loss: 0.008394\n",
            "Done 3500 step, Loss: 0.008395\n",
            "Done 3600 step, Loss: 0.008396\n",
            "Saving checkpoint for iteration 10 at /content/drive/My Drive/abae_logs/checkpoints/abae/trainfrom/ckpt-22\n",
            "3631 tf.Tensor(0.008393283, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 976
        },
        "id": "duouubyyMHzd",
        "outputId": "4d6ed074-9e90-498e-d3b4-1167b8da311f"
      },
      "source": [
        "topics=test.weights[4].numpy()\n",
        "topic_words={}\n",
        "k=1\n",
        "for i in range(len(topics)):\n",
        "  topic_words[\"Aspect_\"+str(k)]=np.argsort(model.wv.cosine_similarities(topics[i],model.wv.vectors))[::-1][0:30]\n",
        "  k=k+1\n",
        "pd.DataFrame(topic_words).applymap(lambda x:model.wv.index2word[x])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Aspect_1</th>\n",
              "      <th>Aspect_2</th>\n",
              "      <th>Aspect_3</th>\n",
              "      <th>Aspect_4</th>\n",
              "      <th>Aspect_5</th>\n",
              "      <th>Aspect_6</th>\n",
              "      <th>Aspect_7</th>\n",
              "      <th>Aspect_8</th>\n",
              "      <th>Aspect_9</th>\n",
              "      <th>Aspect_10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rs</td>\n",
              "      <td>played</td>\n",
              "      <td>cameras</td>\n",
              "      <td>notifications</td>\n",
              "      <td>nice</td>\n",
              "      <td>far</td>\n",
              "      <td>body</td>\n",
              "      <td>ke</td>\n",
              "      <td>cancel</td>\n",
              "      <td>s9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>offer</td>\n",
              "      <td>games</td>\n",
              "      <td>depth</td>\n",
              "      <td>app</td>\n",
              "      <td>osm</td>\n",
              "      <td>think</td>\n",
              "      <td>thin</td>\n",
              "      <td>hai</td>\n",
              "      <td>delivered</td>\n",
              "      <td>obviously</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>discount</td>\n",
              "      <td>runs</td>\n",
              "      <td>captures</td>\n",
              "      <td>exit</td>\n",
              "      <td>xilent</td>\n",
              "      <td>comparison</td>\n",
              "      <td>fits</td>\n",
              "      <td>liye</td>\n",
              "      <td>deliver</td>\n",
              "      <td>amazing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>discounts</td>\n",
              "      <td>playing</td>\n",
              "      <td>portrait</td>\n",
              "      <td>messages</td>\n",
              "      <td>vry</td>\n",
              "      <td>aspects</td>\n",
              "      <td>case</td>\n",
              "      <td>bhi</td>\n",
              "      <td>order</td>\n",
              "      <td>terms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>price</td>\n",
              "      <td>gaming</td>\n",
              "      <td>images</td>\n",
              "      <td>operations</td>\n",
              "      <td>nyc</td>\n",
              "      <td>really</td>\n",
              "      <td>feels</td>\n",
              "      <td>h</td>\n",
              "      <td>informed</td>\n",
              "      <td>s8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>deal</td>\n",
              "      <td>handles</td>\n",
              "      <td>camera</td>\n",
              "      <td>application</td>\n",
              "      <td>lovly</td>\n",
              "      <td>actually</td>\n",
              "      <td>design</td>\n",
              "      <td>hota</td>\n",
              "      <td>promised</td>\n",
              "      <td>absolutely</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>discounted</td>\n",
              "      <td>almost</td>\n",
              "      <td>wide</td>\n",
              "      <td>apps</td>\n",
              "      <td>sm</td>\n",
              "      <td>trust</td>\n",
              "      <td>handy</td>\n",
              "      <td>acha</td>\n",
              "      <td>assured</td>\n",
              "      <td>loving</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>999</td>\n",
              "      <td>heavy</td>\n",
              "      <td>rear</td>\n",
              "      <td>restart</td>\n",
              "      <td>nais</td>\n",
              "      <td>one</td>\n",
              "      <td>grip</td>\n",
              "      <td>ki</td>\n",
              "      <td>address</td>\n",
              "      <td>flagships</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>12000</td>\n",
              "      <td>graphics</td>\n",
              "      <td>capture</td>\n",
              "      <td>able</td>\n",
              "      <td>good</td>\n",
              "      <td>believe</td>\n",
              "      <td>slippery</td>\n",
              "      <td>nhi</td>\n",
              "      <td>asked</td>\n",
              "      <td>beast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>13000</td>\n",
              "      <td>pubg</td>\n",
              "      <td>shots</td>\n",
              "      <td>applications</td>\n",
              "      <td>osom</td>\n",
              "      <td>better</td>\n",
              "      <td>looks</td>\n",
              "      <td>accha</td>\n",
              "      <td>delivery</td>\n",
              "      <td>flagship</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>7000</td>\n",
              "      <td>medium</td>\n",
              "      <td>lens</td>\n",
              "      <td>music</td>\n",
              "      <td>gjb</td>\n",
              "      <td>terms</td>\n",
              "      <td>slim</td>\n",
              "      <td>jyada</td>\n",
              "      <td>customer</td>\n",
              "      <td>absolute</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>5500</td>\n",
              "      <td>90</td>\n",
              "      <td>daylight</td>\n",
              "      <td>icon</td>\n",
              "      <td>super</td>\n",
              "      <td>still</td>\n",
              "      <td>silicon</td>\n",
              "      <td>kam</td>\n",
              "      <td>ekart</td>\n",
              "      <td>iphones</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>19000</td>\n",
              "      <td>extreme</td>\n",
              "      <td>pictures</td>\n",
              "      <td>function</td>\n",
              "      <td>profomance</td>\n",
              "      <td>compared</td>\n",
              "      <td>hands</td>\n",
              "      <td>ye</td>\n",
              "      <td>cancelled</td>\n",
              "      <td>great</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>billion</td>\n",
              "      <td>lasts</td>\n",
              "      <td>front</td>\n",
              "      <td>sometimes</td>\n",
              "      <td>awesome</td>\n",
              "      <td>plus</td>\n",
              "      <td>plastic</td>\n",
              "      <td>bahut</td>\n",
              "      <td>executives</td>\n",
              "      <td>s10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>diwali</td>\n",
              "      <td>lag</td>\n",
              "      <td>aperture</td>\n",
              "      <td>whatsapp</td>\n",
              "      <td>bt</td>\n",
              "      <td>compare</td>\n",
              "      <td>bigger</td>\n",
              "      <td>nahi</td>\n",
              "      <td>date</td>\n",
              "      <td>barring</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>8100</td>\n",
              "      <td>100</td>\n",
              "      <td>angle</td>\n",
              "      <td>browsing</td>\n",
              "      <td>swm</td>\n",
              "      <td>definitely</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>jada</td>\n",
              "      <td>bangalore</td>\n",
              "      <td>shifted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>offers</td>\n",
              "      <td>performance</td>\n",
              "      <td>macro</td>\n",
              "      <td>menu</td>\n",
              "      <td>love</td>\n",
              "      <td>phones</td>\n",
              "      <td>beautiful</td>\n",
              "      <td>ka</td>\n",
              "      <td>said</td>\n",
              "      <td>iphone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18999</td>\n",
              "      <td>lags</td>\n",
              "      <td>selfie</td>\n",
              "      <td>notification</td>\n",
              "      <td>betray</td>\n",
              "      <td>going</td>\n",
              "      <td>cover</td>\n",
              "      <td>mai</td>\n",
              "      <td>item</td>\n",
              "      <td>way</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>competitive</td>\n",
              "      <td>continuous</td>\n",
              "      <td>captured</td>\n",
              "      <td>lock</td>\n",
              "      <td>also</td>\n",
              "      <td>series</td>\n",
              "      <td>glass</td>\n",
              "      <td>lekin</td>\n",
              "      <td>seller</td>\n",
              "      <td>top</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>rupees</td>\n",
              "      <td>usage</td>\n",
              "      <td>stereo</td>\n",
              "      <td>folders</td>\n",
              "      <td>nd</td>\n",
              "      <td>awesome</td>\n",
              "      <td>hand</td>\n",
              "      <td>ko</td>\n",
              "      <td>courier</td>\n",
              "      <td>beat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>sale</td>\n",
              "      <td>browsing</td>\n",
              "      <td>ois</td>\n",
              "      <td>hotstar</td>\n",
              "      <td>bettery</td>\n",
              "      <td>great</td>\n",
              "      <td>sleek</td>\n",
              "      <td>mast</td>\n",
              "      <td>sent</td>\n",
              "      <td>really</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>5400</td>\n",
              "      <td>combat</td>\n",
              "      <td>photography</td>\n",
              "      <td>alarm</td>\n",
              "      <td>performanc</td>\n",
              "      <td>loving</td>\n",
              "      <td>compact</td>\n",
              "      <td>aur</td>\n",
              "      <td>packing</td>\n",
              "      <td>brilliant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22500</td>\n",
              "      <td>gpu</td>\n",
              "      <td>selfies</td>\n",
              "      <td>browser</td>\n",
              "      <td>ppo</td>\n",
              "      <td>best</td>\n",
              "      <td>classy</td>\n",
              "      <td>liya</td>\n",
              "      <td>refused</td>\n",
              "      <td>aspects</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>bbd</td>\n",
              "      <td>6</td>\n",
              "      <td>brilliant</td>\n",
              "      <td>disable</td>\n",
              "      <td>superb</td>\n",
              "      <td>way</td>\n",
              "      <td>thick</td>\n",
              "      <td>baat</td>\n",
              "      <td>reached</td>\n",
              "      <td>oneplus</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>14500</td>\n",
              "      <td>processor</td>\n",
              "      <td>zoom</td>\n",
              "      <td>wifi</td>\n",
              "      <td>suprb</td>\n",
              "      <td>comparing</td>\n",
              "      <td>weight</td>\n",
              "      <td>thoda</td>\n",
              "      <td>received</td>\n",
              "      <td>masterpiece</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>24999</td>\n",
              "      <td>charges</td>\n",
              "      <td>effect</td>\n",
              "      <td>disappears</td>\n",
              "      <td>wsm</td>\n",
              "      <td>amazing</td>\n",
              "      <td>aluminium</td>\n",
              "      <td>na</td>\n",
              "      <td>receive</td>\n",
              "      <td>course</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>9990</td>\n",
              "      <td>multitasking</td>\n",
              "      <td>lighting</td>\n",
              "      <td>inbuilt</td>\n",
              "      <td>veri</td>\n",
              "      <td>earlier</td>\n",
              "      <td>curved</td>\n",
              "      <td>koi</td>\n",
              "      <td>responsible</td>\n",
              "      <td>underrated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>bought</td>\n",
              "      <td>50</td>\n",
              "      <td>saturated</td>\n",
              "      <td>download</td>\n",
              "      <td>a3s</td>\n",
              "      <td>go</td>\n",
              "      <td>hold</td>\n",
              "      <td>kiya</td>\n",
              "      <td>flipkart</td>\n",
              "      <td>awesome</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>12990</td>\n",
              "      <td>min</td>\n",
              "      <td>bokeh</td>\n",
              "      <td>restarts</td>\n",
              "      <td>aswom</td>\n",
              "      <td>absolutely</td>\n",
              "      <td>solid</td>\n",
              "      <td>itna</td>\n",
              "      <td>parcel</td>\n",
              "      <td>xr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>21000</td>\n",
              "      <td>smooth</td>\n",
              "      <td>speakers</td>\n",
              "      <td>unlocking</td>\n",
              "      <td>perfect</td>\n",
              "      <td>like</td>\n",
              "      <td>bulky</td>\n",
              "      <td>bhut</td>\n",
              "      <td>accept</td>\n",
              "      <td>phone</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Aspect_1      Aspect_2     Aspect_3  ... Aspect_8     Aspect_9    Aspect_10\n",
              "0            rs        played      cameras  ...       ke       cancel           s9\n",
              "1         offer         games        depth  ...      hai    delivered    obviously\n",
              "2      discount          runs     captures  ...     liye      deliver      amazing\n",
              "3     discounts       playing     portrait  ...      bhi        order        terms\n",
              "4         price        gaming       images  ...        h     informed           s8\n",
              "5          deal       handles       camera  ...     hota     promised   absolutely\n",
              "6    discounted        almost         wide  ...     acha      assured       loving\n",
              "7           999         heavy         rear  ...       ki      address    flagships\n",
              "8         12000      graphics      capture  ...      nhi        asked        beast\n",
              "9         13000          pubg        shots  ...    accha     delivery     flagship\n",
              "10         7000        medium         lens  ...    jyada     customer     absolute\n",
              "11         5500            90     daylight  ...      kam        ekart      iphones\n",
              "12        19000       extreme     pictures  ...       ye    cancelled        great\n",
              "13      billion         lasts        front  ...    bahut   executives          s10\n",
              "14       diwali           lag     aperture  ...     nahi         date      barring\n",
              "15         8100           100        angle  ...     jada    bangalore      shifted\n",
              "16       offers   performance        macro  ...       ka         said       iphone\n",
              "17        18999          lags       selfie  ...      mai         item          way\n",
              "18  competitive    continuous     captured  ...    lekin       seller          top\n",
              "19       rupees         usage       stereo  ...       ko      courier         beat\n",
              "20         sale      browsing          ois  ...     mast         sent       really\n",
              "21         5400        combat  photography  ...      aur      packing    brilliant\n",
              "22        22500           gpu      selfies  ...     liya      refused      aspects\n",
              "23          bbd             6    brilliant  ...     baat      reached      oneplus\n",
              "24        14500     processor         zoom  ...    thoda     received  masterpiece\n",
              "25        24999       charges       effect  ...       na      receive       course\n",
              "26         9990  multitasking     lighting  ...      koi  responsible   underrated\n",
              "27       bought            50    saturated  ...     kiya     flipkart      awesome\n",
              "28        12990           min        bokeh  ...     itna       parcel           xr\n",
              "29        21000        smooth     speakers  ...     bhut       accept        phone\n",
              "\n",
              "[30 rows x 10 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjkIO1GYS01U",
        "outputId": "e53ec335-1b49-424e-8b07-c93f08024766"
      },
      "source": [
        "!pip install pipreqs\n",
        "\n",
        "!pipreqs \"/content/drive/My Drive/REVIEWS/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pipreqs\n",
            "  Downloading pipreqs-0.4.10-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from pipreqs) (0.6.2)\n",
            "Collecting yarg\n",
            "  Downloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from yarg->pipreqs) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (2.10)\n",
            "Installing collected packages: yarg, pipreqs\n",
            "Successfully installed pipreqs-0.4.10 yarg-0.1.9\n",
            "INFO: Successfully saved requirements file in /content/drive/My Drive/REVIEWS/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI4rDNlz3Jms",
        "outputId": "f9885f95-a54f-4db1-d916-790f0676bb73"
      },
      "source": [
        "%%writefile /content/drive/MyDrive/REVIEWS/init.py\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/drive/MyDrive/REVIEWS/init.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_qA1YEo4Ck8",
        "outputId": "d22d48fc-2db6-4868-9ec5-540e0cb51f07"
      },
      "source": [
        "pd.read_pickle(os.path.join(DATA_PATH,\"reviews.pickle\")).tail(20).values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['5', 'Good phone READ MORE', 'Simply awesome'],\n",
              "       ['5', 'Super fast delivery. thanks you flipkart READ MORE',\n",
              "        'Excellent'],\n",
              "       ['5', 'Value for money, totaly amazing READ MORE',\n",
              "        'Great product'],\n",
              "       ['5',\n",
              "        'Good product .....wroking very well.....just purchase it now!!! READ MORE',\n",
              "        'Super!'],\n",
              "       ['4', 'Ok ok READ MORE', 'Worth the money'],\n",
              "       ['5', 'Nice READ MORE', 'Fabulous!'],\n",
              "       ['5', 'Video call not supported READ MORE', 'Must buy!'],\n",
              "       ['5', 'good product but no google apps READ MORE', 'Excellent'],\n",
              "       ['5', 'Nice phone READ MORE', 'Worth every penny'],\n",
              "       ['5', 'Nice phone READ MORE', 'Worth every penny'],\n",
              "       ['4', \"Google ain't working READ MORE\", 'Value-for-money'],\n",
              "       ['4', 'Not bad READ MORE', 'Wonderful'],\n",
              "       ['4',\n",
              "        'best device for Lowest price missing the google services READ MORE',\n",
              "        'Pretty good'],\n",
              "       ['5', 'Its good iam happy with this product READ MORE',\n",
              "        'Must buy!'],\n",
              "       ['4', 'Nice Product READ MORE', 'Good choice'],\n",
              "       ['5', 'Best Budget smart phone. Especially the cameras READ MORE',\n",
              "        'Excellent'],\n",
              "       ['5', 'GOOD PHONE FOR NORMAL USERS. READ MORE', 'Just wow!'],\n",
              "       ['5',\n",
              "        'Excellent product under the price of just 6k Worth every penny READ MORE',\n",
              "        'Must buy!'],\n",
              "       ['4',\n",
              "        'Nyc products Basic user good mobile Worth for money READ MORE',\n",
              "        'Wonderful'],\n",
              "       ['5', 'Good honor 9s READ MORE', 'Great product']], dtype=object)"
            ]
          },
          "execution_count": 24,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1yK0LpekDRw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}