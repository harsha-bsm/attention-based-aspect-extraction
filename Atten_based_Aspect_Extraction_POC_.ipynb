{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Atten based Aspect Extraction - POC .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iZ_fyLTKLv_"
      },
      "source": [
        "#### Implementation of the following research paper on a real world data.\n",
        "[research paper](https://www.comp.nus.edu.sg/~leews/publications/acl17.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72ZlgLNDHYjK"
      },
      "source": [
        "#### importing packages and dataset\n",
        "#####  Data has been scrapped from a leading ecommerse website in India. Scrapped all the customer reviews about smart phones belonging to different companies. Total data points are about 350k which consists of rating(int), reviews(text) and title(text)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoEKsqQceLKm",
        "outputId": "4da4f3ad-01c3-44ae-ab9d-cf5c158a8573"
      },
      "source": [
        "# Mounting files on google drive which will ease file accessing \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Uqa9fgof-Sx"
      },
      "source": [
        "import pandas as pd "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlRKaLl4e8Ae"
      },
      "source": [
        "df = pd.read_pickle(\"/content/drive/My Drive/REVIEWS/reviews.pickle\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5BFbOGjgIki",
        "outputId": "93c4f49d-7469-4875-dc9e-d6d297edbc7b"
      },
      "source": [
        "# Launching tensorflow 2 and invoking GPU if available on Google Collab\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6rTfXXLghAH"
      },
      "source": [
        "# importing necessary modules\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# import seaborn as sns\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense,Concatenate,TimeDistributed,Masking,GRU,Input,Dot,Reshape,Softmax\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLveUCkNa92z"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NclLvjZhx6G"
      },
      "source": [
        "df[\"review\"]=df[\"review\"].apply(lambda x:x.strip(\"READ MORE\").lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "TotWASK9kA24",
        "outputId": "030f1c1a-fa3f-486d-9680-6c773183a095"
      },
      "source": [
        "df.head()   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>review</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>good</td>\n",
              "      <td>Best in the market!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>flashlight is very very bad</td>\n",
              "      <td>Worst experience ever!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>battery is too bad .</td>\n",
              "      <td>Absolute rubbish!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>bateri  not  warking</td>\n",
              "      <td>Utterly Disappointed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>very bad ðŸ˜ </td>\n",
              "      <td>Very poor</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  rating                       review                   title\n",
              "0      5                         good     Best in the market!\n",
              "1      5  flashlight is very very bad  Worst experience ever!\n",
              "2      4         battery is too bad .       Absolute rubbish!\n",
              "3      5         bateri  not  warking    Utterly Disappointed\n",
              "4      5                   very bad ðŸ˜                Very poor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCf4f7KQcUTL"
      },
      "source": [
        "#### Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBaxgn1wkqnZ"
      },
      "source": [
        "#  removing some of the common contractions.\n",
        "def decontractions(phrase):\n",
        "\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"won\\â€™t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\â€™t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "\n",
        "    phrase = re.sub(r\"n\\â€™t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\â€™re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\â€™s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\â€™d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\â€™ll\", \" will\", phrase)\n",
        "    \n",
        "    phrase = re.sub(r\"\\â€™ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\â€™m\", \" am\", phrase)\n",
        "\n",
        "    return phrase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAroSXo_nsG2",
        "outputId": "11f37efa-2a7b-462a-e848-8dd2d09784f9"
      },
      "source": [
        "# Installing a module to work with emojis. This module provides text format to emojis. \n",
        "# Although this will help in converting emojis to text, I only removed the emojis by replacing them with empty character. Did not use their text format. \n",
        "!pip install emot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emot\n",
            "  Downloading emot-3.1-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                          | 10 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 20 kB 29.1 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 30 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 40 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 51 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61 kB 19 kB/s \n",
            "\u001b[?25hInstalling collected packages: emot\n",
            "Successfully installed emot-3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4Azoo0KndIx"
      },
      "source": [
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS # contains english text of emojis and emoticons in dict format \n",
        "def convert_emojis(review):\n",
        "  for x in review:\n",
        "    if x in UNICODE_EMO.keys():\n",
        "      review=review.replace(x,\"\")\n",
        "  return review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8duOY2wqOMY"
      },
      "source": [
        "#Converting all the extra spaces into single space. This will help while splitting the data in the future.\n",
        "def removecharacters(review):\n",
        "  review= re.sub('[^A-Za-z0-9]+',' ',review)  #anything exept numbers and alphabets, replace them with space\n",
        "  review=re.sub(r\"\\n\",\" \",review)  #new lines into space\n",
        "  review=re.sub(r\"\\t\",\" \",review)  #tabs into space\n",
        "  review=re.sub(r\"\\v\",\" \",review)  #vertical tab into space\n",
        "  review=re.sub(r\"\\s\",\" \",review)   #all extra spaces into single space\n",
        "  return review.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaRuiEgNnpnP"
      },
      "source": [
        "df[\"review\"]=df[\"review\"].apply(convert_emojis)\n",
        "df[\"review\"]=df[\"review\"].apply(decontractions)\n",
        "df[\"review\"]=df[\"review\"].apply(removecharacters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq-oV-9u1a1V",
        "outputId": "42dd3a85-708e-4fce-bb66-6c9b549949ca"
      },
      "source": [
        "# Using NLTK for PoS tagging and Stopwords removal\n",
        "# Downloading the corresponding packages\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIs4RJqe3y0A",
        "outputId": "d84955f6-e87a-4f63-94dd-ab460497f3fa"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "len(stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pVBkrrZ09NG"
      },
      "source": [
        "# Using all the nouns in the reviews only (Ressults are better when considering all the words. So did not implement this)\n",
        "def onlynouns(review):\n",
        "  lis= \" \".join([each.strip(\" \") for each,pos in nltk.pos_tag(review.split()) if (pos==\"NN\" )])\n",
        "  return lis\n",
        "df[\"review\"]=df[\"review\"].apply(lambda x:onlynouns(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MeSS1d7nzrK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JomZFlmQi9cl"
      },
      "source": [
        "#some custom stopwords and also stopwords from NLTK\n",
        "stopword= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
        "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
        "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
        "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
        "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
        "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
        "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
        "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
        "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
        "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
        "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
        "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
        "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
        "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
        "            'won', \"won't\", 'wouldn', \"wouldn't\",\"not\",\"np\",\"should\",\"iam\"]).union(stop_words)\n",
        "def removestopwords(review):\n",
        "  return \" \".join([word for word in review.split(\" \") if not word in stopword])\n",
        "df[\"review\"]=pd.DataFrame(df[\"review\"].apply(lambda x:removestopwords(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iktefR987Xh"
      },
      "source": [
        "df[\"len\"]=df.review.str.split().apply(len)  #number of words in the review\n",
        "\n",
        "df=df.loc[(df[\"len\"]>0) & (df[\"len\"]<225)]  # considering only reviews with number of words less than 225 (this constributes more than 99.5 percentile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8u6QOxsmmYv"
      },
      "source": [
        "tkn = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n') #tensorflow tokenising\n",
        "tkn.fit_on_texts(df['review'].values)\n",
        "word_countdict=tkn.word_counts  #word count dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThgleYKgl4bL"
      },
      "source": [
        "def word_count(review):\n",
        "  return  \" \".join([word for word in review.split() if word_countdict[word]>10] ) #considering all the words that appeared more than 10 times in the entire carpora\n",
        "df[\"review\"]=pd.DataFrame(df[\"review\"].apply(lambda x:word_count(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUDAWjxzm5WO"
      },
      "source": [
        "df[\"len\"]=df.review.str.split(\" \").apply(len)\n",
        "\n",
        "df=df.loc[(df[\"len\"]>0)]     #removing empty reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q5SqoTCIU4I"
      },
      "source": [
        "tkn = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')  #tokenising again\n",
        "tkn.fit_on_texts(df['review'].values)\n",
        "word_countdict=tkn.word_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLeoQlTR4d1h"
      },
      "source": [
        "import pickle   #dumping data frame as a pickel file\n",
        "with open(\"/content/drive/My Drive/REVIEWS/df_nouns.p\",\"wb\") as f:\n",
        "  pickle.dump(df,f)                              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lwfRH09Irfd"
      },
      "source": [
        "import pickle\n",
        "with open(\"/content/drive/My Drive/REVIEWS/df_nouns.p\",\"rb\") as f:\n",
        "  df=pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUMhv-znIyFQ"
      },
      "source": [
        "df.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLIwUvDdsKS0"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le48SaH0wdgc"
      },
      "source": [
        "#df=df.loc[df[\"review\"].apply(lambda x:len(x.strip().split())>0)].reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU6db_nrwpb_"
      },
      "source": [
        "#tkn = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "#tkn.fit_on_texts(df['review'].values)\n",
        "#word_countdict=tkn.word_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjfxNJY2Bb7H"
      },
      "source": [
        "seq_texts=tkn.texts_to_sequences(df['review'].values)  #converting tokenised reviews into sequence of integers with each integer corresponding to one word in the corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUeZowdeBotm"
      },
      "source": [
        "text=pd.Series(df['review'].values).apply(lambda x: x.split())  #spliting the reviews in a format suitable to  feed into Fasttext for trainig vocab "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaMuG75KAvwf",
        "outputId": "55fa0313-5f78-4e4b-8bcd-8022c29b135c"
      },
      "source": [
        "word_countdict[\"delivery\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13231"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA7TOhadDbxF",
        "outputId": "a1b89ae0-9e0e-47d4-c4c1-853d7c8bcdd9"
      },
      "source": [
        "text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                    [good]\n",
              "1                                         [flashlight, bad]\n",
              "2                                            [battery, bad]\n",
              "3                                                 [warking]\n",
              "4                                                     [bad]\n",
              "                                ...                        \n",
              "362991    [best, budget, smart, phone, especially, cameras]\n",
              "362992                         [good, phone, normal, users]\n",
              "362993    [xcellent, product, price, 6k, worth, every, p...\n",
              "362994    [nyc, products, basic, user, good, mobile, wor...\n",
              "362995                                    [good, honor, 9s]\n",
              "Length: 362996, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsUiYqxZhZy6"
      },
      "source": [
        "#### Learning word embeddings \n",
        "##### FastText is a library for learning word embeddings. FastText is bult by Facebook Ai team. One of the key features of fastText word representation is its ability to produce vectors for any words, even made-up ones. Indeed, fastText word vectors are built from vectors of substrings of characters contained in it. This allows to build vectors even for misspelled words or concatenation of words. This is one of the main adavantages over word2vec. Since our data is a real world data with lot of misspelt words and words out of english vocabulary, I have chosen Fasttext. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x36-_8tGCYL_"
      },
      "source": [
        "#FastText is also available in Gensim.\n",
        "from gensim.models import FastText "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNRt5vbM-L6X"
      },
      "source": [
        "#  We are training the model on our own vocabulary(text data) rather than using pretrained wordembeddings \n",
        "# skipgram based training. Considers all the words that appeared atleast once. Window size is ten.\n",
        "model = FastText(size=100, negative=5, min_count=1,window=10,iter=250,sg=1) \n",
        "model.build_vocab(text) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9PpZIh5_5hL"
      },
      "source": [
        "\n",
        "gensim_fasttext = model.train(sentences=text, \n",
        "                           sg=1, ##skipgram\n",
        "                           epochs=250, ##no of iterations\n",
        "                           size=100, ##dimentions of word embedding\n",
        "                           seed=1,\n",
        "                           total_examples=model.corpus_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYrObZWlHKXy"
      },
      "source": [
        "model.save(\"/content/drive/My Drive/REVIEWS/wordembeddings_nouns\") #saving the trainedmodel into drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qynAN09Sljk"
      },
      "source": [
        "from gensim.models import FastText\n",
        "model=FastText.load(\"/content/drive/My Drive/REVIEWS/wordembeddings_nouns\") #reusing already trained model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSRgB1bWr2eS",
        "outputId": "360ee9b1-976a-4a61-f285-59b9403e8309"
      },
      "source": [
        "model.wv.similar_by_word(\"battery\") # you can observe how similar are the misspelt words with the actual correctly spelt word."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('bettery', 0.7582681179046631),\n",
              " ('batter', 0.7468189001083374),\n",
              " ('performance', 0.7118053436279297),\n",
              " ('battry', 0.6854369640350342),\n",
              " ('battary', 0.6752023696899414),\n",
              " ('backup', 0.663201630115509),\n",
              " ('camera', 0.6577006578445435),\n",
              " ('display', 0.638664722442627),\n",
              " ('average', 0.6315059065818787),\n",
              " ('charging', 0.6301984190940857)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GVkeoaV8WhL",
        "outputId": "f3590f85-b54c-4e43-a19d-1a7c40f1fdc9"
      },
      "source": [
        "max(df.len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "223"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY1QK5UfJmk7"
      },
      "source": [
        "#padding the sequences to max length review. used post padding. \n",
        "#Batch training text sequences of variable length is not possible. So padding is required  \n",
        "seq_texts=tf.keras.preprocessing.sequence.pad_sequences(seq_texts,\n",
        "                                                         maxlen=max(df.len),\n",
        "                                                         padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i7uaMhTsCT0"
      },
      "source": [
        "import pickle\n",
        "with open(\"/content/drive/My Drive/REVIEWS/seq_texts_nouns.p\",\"wb\") as f:\n",
        "  pickle.dump(seq_texts,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEsT8fOX2Qmb"
      },
      "source": [
        "import pickle\n",
        "with open(\"/content/drive/My Drive/REVIEWS/seq_texts_nouns.p\",\"rb\") as f:\n",
        "  seq_texts=pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxarNQ_AsBKn",
        "outputId": "d3ba8f61-9007-46f5-f843-61ad21c6da4b"
      },
      "source": [
        "seq_texts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   1,    0,    0, ...,    0,    0,    0],\n",
              "       [2399,   26,    0, ...,    0,    0,    0],\n",
              "       [   7,   26,    0, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [  60,    5,    9, ...,    0,    0,    0],\n",
              "       [ 236,  139,  589, ...,    0,    0,    0],\n",
              "       [   1,  559, 6995, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6hy4kBc8p8s",
        "outputId": "bc93c9c6-c2cc-4939-8ff1-61ec80d02a16"
      },
      "source": [
        "len(model.wv.vocab) #total vocab "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6okdvzTtIQpk"
      },
      "source": [
        "#### Variables that go into the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bcaDvZHtuqF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb45d6e6-fb9c-4fa0-d12f-af7dcae51429"
      },
      "source": [
        "embed_inputdim=len(model.wv.vocab)\n",
        "embed_outputdim=100\n",
        "trained_weights=np.vstack((np.zeros((1,100)),model.wv.vectors)) #used for initialising the embedding layer\n",
        "aspects_k=10 #number of aspects\n",
        "dense=100\n",
        "inputlength=max(df.len) \n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=aspects_k,random_state=0,max_iter=500,n_jobs=-1).fit(trained_weights) #clustering the trained wordembeddings into 10 clusters\n",
        "init=tf.constant_initializer(kmeans.cluster_centers_) #used for initialing the weights of final dense layer\n",
        "init"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.ops.init_ops_v2.Constant at 0x7f56fee25e50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bixEGnbIIa3M"
      },
      "source": [
        "#### Building custom attention layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp8YNv4UFCba"
      },
      "source": [
        "#custom build attention layer\n",
        "\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      \n",
        "      self.soft = tf.keras.layers.Softmax(axis=-2,name=\"softmax_att\") #softmax layer\n",
        "      self.dot=tf.keras.layers.Dot(axes=(-1,-1),name=\"dot_att\")       #dot layer\n",
        "      self.w = tf.Variable(\n",
        "            initial_value=tf.random_normal_initializer()(shape=(100, 100), dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )              #weights that captures essense between the word embedding and global context vector(or average of all the word embedding of the sentence)\n",
        "  \n",
        "    def call(self,embed_output, mask=None):\n",
        "\n",
        "      ys = tf.reduce_mean(embed_output,axis=-2) #average of all word embeddings of the sentence\n",
        "      ys=tf.expand_dims(ys,axis=-2)\n",
        "      eW=tf.matmul(embed_output, self.w)\n",
        "      eW = eW * tf.expand_dims(tf.cast(mask,tf.float32),-1) #maskpropagation. Preventing masked elements into calculations\n",
        "      f=self.dot([eW, ys])\n",
        "      f = f+tf.expand_dims(tf.cast(tf.math.equal(mask, False), f.dtype)*-1e9,-1) #multiplying all the masked elements by -1e9 so that the softmax step do not impact the vectors\n",
        "      f=self.soft(f)\n",
        "      zs=tf.math.reduce_sum(f*embed_output,axis=-2) #zs is aspect embedding space after attention mechanisms on words of the sentence\n",
        "                                                    # f - softmax - gives info about unimportant words in the sentence for extracting aspects \n",
        "      \n",
        "      return zs,f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE5aH_HfIkGH"
      },
      "source": [
        "#### Building custom model which outputs loss on calling it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJJZY6rEsnuW"
      },
      "source": [
        "# custom model\n",
        "\n",
        "class model_(tf.keras.Model):\n",
        "  def __init__(self,embed_inputdim,embed_outputdim,trained_weights,aspects_k,dense,inputlength):\n",
        "    super().__init__()\n",
        "    self.embed_inputdim=embed_inputdim\n",
        "    self.inputlength=inputlength\n",
        "    self.embed_outputdim=embed_outputdim\n",
        "    self.trained_weights=trained_weights\n",
        "    self.embedding=Embedding(input_dim=self.embed_inputdim+1,output_dim=self.embed_outputdim,mask_zero=True,\n",
        "                             input_length=self.inputlength,weights=[self.trained_weights],name=\"embedding_layer\",trainable=True) #embedding layer. Zero masking\n",
        "    self.attention=Attention()\n",
        "    self.aspects_k=aspects_k #number of aspects\n",
        "    self.k = tf.keras.layers.Dense(aspects_k,name=\"dim_reduction_layer\",activation=\"softmax\")\n",
        "    self.dense=dense \n",
        "    self.final=tf.keras.layers.Dense(self.dense,name=\"final_dense\",kernel_initializer=init) #weights are initialised with embedding clusters\n",
        "  def call(self,input):\n",
        "    e=self.embedding(input[0])\n",
        "    mask = self.embedding.compute_mask(input[0]) #computing mask so that this this can be used while propagating mask for subsequent layers\n",
        "    zs=self.attention(e, mask = mask)[0] #aspect vector\n",
        "    pt=self.k(zs)  #dimensionality vector\n",
        "    rs=self.final(pt) #reconstructed vector\n",
        "\n",
        "    # building regulariser to be used in the loss. [(T*transpose(T))-I]\n",
        "    reg=tf.tensordot(tf.linalg.normalize(self.final.weights[0],axis=1)[1],tf.linalg.normalize(self.final.weights[0],axis=1)[1],axes=[[1],[1]])\n",
        "    reg=reg-tf.ones([self.aspects_k,self.aspects_k])\n",
        "    reg=tf.norm(reg, ord='euclidean', axis=None, keepdims=None, name=None)\n",
        "    # calculating loss\n",
        "    r=tf.expand_dims(rs,-2)\n",
        "    f=tf.tensordot(rs,zs,[[0,1],[0,1]])\n",
        "    a=self.embedding(input[1])\n",
        "    a=tf.reduce_mean(a,axis=-2)\n",
        "    loss=tf.reduce_sum(tf.nn.relu(1-tf.reduce_sum(tf.tensordot(a,r,[[0,2],[0,2]]))+f))+1*reg #this is the loss which is to be minimised\n",
        "    return loss,pt,rs \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuqiB0fqI0B-"
      },
      "source": [
        "#### Building tensorflow dataset from generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7IF2ChG5_oH"
      },
      "source": [
        "#data generating\n",
        "from random import seed\n",
        "from random import randint\n",
        "\n",
        "def gendata():\n",
        "  seed(42)\n",
        "  for i in range(0,len(seq_texts)):\n",
        "    lis=[]\n",
        "    lent=[]\n",
        "    while len(lent)<20:\n",
        "      value = randint(0, len(seq_texts)-1)\n",
        "      if value==i:\n",
        "        continue\n",
        "      lis.append(seq_texts[value])\n",
        "      lent.append(value)\n",
        "\n",
        "    yield seq_texts[i],lis\n",
        "datagen=tf.data.Dataset.from_generator(gendata, output_types=(tf.int32,tf.int32))\n",
        "datagen=datagen.repeat(1).shuffle(buffer_size=1024).batch(100).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkrZfTM0I_S3"
      },
      "source": [
        "#### Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEezdVISyhUu",
        "outputId": "f5dd2ff4-d5c3-4154-d12b-31a9a892fda6"
      },
      "source": [
        "#model initialisation\n",
        "abae=model_(embed_inputdim,embed_outputdim,trained_weights,aspects_k,dense,inputlength)\n",
        "# optimiser\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "@tf.function\n",
        "def train_step(input):\n",
        "    with tf.GradientTape() as tape:\n",
        "        #forward propagation\n",
        "        loss = abae(input)[0]\n",
        "        #print(loss)\n",
        "\n",
        "    #getting gradients\n",
        "    gradients = tape.gradient(loss, abae.trainable_variables)\n",
        "    #applying gradients\n",
        "    optimizer.apply_gradients(zip(gradients, abae.trainable_variables))\n",
        "\n",
        "    return loss, gradients\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "\n",
        "##check point to save\n",
        "checkpoint_path = \"/content/drive/My Drive/abae_logs/checkpoints/abae/train\"\n",
        "ckpt = tf.train.Checkpoint(optimizer=optimizer, model=abae)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
        "\n",
        "\n",
        "it=0\n",
        "loss_list=[]\n",
        "for k in range(0,15): # k - number of iterations\n",
        "  counter = 0\n",
        "\n",
        "  # navigating through each batch\n",
        "  for input in datagen:\n",
        "\n",
        "      loss_, gradients = train_step(input)\n",
        "      #adding loss to train loss\n",
        "      train_loss(loss_)\n",
        "      counter = counter + 1\n",
        "      template = '''Done {} step, Loss: {:0.6f}'''\n",
        "\n",
        "    \n",
        "      if counter%100==0:\n",
        "              # printing loss after every 100th batch in an epoch\n",
        "        print(template.format(counter, train_loss.result()))\n",
        "        \n",
        "  loss_list.append(train_loss.result()) #appending loss after every epoch\n",
        "  ckpt_save_path  = ckpt_manager.save() #checkpointing after every epoch\n",
        "  print ('Saving checkpoint for iteration {} at {}'.format(k+1, ckpt_save_path))\n",
        "  print(counter, train_loss.result())\n",
        "  train_loss.reset_states()             #resetting loss after every epoch\n",
        "  \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done 100 step, Loss: 28.968550\n",
            "Done 200 step, Loss: 20.588682\n",
            "Done 300 step, Loss: 15.580440\n",
            "Done 400 step, Loss: 12.309283\n",
            "Done 500 step, Loss: 9.942160\n",
            "Done 600 step, Loss: 8.286394\n",
            "Done 700 step, Loss: 7.103624\n",
            "Done 800 step, Loss: 6.216487\n",
            "Done 900 step, Loss: 5.526466\n",
            "Done 1000 step, Loss: 4.974433\n",
            "Done 1100 step, Loss: 4.522669\n",
            "Done 1200 step, Loss: 4.146284\n",
            "Done 1300 step, Loss: 3.827788\n",
            "Done 1400 step, Loss: 3.554776\n",
            "Done 1500 step, Loss: 3.318174\n",
            "Done 1600 step, Loss: 3.111142\n",
            "Done 1700 step, Loss: 2.928475\n",
            "Done 1800 step, Loss: 2.766082\n",
            "Done 1900 step, Loss: 2.620814\n",
            "Done 2000 step, Loss: 2.490072\n",
            "Done 2100 step, Loss: 2.371783\n",
            "Done 2200 step, Loss: 2.264296\n",
            "Done 2300 step, Loss: 2.166110\n",
            "Done 2400 step, Loss: 2.076089\n",
            "Done 2500 step, Loss: 1.993250\n",
            "Done 2600 step, Loss: 1.916873\n",
            "Done 2700 step, Loss: 1.846112\n",
            "Done 2800 step, Loss: 1.780400\n",
            "Done 2900 step, Loss: 1.719220\n",
            "Done 3000 step, Loss: 1.662116\n",
            "Done 3100 step, Loss: 1.608696\n",
            "Done 3200 step, Loss: 1.558633\n",
            "Done 3300 step, Loss: 1.511585\n",
            "Done 3400 step, Loss: 1.467303\n",
            "Done 3500 step, Loss: 1.425552\n",
            "Done 3600 step, Loss: 1.386119\n",
            "Saving checkpoint for iteration 1 at /content/drive/My Drive/abae_logs/checkpoints/abae/train/ckpt-1\n",
            "3630 tf.Tensor(1.3747128, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.005957\n",
            "Done 200 step, Loss: 0.007334\n",
            "Done 300 step, Loss: 0.008469\n",
            "Done 400 step, Loss: 0.009054\n",
            "Done 500 step, Loss: 0.009397\n",
            "Done 600 step, Loss: 0.009620\n",
            "Done 700 step, Loss: 0.009757\n",
            "Done 800 step, Loss: 0.009882\n",
            "Done 900 step, Loss: 0.009967\n",
            "Done 1000 step, Loss: 0.010028\n",
            "Done 1100 step, Loss: 0.010086\n",
            "Done 1200 step, Loss: 0.010126\n",
            "Done 1300 step, Loss: 0.010161\n",
            "Done 1400 step, Loss: 0.010193\n",
            "Done 1500 step, Loss: 0.010220\n",
            "Done 1600 step, Loss: 0.010235\n",
            "Done 1700 step, Loss: 0.010255\n",
            "Done 1800 step, Loss: 0.010271\n",
            "Done 1900 step, Loss: 0.010279\n",
            "Done 2000 step, Loss: 0.010291\n",
            "Done 2100 step, Loss: 0.010303\n",
            "Done 2200 step, Loss: 0.010306\n",
            "Done 2300 step, Loss: 0.010314\n",
            "Done 2400 step, Loss: 0.010317\n",
            "Done 2500 step, Loss: 0.010323\n",
            "Done 2600 step, Loss: 0.010328\n",
            "Done 2700 step, Loss: 0.010330\n",
            "Done 2800 step, Loss: 0.010331\n",
            "Done 2900 step, Loss: 0.010334\n",
            "Done 3000 step, Loss: 0.010337\n",
            "Done 3100 step, Loss: 0.010335\n",
            "Done 3200 step, Loss: 0.010337\n",
            "Done 3300 step, Loss: 0.010338\n",
            "Done 3400 step, Loss: 0.010337\n",
            "Done 3500 step, Loss: 0.010338\n",
            "Done 3600 step, Loss: 0.010337\n",
            "Saving checkpoint for iteration 2 at /content/drive/My Drive/abae_logs/checkpoints/abae/train/ckpt-2\n",
            "3630 tf.Tensor(0.010334699, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.010367\n",
            "Done 200 step, Loss: 0.010344\n",
            "Done 300 step, Loss: 0.010302\n",
            "Done 400 step, Loss: 0.010306\n",
            "Done 500 step, Loss: 0.010308\n",
            "Done 600 step, Loss: 0.010291\n",
            "Done 700 step, Loss: 0.010291\n",
            "Done 800 step, Loss: 0.010290\n",
            "Done 900 step, Loss: 0.010285\n",
            "Done 1000 step, Loss: 0.010271\n",
            "Done 1100 step, Loss: 0.010273\n",
            "Done 1200 step, Loss: 0.010263\n",
            "Done 1300 step, Loss: 0.010261\n",
            "Done 1400 step, Loss: 0.010258\n",
            "Done 1500 step, Loss: 0.010257\n",
            "Done 1600 step, Loss: 0.010246\n",
            "Done 1700 step, Loss: 0.010245\n",
            "Done 1800 step, Loss: 0.010243\n",
            "Done 1900 step, Loss: 0.010234\n",
            "Done 2000 step, Loss: 0.010233\n",
            "Done 2100 step, Loss: 0.010226\n",
            "Done 2200 step, Loss: 0.010224\n",
            "Done 2300 step, Loss: 0.010221\n",
            "Done 2400 step, Loss: 0.010214\n",
            "Done 2500 step, Loss: 0.010212\n",
            "Done 2600 step, Loss: 0.010209\n",
            "Done 2700 step, Loss: 0.010203\n",
            "Done 2800 step, Loss: 0.010200\n",
            "Done 2900 step, Loss: 0.010197\n",
            "Done 3000 step, Loss: 0.010194\n",
            "Done 3100 step, Loss: 0.010188\n",
            "Done 3200 step, Loss: 0.010185\n",
            "Done 3300 step, Loss: 0.010179\n",
            "Done 3400 step, Loss: 0.010176\n",
            "Done 3500 step, Loss: 0.010173\n",
            "Done 3600 step, Loss: 0.010170\n",
            "Saving checkpoint for iteration 3 at /content/drive/My Drive/abae_logs/checkpoints/abae/train/ckpt-3\n",
            "3630 tf.Tensor(0.010170146, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.009935\n",
            "Done 200 step, Loss: 0.009993\n",
            "Done 300 step, Loss: 0.010009\n",
            "Done 400 step, Loss: 0.009998\n",
            "Done 500 step, Loss: 0.010004\n",
            "Done 600 step, Loss: 0.010006\n",
            "Done 700 step, Loss: 0.009991\n",
            "Done 800 step, Loss: 0.009995\n",
            "Done 900 step, Loss: 0.009998\n",
            "Done 1000 step, Loss: 0.009988\n",
            "Done 1100 step, Loss: 0.009989\n",
            "Done 1200 step, Loss: 0.009986\n",
            "Done 1300 step, Loss: 0.009980\n",
            "Done 1400 step, Loss: 0.009979\n",
            "Done 1500 step, Loss: 0.009978\n",
            "Done 1600 step, Loss: 0.009972\n",
            "Done 1700 step, Loss: 0.009971\n",
            "Done 1800 step, Loss: 0.009970\n",
            "Done 1900 step, Loss: 0.009963\n",
            "Done 2000 step, Loss: 0.009962\n",
            "Done 2100 step, Loss: 0.009960\n",
            "Done 2200 step, Loss: 0.009954\n",
            "Done 2300 step, Loss: 0.009952\n",
            "Done 2400 step, Loss: 0.009950\n",
            "Done 2500 step, Loss: 0.009945\n",
            "Done 2600 step, Loss: 0.009943\n",
            "Done 2700 step, Loss: 0.009937\n",
            "Done 2800 step, Loss: 0.009935\n",
            "Done 2900 step, Loss: 0.009933\n",
            "Done 3000 step, Loss: 0.009930\n",
            "Done 3100 step, Loss: 0.009925\n",
            "Done 3200 step, Loss: 0.009922\n",
            "Done 3300 step, Loss: 0.009921\n",
            "Done 3400 step, Loss: 0.009916\n",
            "Done 3500 step, Loss: 0.009911\n",
            "Done 3600 step, Loss: 0.009911\n",
            "Saving checkpoint for iteration 4 at /content/drive/My Drive/abae_logs/checkpoints/abae/train/ckpt-4\n",
            "3630 tf.Tensor(0.0099106, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.009726\n",
            "Done 200 step, Loss: 0.009785\n",
            "Done 300 step, Loss: 0.009808\n",
            "Done 400 step, Loss: 0.009788\n",
            "Done 500 step, Loss: 0.009793\n",
            "Done 600 step, Loss: 0.009798\n",
            "Done 700 step, Loss: 0.009781\n",
            "Done 800 step, Loss: 0.009784\n",
            "Done 900 step, Loss: 0.009787\n",
            "Done 1000 step, Loss: 0.009776\n",
            "Done 1100 step, Loss: 0.009778\n",
            "Done 1200 step, Loss: 0.009776\n",
            "Done 1300 step, Loss: 0.009769\n",
            "Done 1400 step, Loss: 0.009770\n",
            "Done 1500 step, Loss: 0.009770\n",
            "Done 1600 step, Loss: 0.009760\n",
            "Done 1700 step, Loss: 0.009759\n",
            "Done 1800 step, Loss: 0.009760\n",
            "Done 1900 step, Loss: 0.009753\n",
            "Done 2000 step, Loss: 0.009752\n",
            "Done 2100 step, Loss: 0.009750\n",
            "Done 2200 step, Loss: 0.009746\n",
            "Done 2300 step, Loss: 0.009744\n",
            "Done 2400 step, Loss: 0.009741\n",
            "Done 2500 step, Loss: 0.009737\n",
            "Done 2600 step, Loss: 0.009736\n",
            "Done 2700 step, Loss: 0.009735\n",
            "Done 2800 step, Loss: 0.009728\n",
            "Done 2900 step, Loss: 0.009727\n",
            "Done 3000 step, Loss: 0.009725\n",
            "Done 3100 step, Loss: 0.009720\n",
            "Done 3200 step, Loss: 0.009718\n",
            "Done 3300 step, Loss: 0.009716\n",
            "Done 3400 step, Loss: 0.009712\n",
            "Done 3500 step, Loss: 0.009711\n",
            "Done 3600 step, Loss: 0.009708\n",
            "Saving checkpoint for iteration 5 at /content/drive/My Drive/abae_logs/checkpoints/abae/train/ckpt-5\n",
            "3630 tf.Tensor(0.009707838, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.009555\n",
            "Done 200 step, Loss: 0.009585\n",
            "Done 300 step, Loss: 0.009611\n",
            "Done 400 step, Loss: 0.009585\n",
            "Done 500 step, Loss: 0.009592\n",
            "Done 600 step, Loss: 0.009587\n",
            "Done 700 step, Loss: 0.009587\n",
            "Done 800 step, Loss: 0.009590\n",
            "Done 900 step, Loss: 0.009581\n",
            "Done 1000 step, Loss: 0.009579\n",
            "Done 1100 step, Loss: 0.009581\n",
            "Done 1200 step, Loss: 0.009576\n",
            "Done 1300 step, Loss: 0.009574\n",
            "Done 1400 step, Loss: 0.009575\n",
            "Done 1500 step, Loss: 0.009569\n",
            "Done 1600 step, Loss: 0.009567\n",
            "Done 1700 step, Loss: 0.009567\n",
            "Done 1800 step, Loss: 0.009568\n",
            "Done 1900 step, Loss: 0.009562\n",
            "Done 2000 step, Loss: 0.009562\n",
            "Done 2100 step, Loss: 0.009556\n",
            "Done 2200 step, Loss: 0.009556\n",
            "Done 2300 step, Loss: 0.009555\n",
            "Done 2400 step, Loss: 0.009552\n",
            "Done 2500 step, Loss: 0.009549\n",
            "Done 2600 step, Loss: 0.009548\n",
            "Done 2700 step, Loss: 0.009548\n",
            "Done 2800 step, Loss: 0.009543\n",
            "Done 2900 step, Loss: 0.009543\n",
            "Done 3000 step, Loss: 0.009539\n",
            "Done 3100 step, Loss: 0.009537\n",
            "Done 3200 step, Loss: 0.009536\n",
            "Done 3300 step, Loss: 0.009536\n",
            "Done 3400 step, Loss: 0.009531\n",
            "Done 3500 step, Loss: 0.009531\n",
            "Done 3600 step, Loss: 0.009528\n",
            "Saving checkpoint for iteration 6 at /content/drive/My Drive/abae_logs/checkpoints/abae/train/ckpt-6\n",
            "3630 tf.Tensor(0.009528587, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.009403\n",
            "Done 200 step, Loss: 0.009448\n",
            "Done 300 step, Loss: 0.009416\n",
            "Done 400 step, Loss: 0.009433\n",
            "Done 500 step, Loss: 0.009442\n",
            "Done 600 step, Loss: 0.009447\n",
            "Done 700 step, Loss: 0.009436\n",
            "Done 800 step, Loss: 0.009442\n",
            "Done 900 step, Loss: 0.009441\n",
            "Done 1000 step, Loss: 0.009437\n",
            "Done 1100 step, Loss: 0.009439\n",
            "Done 1200 step, Loss: 0.009434\n",
            "Done 1300 step, Loss: 0.009432\n",
            "Done 1400 step, Loss: 0.009433\n",
            "Done 1500 step, Loss: 0.009432\n",
            "Done 1600 step, Loss: 0.009427\n",
            "Done 1700 step, Loss: 0.009429\n",
            "Done 1800 step, Loss: 0.009428\n",
            "Done 1900 step, Loss: 0.009424\n",
            "Done 2000 step, Loss: 0.009425\n",
            "Done 2100 step, Loss: 0.009425\n",
            "Done 2200 step, Loss: 0.009421\n",
            "Done 2300 step, Loss: 0.009420\n",
            "Done 2400 step, Loss: 0.009422\n",
            "Done 2500 step, Loss: 0.009416\n",
            "Done 2600 step, Loss: 0.009416\n",
            "Done 2700 step, Loss: 0.009414\n",
            "Done 2800 step, Loss: 0.009413\n",
            "Done 2900 step, Loss: 0.009413\n",
            "Done 3000 step, Loss: 0.009410\n",
            "Done 3100 step, Loss: 0.009408\n",
            "Done 3200 step, Loss: 0.009408\n",
            "Done 3300 step, Loss: 0.009407\n",
            "Done 3400 step, Loss: 0.009404\n",
            "Done 3500 step, Loss: 0.009404\n",
            "Done 3600 step, Loss: 0.009401\n",
            "Saving checkpoint for iteration 7 at /content/drive/My Drive/abae_logs/checkpoints/abae/train/ckpt-7\n",
            "3630 tf.Tensor(0.009401672, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.009321\n",
            "Done 200 step, Loss: 0.009354\n",
            "Done 300 step, Loss: 0.009334\n",
            "Done 400 step, Loss: 0.009335\n",
            "Done 500 step, Loss: 0.009341\n",
            "Done 600 step, Loss: 0.009346\n",
            "Done 700 step, Loss: 0.009335\n",
            "Done 800 step, Loss: 0.009337\n",
            "Done 900 step, Loss: 0.009333\n",
            "Done 1000 step, Loss: 0.009330\n",
            "Done 1100 step, Loss: 0.009332\n",
            "Done 1200 step, Loss: 0.009329\n",
            "Done 1300 step, Loss: 0.009326\n",
            "Done 1400 step, Loss: 0.009328\n",
            "Done 1500 step, Loss: 0.009329\n",
            "Done 1600 step, Loss: 0.009321\n",
            "Done 1700 step, Loss: 0.009322\n",
            "Done 1800 step, Loss: 0.009323\n",
            "Done 1900 step, Loss: 0.009318\n",
            "Done 2000 step, Loss: 0.009318\n",
            "Done 2100 step, Loss: 0.009316\n",
            "Done 2200 step, Loss: 0.009314\n",
            "Done 2300 step, Loss: 0.009314\n",
            "Done 2400 step, Loss: 0.009312\n",
            "Done 2500 step, Loss: 0.009308\n",
            "Done 2600 step, Loss: 0.009309\n",
            "Done 2700 step, Loss: 0.009309\n",
            "Done 2800 step, Loss: 0.009305\n",
            "Done 2900 step, Loss: 0.009305\n",
            "Done 3000 step, Loss: 0.009302\n",
            "Done 3100 step, Loss: 0.009301\n",
            "Done 3200 step, Loss: 0.009299\n",
            "Done 3300 step, Loss: 0.009299\n",
            "Done 3400 step, Loss: 0.009295\n",
            "Done 3500 step, Loss: 0.009295\n",
            "Done 3600 step, Loss: 0.009294\n",
            "Saving checkpoint for iteration 8 at /content/drive/My Drive/abae_logs/checkpoints/abae/train/ckpt-8\n",
            "3630 tf.Tensor(0.009293403, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.009187\n",
            "Done 200 step, Loss: 0.009221\n",
            "Done 300 step, Loss: 0.009232\n",
            "Done 400 step, Loss: 0.009217\n",
            "Done 500 step, Loss: 0.009220\n",
            "Done 600 step, Loss: 0.009227\n",
            "Done 700 step, Loss: 0.009216\n",
            "Done 800 step, Loss: 0.009219\n",
            "Done 900 step, Loss: 0.009218\n",
            "Done 1000 step, Loss: 0.009215\n",
            "Done 1100 step, Loss: 0.009218\n",
            "Done 1200 step, Loss: 0.009219\n",
            "Done 1300 step, Loss: 0.009210\n",
            "Done 1400 step, Loss: 0.009211\n",
            "Done 1500 step, Loss: 0.009211\n",
            "Done 1600 step, Loss: 0.009207\n",
            "Done 1700 step, Loss: 0.009208\n",
            "Done 1800 step, Loss: 0.009208\n",
            "Done 1900 step, Loss: 0.009203\n",
            "Done 2000 step, Loss: 0.009203\n",
            "Done 2100 step, Loss: 0.009201\n",
            "Done 2200 step, Loss: 0.009199\n",
            "Done 2300 step, Loss: 0.009199\n",
            "Done 2400 step, Loss: 0.009194\n",
            "Done 2500 step, Loss: 0.009195\n",
            "Done 2600 step, Loss: 0.009194\n",
            "Done 2700 step, Loss: 0.009194\n",
            "Done 2800 step, Loss: 0.009190\n",
            "Done 2900 step, Loss: 0.009190\n",
            "Done 3000 step, Loss: 0.009188\n",
            "Done 3100 step, Loss: 0.009187\n",
            "Done 3200 step, Loss: 0.009186\n",
            "Done 3300 step, Loss: 0.009185\n",
            "Done 3400 step, Loss: 0.009182\n",
            "Done 3500 step, Loss: 0.009182\n",
            "Done 3600 step, Loss: 0.009179\n",
            "Saving checkpoint for iteration 9 at /content/drive/My Drive/abae_logs/checkpoints/abae/train/ckpt-9\n",
            "3630 tf.Tensor(0.00918035, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.009085\n",
            "Done 200 step, Loss: 0.009117\n",
            "Done 300 step, Loss: 0.009124\n",
            "Done 400 step, Loss: 0.009110\n",
            "Done 500 step, Loss: 0.009116\n",
            "Done 600 step, Loss: 0.009104\n",
            "Done 700 step, Loss: 0.009108\n",
            "Done 800 step, Loss: 0.009114\n",
            "Done 900 step, Loss: 0.009118\n",
            "Done 1000 step, Loss: 0.009111\n",
            "Done 1100 step, Loss: 0.009113\n",
            "Done 1200 step, Loss: 0.009115\n",
            "Done 1300 step, Loss: 0.009108\n",
            "Done 1400 step, Loss: 0.009110\n",
            "Done 1500 step, Loss: 0.009109\n",
            "Done 1600 step, Loss: 0.009106\n",
            "Done 1700 step, Loss: 0.009107\n",
            "Done 1800 step, Loss: 0.009106\n",
            "Done 1900 step, Loss: 0.009103\n",
            "Done 2000 step, Loss: 0.009104\n",
            "Done 2100 step, Loss: 0.009105\n",
            "Done 2200 step, Loss: 0.009099\n",
            "Done 2300 step, Loss: 0.009100\n",
            "Done 2400 step, Loss: 0.009097\n",
            "Done 2500 step, Loss: 0.009096\n",
            "Done 2600 step, Loss: 0.009097\n",
            "Done 2700 step, Loss: 0.009096\n",
            "Done 2800 step, Loss: 0.009093\n",
            "Done 2900 step, Loss: 0.009093\n",
            "Done 3000 step, Loss: 0.009090\n",
            "Done 3100 step, Loss: 0.009090\n",
            "Done 3200 step, Loss: 0.009089\n",
            "Done 3300 step, Loss: 0.009089\n",
            "Done 3400 step, Loss: 0.009086\n",
            "Done 3500 step, Loss: 0.009086\n",
            "Done 3600 step, Loss: 0.009084\n",
            "Saving checkpoint for iteration 10 at /content/drive/My Drive/abae_logs/checkpoints/abae/train/ckpt-10\n",
            "3630 tf.Tensor(0.009084691, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.009001\n",
            "Done 200 step, Loss: 0.009040\n",
            "Done 300 step, Loss: 0.009018\n",
            "Done 400 step, Loss: 0.009027\n",
            "Done 500 step, Loss: 0.009033\n",
            "Done 600 step, Loss: 0.009037\n",
            "Done 700 step, Loss: 0.009029\n",
            "Done 800 step, Loss: 0.009028\n",
            "Done 900 step, Loss: 0.009027\n",
            "Done 1000 step, Loss: 0.009024\n",
            "Done 1100 step, Loss: 0.009024\n",
            "Done 1200 step, Loss: 0.009025\n",
            "Done 1300 step, Loss: 0.009021\n",
            "Done 1400 step, Loss: 0.009021\n",
            "Done 1500 step, Loss: 0.009023\n",
            "Done 1600 step, Loss: 0.009018\n",
            "Done 1700 step, Loss: 0.009019\n",
            "Done 1800 step, Loss: 0.009019\n",
            "Done 1900 step, Loss: 0.009015\n",
            "Done 2000 step, Loss: 0.009016\n",
            "Done 2100 step, Loss: 0.009011\n",
            "Done 2200 step, Loss: 0.009012\n",
            "Done 2300 step, Loss: 0.009012\n",
            "Done 2400 step, Loss: 0.009012\n",
            "Done 2500 step, Loss: 0.009009\n",
            "Done 2600 step, Loss: 0.009009\n",
            "Done 2700 step, Loss: 0.009006\n",
            "Done 2800 step, Loss: 0.009006\n",
            "Done 2900 step, Loss: 0.009005\n",
            "Done 3000 step, Loss: 0.009005\n",
            "Done 3100 step, Loss: 0.009001\n",
            "Done 3200 step, Loss: 0.009002\n",
            "Done 3300 step, Loss: 0.008999\n",
            "Done 3400 step, Loss: 0.008998\n",
            "Done 3500 step, Loss: 0.008998\n",
            "Done 3600 step, Loss: 0.008995\n",
            "Saving checkpoint for iteration 11 at /content/drive/My Drive/abae_logs/checkpoints/abae/train/ckpt-11\n",
            "3630 tf.Tensor(0.008994485, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.008986\n",
            "Done 200 step, Loss: 0.008977\n",
            "Done 300 step, Loss: 0.008977\n",
            "Done 400 step, Loss: 0.008955\n",
            "Done 500 step, Loss: 0.008957\n",
            "Done 600 step, Loss: 0.008948\n",
            "Done 700 step, Loss: 0.008946\n",
            "Done 800 step, Loss: 0.008948\n",
            "Done 900 step, Loss: 0.008951\n",
            "Done 1000 step, Loss: 0.008941\n",
            "Done 1100 step, Loss: 0.008943\n",
            "Done 1200 step, Loss: 0.008945\n",
            "Done 1300 step, Loss: 0.008938\n",
            "Done 1400 step, Loss: 0.008940\n",
            "Done 1500 step, Loss: 0.008936\n",
            "Done 1600 step, Loss: 0.008935\n",
            "Done 1700 step, Loss: 0.008934\n",
            "Done 1800 step, Loss: 0.008936\n",
            "Done 1900 step, Loss: 0.008930\n",
            "Done 2000 step, Loss: 0.008931\n",
            "Done 2100 step, Loss: 0.008929\n",
            "Done 2200 step, Loss: 0.008926\n",
            "Done 2300 step, Loss: 0.008926\n",
            "Done 2400 step, Loss: 0.008927\n",
            "Done 2500 step, Loss: 0.008923\n",
            "Done 2600 step, Loss: 0.008923\n",
            "Done 2700 step, Loss: 0.008923\n",
            "Done 2800 step, Loss: 0.008919\n",
            "Done 2900 step, Loss: 0.008919\n",
            "Done 3000 step, Loss: 0.008917\n",
            "Done 3100 step, Loss: 0.008915\n",
            "Done 3200 step, Loss: 0.008915\n",
            "Done 3300 step, Loss: 0.008915\n",
            "Done 3400 step, Loss: 0.008912\n",
            "Done 3500 step, Loss: 0.008912\n",
            "Done 3600 step, Loss: 0.008912\n",
            "Saving checkpoint for iteration 12 at /content/drive/My Drive/abae_logs/checkpoints/abae/train/ckpt-12\n",
            "3630 tf.Tensor(0.008910726, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.008829\n",
            "Done 200 step, Loss: 0.008852\n",
            "Done 300 step, Loss: 0.008871\n",
            "Done 400 step, Loss: 0.008852\n",
            "Done 500 step, Loss: 0.008859\n",
            "Done 600 step, Loss: 0.008849\n",
            "Done 700 step, Loss: 0.008852\n",
            "Done 800 step, Loss: 0.008856\n",
            "Done 900 step, Loss: 0.008858\n",
            "Done 1000 step, Loss: 0.008852\n",
            "Done 1100 step, Loss: 0.008853\n",
            "Done 1200 step, Loss: 0.008855\n",
            "Done 1300 step, Loss: 0.008848\n",
            "Done 1400 step, Loss: 0.008849\n",
            "Done 1500 step, Loss: 0.008848\n",
            "Done 1600 step, Loss: 0.008846\n",
            "Done 1700 step, Loss: 0.008847\n",
            "Done 1800 step, Loss: 0.008843\n",
            "Done 1900 step, Loss: 0.008842\n",
            "Done 2000 step, Loss: 0.008842\n",
            "Done 2100 step, Loss: 0.008843\n",
            "Done 2200 step, Loss: 0.008838\n",
            "Done 2300 step, Loss: 0.008839\n",
            "Done 2400 step, Loss: 0.008836\n",
            "Done 2500 step, Loss: 0.008836\n",
            "Done 2600 step, Loss: 0.008835\n",
            "Done 2700 step, Loss: 0.008836\n",
            "Done 2800 step, Loss: 0.008832\n",
            "Done 2900 step, Loss: 0.008832\n",
            "Done 3000 step, Loss: 0.008830\n",
            "Done 3100 step, Loss: 0.008829\n",
            "Done 3200 step, Loss: 0.008829\n",
            "Done 3300 step, Loss: 0.008829\n",
            "Done 3400 step, Loss: 0.008825\n",
            "Done 3500 step, Loss: 0.008824\n",
            "Done 3600 step, Loss: 0.008824\n",
            "Saving checkpoint for iteration 13 at /content/drive/My Drive/abae_logs/checkpoints/abae/train/ckpt-13\n",
            "3630 tf.Tensor(0.008824019, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.008722\n",
            "Done 200 step, Loss: 0.008767\n",
            "Done 300 step, Loss: 0.008756\n",
            "Done 400 step, Loss: 0.008767\n",
            "Done 500 step, Loss: 0.008773\n",
            "Done 600 step, Loss: 0.008780\n",
            "Done 700 step, Loss: 0.008768\n",
            "Done 800 step, Loss: 0.008772\n",
            "Done 900 step, Loss: 0.008770\n",
            "Done 1000 step, Loss: 0.008769\n",
            "Done 1100 step, Loss: 0.008770\n",
            "Done 1200 step, Loss: 0.008768\n",
            "Done 1300 step, Loss: 0.008766\n",
            "Done 1400 step, Loss: 0.008768\n",
            "Done 1500 step, Loss: 0.008764\n",
            "Done 1600 step, Loss: 0.008762\n",
            "Done 1700 step, Loss: 0.008762\n",
            "Done 1800 step, Loss: 0.008763\n",
            "Done 1900 step, Loss: 0.008759\n",
            "Done 2000 step, Loss: 0.008760\n",
            "Done 2100 step, Loss: 0.008755\n",
            "Done 2200 step, Loss: 0.008756\n",
            "Done 2300 step, Loss: 0.008756\n",
            "Done 2400 step, Loss: 0.008755\n",
            "Done 2500 step, Loss: 0.008754\n",
            "Done 2600 step, Loss: 0.008754\n",
            "Done 2700 step, Loss: 0.008754\n",
            "Done 2800 step, Loss: 0.008750\n",
            "Done 2900 step, Loss: 0.008750\n",
            "Done 3000 step, Loss: 0.008747\n",
            "Done 3100 step, Loss: 0.008747\n",
            "Done 3200 step, Loss: 0.008747\n",
            "Done 3300 step, Loss: 0.008747\n",
            "Done 3400 step, Loss: 0.008743\n",
            "Done 3500 step, Loss: 0.008743\n",
            "Done 3600 step, Loss: 0.008741\n",
            "Saving checkpoint for iteration 14 at /content/drive/My Drive/abae_logs/checkpoints/abae/train/ckpt-14\n",
            "3630 tf.Tensor(0.008742294, shape=(), dtype=float32)\n",
            "Done 100 step, Loss: 0.008653\n",
            "Done 200 step, Loss: 0.008693\n",
            "Done 300 step, Loss: 0.008671\n",
            "Done 400 step, Loss: 0.008686\n",
            "Done 500 step, Loss: 0.008690\n",
            "Done 600 step, Loss: 0.008693\n",
            "Done 700 step, Loss: 0.008684\n",
            "Done 800 step, Loss: 0.008689\n",
            "Done 900 step, Loss: 0.008679\n",
            "Done 1000 step, Loss: 0.008682\n",
            "Done 1100 step, Loss: 0.008685\n",
            "Done 1200 step, Loss: 0.008680\n",
            "Done 1300 step, Loss: 0.008680\n",
            "Done 1400 step, Loss: 0.008681\n",
            "Done 1500 step, Loss: 0.008682\n",
            "Done 1600 step, Loss: 0.008678\n",
            "Done 1700 step, Loss: 0.008680\n",
            "Done 1800 step, Loss: 0.008679\n",
            "Done 1900 step, Loss: 0.008674\n",
            "Done 2000 step, Loss: 0.008676\n",
            "Done 2100 step, Loss: 0.008671\n",
            "Done 2200 step, Loss: 0.008671\n",
            "Done 2300 step, Loss: 0.008672\n",
            "Done 2400 step, Loss: 0.008670\n",
            "Done 2500 step, Loss: 0.008668\n",
            "Done 2600 step, Loss: 0.008668\n",
            "Done 2700 step, Loss: 0.008669\n",
            "Done 2800 step, Loss: 0.008666\n",
            "Done 2900 step, Loss: 0.008665\n",
            "Done 3000 step, Loss: 0.008665\n",
            "Done 3100 step, Loss: 0.008662\n",
            "Done 3200 step, Loss: 0.008662\n",
            "Done 3300 step, Loss: 0.008662\n",
            "Done 3400 step, Loss: 0.008659\n",
            "Done 3500 step, Loss: 0.008659\n",
            "Done 3600 step, Loss: 0.008659\n",
            "Saving checkpoint for iteration 15 at /content/drive/My Drive/abae_logs/checkpoints/abae/train/ckpt-15\n",
            "3630 tf.Tensor(0.008657179, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwLfrjc3JFkU"
      },
      "source": [
        "#### Loss - plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPG-sV2kM6HB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "49c21535-a847-4e85-a316-d51d8a29dc3c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(loss_list[1:])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dU9_GOC5W8P",
        "outputId": "062fd6b6-42cf-4216-876f-db5d2907606c"
      },
      "source": [
        "abae.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model__1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_layer (Embedding)  multiple                  699600    \n",
            "_________________________________________________________________\n",
            "attention_1 (Attention)      multiple                  10000     \n",
            "_________________________________________________________________\n",
            "dim_reduction_layer (Dense)  multiple                  1010      \n",
            "_________________________________________________________________\n",
            "final_dense (Dense)          multiple                  1100      \n",
            "=================================================================\n",
            "Total params: 711,710\n",
            "Trainable params: 711,710\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZL3E3W4M5FT",
        "outputId": "b9beee10-c6df-4e99-e53e-02e9a8c39d3b"
      },
      "source": [
        "for i in abae.weights:\n",
        "  print(i.name, i.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model__1/embedding_layer/embeddings:0 (6996, 100)\n",
            "Variable:0 (100, 100)\n",
            "model__1/dim_reduction_layer/kernel:0 (100, 10)\n",
            "model__1/dim_reduction_layer/bias:0 (10,)\n",
            "model__1/final_dense/kernel:0 (10, 100)\n",
            "model__1/final_dense/bias:0 (100,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2YfYl7XYhm9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDdfM6jYbuOf"
      },
      "source": [
        "topics=abae.weights[4].numpy()\n",
        "topic_words={}\n",
        "k=1\n",
        "for i in range(len(topics)):\n",
        "  topic_words[\"topic_\"+str(k)]=np.argsort(model.wv.cosine_similarities(topics[i],model.wv.vectors))[::-1][0:20]\n",
        "  k=k+1\n",
        "pd.DataFrame(topic_words).applymap(lambda x:model.wv.index2word[x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9H2nJKnJQeO"
      },
      "source": [
        "#### Aspects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "6muA-gX94FNw",
        "outputId": "ba0a4e54-c217-4b58-cc50-2ecb203e7ec2"
      },
      "source": [
        "pd.DataFrame(topic_words).applymap(lambda x:model.wv.index2word[x])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic_1</th>\n",
              "      <th>topic_2</th>\n",
              "      <th>topic_3</th>\n",
              "      <th>topic_4</th>\n",
              "      <th>topic_5</th>\n",
              "      <th>topic_6</th>\n",
              "      <th>topic_7</th>\n",
              "      <th>topic_8</th>\n",
              "      <th>topic_9</th>\n",
              "      <th>topic_10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rs</td>\n",
              "      <td>amoled</td>\n",
              "      <td>deliver</td>\n",
              "      <td>awesome</td>\n",
              "      <td>feels</td>\n",
              "      <td>achha</td>\n",
              "      <td>turn</td>\n",
              "      <td>stunning</td>\n",
              "      <td>nice</td>\n",
              "      <td>fraud</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>billion</td>\n",
              "      <td>stunning</td>\n",
              "      <td>shipping</td>\n",
              "      <td>really</td>\n",
              "      <td>classy</td>\n",
              "      <td>koi</td>\n",
              "      <td>notifications</td>\n",
              "      <td>disappoint</td>\n",
              "      <td>good</td>\n",
              "      <td>seller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>discount</td>\n",
              "      <td>amazing</td>\n",
              "      <td>ekart</td>\n",
              "      <td>phone</td>\n",
              "      <td>grip</td>\n",
              "      <td>mast</td>\n",
              "      <td>app</td>\n",
              "      <td>great</td>\n",
              "      <td>nicr</td>\n",
              "      <td>refused</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>offer</td>\n",
              "      <td>cameras</td>\n",
              "      <td>30th</td>\n",
              "      <td>good</td>\n",
              "      <td>sleek</td>\n",
              "      <td>hai</td>\n",
              "      <td>disable</td>\n",
              "      <td>awesome</td>\n",
              "      <td>awesome</td>\n",
              "      <td>fault</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3500</td>\n",
              "      <td>awesome</td>\n",
              "      <td>ordered</td>\n",
              "      <td>superb</td>\n",
              "      <td>premium</td>\n",
              "      <td>ko</td>\n",
              "      <td>access</td>\n",
              "      <td>really</td>\n",
              "      <td>wesome</td>\n",
              "      <td>sent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>discounts</td>\n",
              "      <td>brilliant</td>\n",
              "      <td>delivered</td>\n",
              "      <td>also</td>\n",
              "      <td>design</td>\n",
              "      <td>acha</td>\n",
              "      <td>application</td>\n",
              "      <td>amazing</td>\n",
              "      <td>greate</td>\n",
              "      <td>executives</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>13500</td>\n",
              "      <td>outstanding</td>\n",
              "      <td>delivery</td>\n",
              "      <td>one</td>\n",
              "      <td>looks</td>\n",
              "      <td>nhi</td>\n",
              "      <td>vibrate</td>\n",
              "      <td>indeed</td>\n",
              "      <td>thanku</td>\n",
              "      <td>accepting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3k</td>\n",
              "      <td>ai</td>\n",
              "      <td>flipkart</td>\n",
              "      <td>great</td>\n",
              "      <td>handy</td>\n",
              "      <td>kharab</td>\n",
              "      <td>irritates</td>\n",
              "      <td>beast</td>\n",
              "      <td>avarge</td>\n",
              "      <td>accept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>13000</td>\n",
              "      <td>decent</td>\n",
              "      <td>saturday</td>\n",
              "      <td>excellent</td>\n",
              "      <td>hold</td>\n",
              "      <td>ek</td>\n",
              "      <td>bluetooth</td>\n",
              "      <td>loving</td>\n",
              "      <td>mast</td>\n",
              "      <td>replacement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>8999</td>\n",
              "      <td>really</td>\n",
              "      <td>oct</td>\n",
              "      <td>battery</td>\n",
              "      <td>slippery</td>\n",
              "      <td>ki</td>\n",
              "      <td>button</td>\n",
              "      <td>outstanding</td>\n",
              "      <td>fantastic</td>\n",
              "      <td>flipkart</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>5k</td>\n",
              "      <td>especially</td>\n",
              "      <td>seller</td>\n",
              "      <td>amazing</td>\n",
              "      <td>sturdy</td>\n",
              "      <td>mai</td>\n",
              "      <td>disabled</td>\n",
              "      <td>truly</td>\n",
              "      <td>wsome</td>\n",
              "      <td>electronics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12000</td>\n",
              "      <td>great</td>\n",
              "      <td>order</td>\n",
              "      <td>fantastic</td>\n",
              "      <td>compact</td>\n",
              "      <td>kam</td>\n",
              "      <td>launcher</td>\n",
              "      <td>incredible</td>\n",
              "      <td>osm</td>\n",
              "      <td>send</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12999</td>\n",
              "      <td>natural</td>\n",
              "      <td>august</td>\n",
              "      <td>actually</td>\n",
              "      <td>stunning</td>\n",
              "      <td>bhi</td>\n",
              "      <td>icon</td>\n",
              "      <td>classy</td>\n",
              "      <td>delevry</td>\n",
              "      <td>customers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2999</td>\n",
              "      <td>makes</td>\n",
              "      <td>12th</td>\n",
              "      <td>design</td>\n",
              "      <td>hands</td>\n",
              "      <td>accha</td>\n",
              "      <td>rings</td>\n",
              "      <td>premium</td>\n",
              "      <td>vry</td>\n",
              "      <td>request</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>11700</td>\n",
              "      <td>stereo</td>\n",
              "      <td>packing</td>\n",
              "      <td>ok</td>\n",
              "      <td>pretty</td>\n",
              "      <td>ye</td>\n",
              "      <td>scroll</td>\n",
              "      <td>killer</td>\n",
              "      <td>also</td>\n",
              "      <td>saying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>8000</td>\n",
              "      <td>crisp</td>\n",
              "      <td>launch</td>\n",
              "      <td>mobile</td>\n",
              "      <td>hand</td>\n",
              "      <td>lekin</td>\n",
              "      <td>notification</td>\n",
              "      <td>beat</td>\n",
              "      <td>exelent</td>\n",
              "      <td>rejected</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>billions</td>\n",
              "      <td>captures</td>\n",
              "      <td>booked</td>\n",
              "      <td>outstanding</td>\n",
              "      <td>metallic</td>\n",
              "      <td>bahut</td>\n",
              "      <td>silent</td>\n",
              "      <td>terms</td>\n",
              "      <td>osom</td>\n",
              "      <td>staff</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>8k</td>\n",
              "      <td>specially</td>\n",
              "      <td>logistics</td>\n",
              "      <td>decent</td>\n",
              "      <td>glossy</td>\n",
              "      <td>ka</td>\n",
              "      <td>shortcut</td>\n",
              "      <td>brilliant</td>\n",
              "      <td>fabulous</td>\n",
              "      <td>genuine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>10000</td>\n",
              "      <td>camera</td>\n",
              "      <td>22nd</td>\n",
              "      <td>disappointed</td>\n",
              "      <td>elegant</td>\n",
              "      <td>kiya</td>\n",
              "      <td>apps</td>\n",
              "      <td>flagship</td>\n",
              "      <td>bettery</td>\n",
              "      <td>said</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>9000</td>\n",
              "      <td>display</td>\n",
              "      <td>dilevry</td>\n",
              "      <td>think</td>\n",
              "      <td>build</td>\n",
              "      <td>nahi</td>\n",
              "      <td>wifi</td>\n",
              "      <td>decent</td>\n",
              "      <td>delevery</td>\n",
              "      <td>return</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      topic_1      topic_2    topic_3  ...      topic_8    topic_9     topic_10\n",
              "0          rs       amoled    deliver  ...     stunning       nice        fraud\n",
              "1     billion     stunning   shipping  ...   disappoint       good       seller\n",
              "2    discount      amazing      ekart  ...        great       nicr      refused\n",
              "3       offer      cameras       30th  ...      awesome    awesome        fault\n",
              "4        3500      awesome    ordered  ...       really     wesome         sent\n",
              "5   discounts    brilliant  delivered  ...      amazing     greate   executives\n",
              "6       13500  outstanding   delivery  ...       indeed     thanku    accepting\n",
              "7          3k           ai   flipkart  ...        beast     avarge       accept\n",
              "8       13000       decent   saturday  ...       loving       mast  replacement\n",
              "9        8999       really        oct  ...  outstanding  fantastic     flipkart\n",
              "10         5k   especially     seller  ...        truly      wsome  electronics\n",
              "11      12000        great      order  ...   incredible        osm         send\n",
              "12      12999      natural     august  ...       classy    delevry    customers\n",
              "13       2999        makes       12th  ...      premium        vry      request\n",
              "14      11700       stereo    packing  ...       killer       also       saying\n",
              "15       8000        crisp     launch  ...         beat    exelent     rejected\n",
              "16   billions     captures     booked  ...        terms       osom        staff\n",
              "17         8k    specially  logistics  ...    brilliant   fabulous      genuine\n",
              "18      10000       camera       22nd  ...     flagship    bettery         said\n",
              "19       9000      display    dilevry  ...       decent   delevery       return\n",
              "\n",
              "[20 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a23a3UYL5L3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50041b23-304b-40e9-a28b-855634986cfd"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(362996, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng1xaE97krIJ",
        "outputId": "1a70c510-348e-458e-a71a-b6f173ab6691"
      },
      "source": [
        "actual=pd.read_pickle(\"/content/drive/My Drive/REVIEWS/reviews.pickle\")\n",
        "actual.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(363572, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgxMfn6Dk01Y",
        "outputId": "a8101ca7-0a97-4086-8f11-4ef69605b9b0"
      },
      "source": [
        "abae.weights[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'model_/embedding_layer/embeddings:0' shape=(6996, 100) dtype=float32, numpy=\n",
              "array([[ 0.01037038, -0.01036337, -0.01036488, ..., -0.01038307,\n",
              "        -0.01031592,  0.01036366],\n",
              "       [-0.12056135,  0.02119086, -0.3640991 , ...,  0.06567799,\n",
              "         0.12151092,  0.2287009 ],\n",
              "       [ 0.02999476, -0.09732407, -0.22800502, ..., -0.02116774,\n",
              "        -0.02559429,  0.17921406],\n",
              "       ...,\n",
              "       [-0.16476521, -0.31051284,  1.4704353 , ..., -0.34270912,\n",
              "        -0.9164348 ,  0.37284312],\n",
              "       [-0.1407243 , -0.76751727, -0.35094082, ..., -0.07251757,\n",
              "        -0.336739  , -0.24188508],\n",
              "       [-0.17931011,  0.65131986,  0.03546059, ...,  0.03621732,\n",
              "        -0.5659493 , -0.00681651]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMQgP2EbPokE"
      },
      "source": [
        "### Restoring the training of a model from the latest checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-mUED4CPkAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b2e3fb5-e8e9-4718-d983-2d3e552c7172"
      },
      "source": [
        "for i in datagen.take(1):\n",
        "  k=i\n",
        "test=model_(embed_inputdim,embed_outputdim,trained_weights,aspects_k,dense,inputlength)\n",
        "test(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(), dtype=float32, numpy=119.447205>,\n",
              " <tf.Tensor: shape=(100, 10), dtype=float32, numpy=\n",
              " array([[0.09102347, 0.09757339, 0.06039898, 0.08577572, 0.09481385,\n",
              "         0.11303271, 0.11888027, 0.1785904 , 0.07754516, 0.08236608],\n",
              "        [0.09461122, 0.10913482, 0.06149137, 0.08088474, 0.1029571 ,\n",
              "         0.09070311, 0.1022353 , 0.14011365, 0.11621718, 0.10165148],\n",
              "        [0.11325913, 0.13134882, 0.079891  , 0.07514646, 0.106176  ,\n",
              "         0.08472864, 0.10298117, 0.10068797, 0.12339724, 0.08238351],\n",
              "        [0.10802439, 0.10670919, 0.09470153, 0.0605552 , 0.11308976,\n",
              "         0.10538805, 0.07689005, 0.0986105 , 0.12019341, 0.11583799],\n",
              "        [0.11133365, 0.13383405, 0.06969249, 0.08906975, 0.08463766,\n",
              "         0.10438267, 0.10766024, 0.09024678, 0.11245451, 0.09668816],\n",
              "        [0.08561025, 0.11955554, 0.06765518, 0.07148482, 0.12827294,\n",
              "         0.0780909 , 0.10072514, 0.13991824, 0.11747786, 0.09120909],\n",
              "        [0.10363661, 0.10567259, 0.05974306, 0.07917746, 0.10175842,\n",
              "         0.10405948, 0.10772116, 0.1528896 , 0.10369561, 0.08164604],\n",
              "        [0.12175538, 0.13200529, 0.06985569, 0.08022   , 0.08718123,\n",
              "         0.10303218, 0.08974104, 0.08969654, 0.12200069, 0.10451201],\n",
              "        [0.12074658, 0.12566645, 0.07169454, 0.09779375, 0.07739361,\n",
              "         0.09515195, 0.0937648 , 0.10019705, 0.11473286, 0.10285838],\n",
              "        [0.09886338, 0.13146597, 0.08263955, 0.08002766, 0.10321559,\n",
              "         0.08248504, 0.09798323, 0.07692323, 0.1160745 , 0.13032185],\n",
              "        [0.10768379, 0.12375995, 0.06867782, 0.08449475, 0.08971034,\n",
              "         0.10698824, 0.09250081, 0.125634  , 0.10759638, 0.09295391],\n",
              "        [0.1095079 , 0.11815301, 0.05267476, 0.09652343, 0.10856581,\n",
              "         0.09292895, 0.10655212, 0.10432363, 0.10927723, 0.10149311],\n",
              "        [0.11577524, 0.16174659, 0.0630384 , 0.07515275, 0.09152203,\n",
              "         0.10552064, 0.09667058, 0.08077734, 0.11411998, 0.09567649],\n",
              "        [0.11165354, 0.13132979, 0.0747886 , 0.07666484, 0.09490213,\n",
              "         0.08831335, 0.09778915, 0.10095843, 0.11980331, 0.10379685],\n",
              "        [0.12083677, 0.1049542 , 0.06402447, 0.08391137, 0.09989762,\n",
              "         0.10578112, 0.09023485, 0.12568812, 0.11439031, 0.09028113],\n",
              "        [0.09843323, 0.13680828, 0.03890724, 0.0871855 , 0.12622038,\n",
              "         0.11501731, 0.0966212 , 0.12702113, 0.05922641, 0.11455935],\n",
              "        [0.09043989, 0.10481732, 0.12734039, 0.06700297, 0.06765928,\n",
              "         0.11421949, 0.11220605, 0.10140663, 0.1373363 , 0.07757163],\n",
              "        [0.10037941, 0.10212877, 0.05816561, 0.05847821, 0.11237638,\n",
              "         0.10653499, 0.07727214, 0.09529427, 0.1457999 , 0.14357029],\n",
              "        [0.09102347, 0.09757339, 0.06039898, 0.08577572, 0.09481385,\n",
              "         0.11303271, 0.11888027, 0.1785904 , 0.07754516, 0.08236608],\n",
              "        [0.13497874, 0.11316898, 0.06893055, 0.08821201, 0.10196709,\n",
              "         0.11843426, 0.08219808, 0.09632286, 0.09361298, 0.10217449],\n",
              "        [0.10966823, 0.10642995, 0.07438052, 0.07089973, 0.10733751,\n",
              "         0.09546182, 0.10941423, 0.12861413, 0.10282718, 0.0949667 ],\n",
              "        [0.09786605, 0.1234955 , 0.06196155, 0.0879958 , 0.1003615 ,\n",
              "         0.08923211, 0.10783166, 0.10132415, 0.12432933, 0.10560235],\n",
              "        [0.12234167, 0.12540686, 0.05500751, 0.09281339, 0.07861283,\n",
              "         0.12688003, 0.10271812, 0.10495544, 0.09444775, 0.09681638],\n",
              "        [0.11381571, 0.11925308, 0.07164586, 0.07838386, 0.09809804,\n",
              "         0.0928649 , 0.09612968, 0.09712642, 0.12101991, 0.11166254],\n",
              "        [0.12205154, 0.14286096, 0.07490735, 0.06937606, 0.09267161,\n",
              "         0.09108149, 0.09364349, 0.09014969, 0.1197153 , 0.10354251],\n",
              "        [0.11363789, 0.11863619, 0.05327193, 0.08498502, 0.07387763,\n",
              "         0.10953791, 0.10261922, 0.10421922, 0.13979392, 0.09942117],\n",
              "        [0.09409532, 0.1409202 , 0.04101231, 0.10315468, 0.07073195,\n",
              "         0.10606627, 0.10816003, 0.14257406, 0.11313847, 0.08014674],\n",
              "        [0.10280677, 0.13377252, 0.05566664, 0.09399246, 0.08218116,\n",
              "         0.10585705, 0.10044964, 0.1128806 , 0.12383468, 0.08855844],\n",
              "        [0.10068435, 0.1112282 , 0.08180123, 0.07313139, 0.10519364,\n",
              "         0.10391356, 0.09142353, 0.11156727, 0.11287722, 0.10817959],\n",
              "        [0.1270282 , 0.14128922, 0.05785076, 0.08127201, 0.08135925,\n",
              "         0.11459762, 0.09691458, 0.09998762, 0.10615646, 0.09354434],\n",
              "        [0.11502203, 0.11330213, 0.06756835, 0.07642207, 0.10134819,\n",
              "         0.10176824, 0.10386438, 0.10140561, 0.11586827, 0.10343073],\n",
              "        [0.12612322, 0.18005653, 0.06738123, 0.06941486, 0.07866958,\n",
              "         0.08583973, 0.07784604, 0.08121806, 0.13411573, 0.09933506],\n",
              "        [0.08294528, 0.09864344, 0.05798886, 0.05454007, 0.13714382,\n",
              "         0.09278899, 0.09764919, 0.14923649, 0.11985113, 0.10921278],\n",
              "        [0.13459942, 0.08004526, 0.04024955, 0.0603727 , 0.09173356,\n",
              "         0.10261993, 0.08442634, 0.14335342, 0.16711502, 0.09548476],\n",
              "        [0.09368701, 0.07746445, 0.09389038, 0.06913273, 0.12185442,\n",
              "         0.08918057, 0.09123114, 0.13792929, 0.1462744 , 0.07935559],\n",
              "        [0.12332999, 0.10673973, 0.05568034, 0.09954908, 0.10383832,\n",
              "         0.10854805, 0.10469811, 0.10847077, 0.09732506, 0.09182052],\n",
              "        [0.13302438, 0.1062191 , 0.05936985, 0.09372568, 0.09497083,\n",
              "         0.12580968, 0.08934028, 0.10044914, 0.08747766, 0.1096134 ],\n",
              "        [0.09102347, 0.09757339, 0.06039898, 0.08577572, 0.09481385,\n",
              "         0.11303271, 0.11888027, 0.1785904 , 0.07754516, 0.08236608],\n",
              "        [0.14778432, 0.14030357, 0.07783876, 0.07363558, 0.08715195,\n",
              "         0.09632382, 0.08406294, 0.08004241, 0.11311524, 0.09974136],\n",
              "        [0.11177283, 0.1264661 , 0.0812773 , 0.07574736, 0.10206309,\n",
              "         0.09516618, 0.08678146, 0.08942142, 0.11968325, 0.11162101],\n",
              "        [0.09046129, 0.10278266, 0.08777774, 0.07405172, 0.10918929,\n",
              "         0.10365616, 0.09397626, 0.09804325, 0.1196616 , 0.12039999],\n",
              "        [0.1193002 , 0.1037109 , 0.07014922, 0.10030112, 0.10514188,\n",
              "         0.10504957, 0.10006568, 0.10126953, 0.10056631, 0.09444564],\n",
              "        [0.10255358, 0.11639341, 0.07475613, 0.08202552, 0.10372476,\n",
              "         0.0906248 , 0.10921638, 0.11287241, 0.11434634, 0.09348661],\n",
              "        [0.1437163 , 0.11537608, 0.05735724, 0.09788139, 0.11052201,\n",
              "         0.10120643, 0.08962943, 0.08681384, 0.10672065, 0.0907766 ],\n",
              "        [0.11406942, 0.10159089, 0.06484608, 0.08797792, 0.09713548,\n",
              "         0.11105692, 0.09549423, 0.13684778, 0.09964011, 0.09134119],\n",
              "        [0.09843323, 0.13680828, 0.03890724, 0.0871855 , 0.12622038,\n",
              "         0.11501731, 0.0966212 , 0.12702113, 0.05922641, 0.11455935],\n",
              "        [0.15628204, 0.10124268, 0.07060648, 0.08074089, 0.09260752,\n",
              "         0.10777557, 0.08805272, 0.09469524, 0.09406289, 0.11393402],\n",
              "        [0.11229364, 0.08919087, 0.04533492, 0.11894187, 0.07275118,\n",
              "         0.16654913, 0.11523651, 0.10080198, 0.07050783, 0.10839207],\n",
              "        [0.11581243, 0.12044919, 0.07341372, 0.11815726, 0.0945267 ,\n",
              "         0.07435329, 0.08844532, 0.09405556, 0.10436672, 0.11641983],\n",
              "        [0.09843288, 0.10273967, 0.07517953, 0.0638321 , 0.11023504,\n",
              "         0.09425118, 0.09789983, 0.11845157, 0.1163445 , 0.12263376],\n",
              "        [0.09102347, 0.09757339, 0.06039898, 0.08577572, 0.09481385,\n",
              "         0.11303271, 0.11888027, 0.1785904 , 0.07754516, 0.08236608],\n",
              "        [0.07685541, 0.14169548, 0.05916088, 0.08283086, 0.1496345 ,\n",
              "         0.08706771, 0.08297537, 0.1112875 , 0.10546   , 0.10303229],\n",
              "        [0.09994338, 0.10397831, 0.06076644, 0.08921261, 0.11488084,\n",
              "         0.10704196, 0.10387021, 0.11903933, 0.1088746 , 0.09239233],\n",
              "        [0.11022187, 0.12885636, 0.07249928, 0.07253929, 0.09578782,\n",
              "         0.09499149, 0.095747  , 0.09735841, 0.12286294, 0.10913555],\n",
              "        [0.10450928, 0.13439621, 0.0674422 , 0.08977467, 0.09082437,\n",
              "         0.09849981, 0.09634157, 0.10185143, 0.12225613, 0.09410431],\n",
              "        [0.09650536, 0.11023679, 0.07822371, 0.07179869, 0.11301455,\n",
              "         0.08641563, 0.11039878, 0.12386334, 0.12075081, 0.08879241],\n",
              "        [0.14010054, 0.09438284, 0.06794211, 0.10020424, 0.09412197,\n",
              "         0.12604873, 0.08044472, 0.08990836, 0.10341953, 0.10342693],\n",
              "        [0.1072676 , 0.10608962, 0.07824175, 0.08461586, 0.10076954,\n",
              "         0.09904459, 0.09886751, 0.10741249, 0.10360173, 0.11408935],\n",
              "        [0.15602452, 0.1184999 , 0.0695616 , 0.08754668, 0.09191349,\n",
              "         0.11451484, 0.07921531, 0.08800456, 0.09346393, 0.10125513],\n",
              "        [0.09371822, 0.10989958, 0.06570219, 0.07513691, 0.10399111,\n",
              "         0.09336042, 0.08880644, 0.1400447 , 0.1273036 , 0.1020368 ],\n",
              "        [0.08294528, 0.09864344, 0.05798886, 0.05454007, 0.13714382,\n",
              "         0.09278899, 0.09764919, 0.14923649, 0.11985113, 0.10921278],\n",
              "        [0.11776922, 0.1017992 , 0.0395292 , 0.10215184, 0.10355297,\n",
              "         0.12824546, 0.09809738, 0.11993734, 0.0979647 , 0.0909527 ],\n",
              "        [0.11219335, 0.08934554, 0.04491899, 0.09041521, 0.08238383,\n",
              "         0.12679078, 0.11947056, 0.13230331, 0.08725331, 0.1149251 ],\n",
              "        [0.10214883, 0.10369729, 0.08466438, 0.07360507, 0.11810613,\n",
              "         0.09975203, 0.09867772, 0.09899076, 0.11588082, 0.10447695],\n",
              "        [0.09514356, 0.0840473 , 0.08955474, 0.09549582, 0.10187379,\n",
              "         0.11186992, 0.07337503, 0.12773554, 0.10842883, 0.11247543],\n",
              "        [0.1288774 , 0.09658969, 0.07142612, 0.07046188, 0.09107547,\n",
              "         0.08725289, 0.10574154, 0.1196054 , 0.11800592, 0.11096372],\n",
              "        [0.13454698, 0.1382774 , 0.07203634, 0.07958676, 0.0643901 ,\n",
              "         0.1307732 , 0.08972348, 0.09120489, 0.10418835, 0.09527247],\n",
              "        [0.09992205, 0.1448912 , 0.06049781, 0.10335918, 0.06971262,\n",
              "         0.10094378, 0.1035734 , 0.10421263, 0.13282183, 0.08006543],\n",
              "        [0.10200881, 0.10996534, 0.07772636, 0.07746572, 0.10911443,\n",
              "         0.09093094, 0.1041798 , 0.12059155, 0.12455855, 0.08345849],\n",
              "        [0.1206232 , 0.11876403, 0.05956074, 0.09286504, 0.09103238,\n",
              "         0.12462827, 0.1030242 , 0.10510171, 0.08587679, 0.09852362],\n",
              "        [0.09843323, 0.13680828, 0.03890724, 0.0871855 , 0.12622038,\n",
              "         0.11501731, 0.0966212 , 0.12702113, 0.05922641, 0.11455935],\n",
              "        [0.10876068, 0.10412666, 0.08145559, 0.05811454, 0.09254317,\n",
              "         0.09182794, 0.11171102, 0.13119057, 0.11730621, 0.10296363],\n",
              "        [0.11410481, 0.11342342, 0.06903362, 0.08993421, 0.09836479,\n",
              "         0.11714321, 0.11546186, 0.08897451, 0.10446736, 0.08909225],\n",
              "        [0.10623898, 0.11096817, 0.06083372, 0.09395166, 0.10448372,\n",
              "         0.097261  , 0.10724517, 0.10270771, 0.11014724, 0.10616268],\n",
              "        [0.10731708, 0.13392618, 0.08299652, 0.07903486, 0.05315885,\n",
              "         0.10104643, 0.07874923, 0.11397091, 0.14435294, 0.10544692],\n",
              "        [0.13344806, 0.14030646, 0.05876264, 0.09517987, 0.09615133,\n",
              "         0.12352727, 0.08317634, 0.08347587, 0.08842599, 0.09754614],\n",
              "        [0.11072048, 0.12007309, 0.0720467 , 0.08016524, 0.11097187,\n",
              "         0.08996183, 0.09792287, 0.10120019, 0.11596254, 0.10097514],\n",
              "        [0.10512997, 0.12231472, 0.05872236, 0.10296156, 0.08375867,\n",
              "         0.10069896, 0.09813084, 0.11957964, 0.10762398, 0.10107924],\n",
              "        [0.10920625, 0.12242422, 0.07306436, 0.07872839, 0.09864315,\n",
              "         0.09782827, 0.10708044, 0.08191646, 0.11775439, 0.11335407],\n",
              "        [0.10249998, 0.10132707, 0.07336453, 0.07431751, 0.11791597,\n",
              "         0.09023751, 0.10837872, 0.08985043, 0.11844156, 0.12366673],\n",
              "        [0.14421117, 0.1160873 , 0.05812858, 0.10739569, 0.10476472,\n",
              "         0.11408918, 0.10060101, 0.07246318, 0.0812282 , 0.10103098],\n",
              "        [0.11161467, 0.11700711, 0.06582183, 0.08881649, 0.09197623,\n",
              "         0.10047736, 0.10369109, 0.11310128, 0.10675497, 0.10073898],\n",
              "        [0.08294528, 0.09864344, 0.05798886, 0.05454007, 0.13714382,\n",
              "         0.09278899, 0.09764919, 0.14923649, 0.11985113, 0.10921278],\n",
              "        [0.11451159, 0.1103459 , 0.08075569, 0.07561272, 0.10394853,\n",
              "         0.10168751, 0.08382832, 0.07752543, 0.11341187, 0.13837245],\n",
              "        [0.11119668, 0.13943109, 0.07320657, 0.07858419, 0.08292571,\n",
              "         0.09787799, 0.10336684, 0.09316088, 0.12764609, 0.09260395],\n",
              "        [0.12326165, 0.13486572, 0.07283065, 0.07455833, 0.08557045,\n",
              "         0.10115083, 0.09864704, 0.09297866, 0.11002804, 0.10610859],\n",
              "        [0.09796619, 0.11238685, 0.07127564, 0.07012141, 0.11174534,\n",
              "         0.08868899, 0.11186954, 0.12271801, 0.10995249, 0.1032755 ],\n",
              "        [0.13671097, 0.12045936, 0.07372427, 0.10363618, 0.10074627,\n",
              "         0.10980107, 0.10033737, 0.07333369, 0.08606309, 0.09518772],\n",
              "        [0.12164573, 0.10187594, 0.08879802, 0.08035747, 0.08026727,\n",
              "         0.11521995, 0.07940497, 0.10297462, 0.12111054, 0.10834555],\n",
              "        [0.11700373, 0.12044968, 0.06443618, 0.08572507, 0.10056087,\n",
              "         0.11061974, 0.10532637, 0.0917575 , 0.11146441, 0.09265641],\n",
              "        [0.10573108, 0.13252015, 0.05452974, 0.08687763, 0.1103872 ,\n",
              "         0.11083354, 0.09545371, 0.10710732, 0.09010012, 0.10645954],\n",
              "        [0.12194899, 0.11568983, 0.07570728, 0.0856947 , 0.09768391,\n",
              "         0.1024548 , 0.09342764, 0.09715788, 0.09738296, 0.11285195],\n",
              "        [0.11103784, 0.11048789, 0.06821129, 0.08811706, 0.10796829,\n",
              "         0.10729751, 0.09496345, 0.1154433 , 0.10080875, 0.09566462],\n",
              "        [0.1080125 , 0.1121287 , 0.07366811, 0.07293118, 0.10371925,\n",
              "         0.1034295 , 0.08981749, 0.10494686, 0.11325417, 0.11809219],\n",
              "        [0.10171346, 0.11195928, 0.06796297, 0.08621716, 0.09572536,\n",
              "         0.10993495, 0.10594796, 0.12711875, 0.10310987, 0.09031023],\n",
              "        [0.11450336, 0.10275435, 0.06730125, 0.08714356, 0.10825722,\n",
              "         0.10548643, 0.09374133, 0.12612745, 0.10068157, 0.09400347],\n",
              "        [0.10811579, 0.10177977, 0.08150685, 0.08005818, 0.09774921,\n",
              "         0.09621519, 0.09520968, 0.11077266, 0.10446369, 0.12412898],\n",
              "        [0.12700571, 0.10891881, 0.07044307, 0.08752782, 0.1003249 ,\n",
              "         0.10650223, 0.08901795, 0.11294604, 0.10721587, 0.09009754],\n",
              "        [0.10004397, 0.10038221, 0.10657319, 0.05859204, 0.10315572,\n",
              "         0.08907883, 0.11509279, 0.10468394, 0.12070014, 0.10169723],\n",
              "        [0.1056345 , 0.13414365, 0.0668655 , 0.05976947, 0.09782533,\n",
              "         0.10833876, 0.09423655, 0.09501781, 0.13036938, 0.10779899]],\n",
              "       dtype=float32)>,\n",
              " <tf.Tensor: shape=(100, 100), dtype=float32, numpy=\n",
              " array([[ 0.14155078, -0.13662763, -0.35994872, ..., -0.10069795,\n",
              "         -0.07542082,  0.3199444 ],\n",
              "        [ 0.13395348, -0.13620259, -0.35874954, ..., -0.09063661,\n",
              "         -0.06494228,  0.33203268],\n",
              "        [ 0.13391255, -0.13483776, -0.35739726, ..., -0.08166029,\n",
              "         -0.06427686,  0.3397202 ],\n",
              "        ...,\n",
              "        [ 0.13550034, -0.1478856 , -0.36815915, ..., -0.08038923,\n",
              "         -0.0795103 ,  0.33705953],\n",
              "        [ 0.13744658, -0.12875472, -0.351242  , ..., -0.08058126,\n",
              "         -0.06340886,  0.34717306],\n",
              "        [ 0.12779577, -0.1386618 , -0.36538237, ..., -0.07945152,\n",
              "         -0.0735895 ,  0.34374124]], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_cYS1J5oI3D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c252a58b-6846-42b4-a25f-0fbfc7da5356"
      },
      "source": [
        "#model initialisation\n",
        "#abae=model_(embed_inputdim,embed_outputdim,trained_weights,aspects_k,dense,inputlength)\n",
        "# optimiser\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "@tf.function\n",
        "def train_step(input):\n",
        "    with tf.GradientTape() as tape:\n",
        "        #forward propagation\n",
        "        loss = test(input)[0]\n",
        "        #print(loss)\n",
        "\n",
        "    #getting gradients\n",
        "    gradients = tape.gradient(loss, test.trainable_variables)\n",
        "    #applying gradients\n",
        "    optimizer.apply_gradients(zip(gradients, test.trainable_variables))\n",
        "\n",
        "    return loss, gradients\n",
        "#no_iterations=1147*5          #epochs\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "\n",
        "checkpoint_path = \"/content/drive/My Drive/abae_logs/checkpoints/abae/train\"\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "ckpt = tf.train.Checkpoint(optimizer=optimizer, model=test)\n",
        "\n",
        "latest = tf.train.latest_checkpoint(checkpoint_path)\n",
        "ckpt.restore(latest)\n",
        "\n",
        "\n",
        "##check point to save\n",
        "#checkpoint_path = \"/content/drive/My Drive/abae_logs/checkpoints/abae/train_v2\"\n",
        "#ckpt = tf.train.Checkpoint(optimizer=optimizer, model=test)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
        "\n",
        "\n",
        "it=0\n",
        "loss_list=[]\n",
        "for k in range(0,1):\n",
        "  counter = 0\n",
        "  for input in datagen:\n",
        "\n",
        "      loss_, gradients = train_step(input)\n",
        "      #adding loss to train loss\n",
        "      train_loss(loss_)\n",
        "      counter = counter + 1\n",
        "      template = '''Done {} step, Loss: {:0.6f}'''\n",
        "\n",
        "    \n",
        "      if counter%100==0:\n",
        "        print(template.format(counter, train_loss.result()))\n",
        "        \n",
        "  loss_list.append(train_loss.result())\n",
        "  ckpt_save_path  = ckpt_manager.save()\n",
        "  print ('Saving checkpoint for iteration {} at {}'.format(k+1, ckpt_save_path))\n",
        "  print(counter, train_loss.result())\n",
        "  train_loss.reset_states()\n",
        "  \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done 100 step, Loss: 0.008561\n",
            "Done 200 step, Loss: 0.008550\n",
            "Done 300 step, Loss: 0.008563\n",
            "Done 400 step, Loss: 0.008538\n",
            "Done 500 step, Loss: 0.008546\n",
            "Done 600 step, Loss: 0.008532\n",
            "Done 700 step, Loss: 0.008539\n",
            "Done 800 step, Loss: 0.008539\n",
            "Done 900 step, Loss: 0.008542\n",
            "Done 1000 step, Loss: 0.008534\n",
            "Done 1100 step, Loss: 0.008536\n",
            "Done 1200 step, Loss: 0.008532\n",
            "Done 1300 step, Loss: 0.008531\n",
            "Done 1400 step, Loss: 0.008532\n",
            "Done 1500 step, Loss: 0.008530\n",
            "Done 1600 step, Loss: 0.008528\n",
            "Done 1700 step, Loss: 0.008530\n",
            "Done 1800 step, Loss: 0.008530\n",
            "Done 1900 step, Loss: 0.008526\n",
            "Done 2000 step, Loss: 0.008526\n",
            "Done 2100 step, Loss: 0.008525\n",
            "Done 2200 step, Loss: 0.008523\n",
            "Done 2300 step, Loss: 0.008524\n",
            "Done 2400 step, Loss: 0.008524\n",
            "Done 2500 step, Loss: 0.008521\n",
            "Done 2600 step, Loss: 0.008521\n",
            "Done 2700 step, Loss: 0.008522\n",
            "Done 2800 step, Loss: 0.008518\n",
            "Done 2900 step, Loss: 0.008518\n",
            "Done 3000 step, Loss: 0.008517\n",
            "Done 3100 step, Loss: 0.008516\n",
            "Done 3200 step, Loss: 0.008515\n",
            "Done 3300 step, Loss: 0.008514\n",
            "Done 3400 step, Loss: 0.008512\n",
            "Done 3500 step, Loss: 0.008512\n",
            "Done 3600 step, Loss: 0.008512\n",
            "Saving checkpoint for iteration 1 at /content/drive/My Drive/abae_logs/checkpoints/abae/train/ckpt-17\n",
            "3630 tf.Tensor(0.008512795, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "id": "duouubyyMHzd",
        "outputId": "dafa19f2-8ec3-4d05-bd12-fd93b2a0cd20"
      },
      "source": [
        "topics=test.weights[4].numpy()\n",
        "topic_words={}\n",
        "k=1\n",
        "for i in range(len(topics)):\n",
        "  topic_words[\"Aspect_\"+str(k)]=np.argsort(model.wv.cosine_similarities(topics[i],model.wv.vectors))[::-1][0:20]\n",
        "  k=k+1\n",
        "pd.DataFrame(topic_words).applymap(lambda x:model.wv.index2word[x])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Aspect_1</th>\n",
              "      <th>Aspect_2</th>\n",
              "      <th>Aspect_3</th>\n",
              "      <th>Aspect_4</th>\n",
              "      <th>Aspect_5</th>\n",
              "      <th>Aspect_6</th>\n",
              "      <th>Aspect_7</th>\n",
              "      <th>Aspect_8</th>\n",
              "      <th>Aspect_9</th>\n",
              "      <th>Aspect_10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rs</td>\n",
              "      <td>amoled</td>\n",
              "      <td>shipping</td>\n",
              "      <td>awesome</td>\n",
              "      <td>feels</td>\n",
              "      <td>achha</td>\n",
              "      <td>turn</td>\n",
              "      <td>disappoint</td>\n",
              "      <td>nice</td>\n",
              "      <td>fraud</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>billion</td>\n",
              "      <td>stunning</td>\n",
              "      <td>deliver</td>\n",
              "      <td>really</td>\n",
              "      <td>classy</td>\n",
              "      <td>koi</td>\n",
              "      <td>notifications</td>\n",
              "      <td>stunning</td>\n",
              "      <td>good</td>\n",
              "      <td>seller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>discount</td>\n",
              "      <td>amazing</td>\n",
              "      <td>30th</td>\n",
              "      <td>good</td>\n",
              "      <td>grip</td>\n",
              "      <td>mast</td>\n",
              "      <td>access</td>\n",
              "      <td>great</td>\n",
              "      <td>nicr</td>\n",
              "      <td>refused</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>offer</td>\n",
              "      <td>cameras</td>\n",
              "      <td>ekart</td>\n",
              "      <td>phone</td>\n",
              "      <td>sleek</td>\n",
              "      <td>acha</td>\n",
              "      <td>app</td>\n",
              "      <td>awesome</td>\n",
              "      <td>awesome</td>\n",
              "      <td>fault</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3500</td>\n",
              "      <td>awesome</td>\n",
              "      <td>ordered</td>\n",
              "      <td>superb</td>\n",
              "      <td>premium</td>\n",
              "      <td>kharab</td>\n",
              "      <td>disable</td>\n",
              "      <td>really</td>\n",
              "      <td>greate</td>\n",
              "      <td>executives</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>13000</td>\n",
              "      <td>brilliant</td>\n",
              "      <td>delivered</td>\n",
              "      <td>great</td>\n",
              "      <td>design</td>\n",
              "      <td>hai</td>\n",
              "      <td>vibrate</td>\n",
              "      <td>indeed</td>\n",
              "      <td>wesome</td>\n",
              "      <td>accepting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>8999</td>\n",
              "      <td>outstanding</td>\n",
              "      <td>delivery</td>\n",
              "      <td>also</td>\n",
              "      <td>looks</td>\n",
              "      <td>ko</td>\n",
              "      <td>irritates</td>\n",
              "      <td>amazing</td>\n",
              "      <td>mast</td>\n",
              "      <td>sent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3k</td>\n",
              "      <td>decent</td>\n",
              "      <td>flipkart</td>\n",
              "      <td>excellent</td>\n",
              "      <td>handy</td>\n",
              "      <td>nhi</td>\n",
              "      <td>button</td>\n",
              "      <td>beast</td>\n",
              "      <td>avarge</td>\n",
              "      <td>accept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>discounts</td>\n",
              "      <td>ai</td>\n",
              "      <td>saturday</td>\n",
              "      <td>one</td>\n",
              "      <td>hold</td>\n",
              "      <td>ek</td>\n",
              "      <td>application</td>\n",
              "      <td>loving</td>\n",
              "      <td>thanku</td>\n",
              "      <td>flipkart</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>13500</td>\n",
              "      <td>especially</td>\n",
              "      <td>august</td>\n",
              "      <td>amazing</td>\n",
              "      <td>hands</td>\n",
              "      <td>ki</td>\n",
              "      <td>icon</td>\n",
              "      <td>outstanding</td>\n",
              "      <td>fantastic</td>\n",
              "      <td>replacement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>5k</td>\n",
              "      <td>really</td>\n",
              "      <td>oct</td>\n",
              "      <td>fantastic</td>\n",
              "      <td>slippery</td>\n",
              "      <td>mai</td>\n",
              "      <td>rings</td>\n",
              "      <td>classy</td>\n",
              "      <td>osm</td>\n",
              "      <td>electronics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12999</td>\n",
              "      <td>great</td>\n",
              "      <td>seller</td>\n",
              "      <td>battery</td>\n",
              "      <td>compact</td>\n",
              "      <td>kam</td>\n",
              "      <td>bluetooth</td>\n",
              "      <td>incredible</td>\n",
              "      <td>wsome</td>\n",
              "      <td>customers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12000</td>\n",
              "      <td>makes</td>\n",
              "      <td>order</td>\n",
              "      <td>actually</td>\n",
              "      <td>sturdy</td>\n",
              "      <td>ye</td>\n",
              "      <td>silent</td>\n",
              "      <td>premium</td>\n",
              "      <td>delevry</td>\n",
              "      <td>send</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2999</td>\n",
              "      <td>natural</td>\n",
              "      <td>12th</td>\n",
              "      <td>ok</td>\n",
              "      <td>stunning</td>\n",
              "      <td>accha</td>\n",
              "      <td>launcher</td>\n",
              "      <td>killer</td>\n",
              "      <td>vry</td>\n",
              "      <td>saying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>11700</td>\n",
              "      <td>stereo</td>\n",
              "      <td>launch</td>\n",
              "      <td>design</td>\n",
              "      <td>pretty</td>\n",
              "      <td>bhi</td>\n",
              "      <td>notification</td>\n",
              "      <td>truly</td>\n",
              "      <td>exelent</td>\n",
              "      <td>staff</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>billions</td>\n",
              "      <td>specially</td>\n",
              "      <td>packing</td>\n",
              "      <td>disappointed</td>\n",
              "      <td>hand</td>\n",
              "      <td>lekin</td>\n",
              "      <td>scroll</td>\n",
              "      <td>beat</td>\n",
              "      <td>also</td>\n",
              "      <td>request</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>8000</td>\n",
              "      <td>crisp</td>\n",
              "      <td>booked</td>\n",
              "      <td>decent</td>\n",
              "      <td>metallic</td>\n",
              "      <td>bahut</td>\n",
              "      <td>disabled</td>\n",
              "      <td>brilliant</td>\n",
              "      <td>fabulous</td>\n",
              "      <td>genuine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>8k</td>\n",
              "      <td>captures</td>\n",
              "      <td>dilevry</td>\n",
              "      <td>outstanding</td>\n",
              "      <td>glossy</td>\n",
              "      <td>kiya</td>\n",
              "      <td>vibration</td>\n",
              "      <td>terms</td>\n",
              "      <td>osom</td>\n",
              "      <td>rejected</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>10000</td>\n",
              "      <td>camera</td>\n",
              "      <td>26th</td>\n",
              "      <td>think</td>\n",
              "      <td>lightweight</td>\n",
              "      <td>nahi</td>\n",
              "      <td>shortcut</td>\n",
              "      <td>decent</td>\n",
              "      <td>bettery</td>\n",
              "      <td>said</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>deal</td>\n",
              "      <td>display</td>\n",
              "      <td>22nd</td>\n",
              "      <td>disappointing</td>\n",
              "      <td>elegant</td>\n",
              "      <td>ka</td>\n",
              "      <td>tap</td>\n",
              "      <td>flagship</td>\n",
              "      <td>ok</td>\n",
              "      <td>fake</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Aspect_1     Aspect_2   Aspect_3  ...     Aspect_8   Aspect_9    Aspect_10\n",
              "0          rs       amoled   shipping  ...   disappoint       nice        fraud\n",
              "1     billion     stunning    deliver  ...     stunning       good       seller\n",
              "2    discount      amazing       30th  ...        great       nicr      refused\n",
              "3       offer      cameras      ekart  ...      awesome    awesome        fault\n",
              "4        3500      awesome    ordered  ...       really     greate   executives\n",
              "5       13000    brilliant  delivered  ...       indeed     wesome    accepting\n",
              "6        8999  outstanding   delivery  ...      amazing       mast         sent\n",
              "7          3k       decent   flipkart  ...        beast     avarge       accept\n",
              "8   discounts           ai   saturday  ...       loving     thanku     flipkart\n",
              "9       13500   especially     august  ...  outstanding  fantastic  replacement\n",
              "10         5k       really        oct  ...       classy        osm  electronics\n",
              "11      12999        great     seller  ...   incredible      wsome    customers\n",
              "12      12000        makes      order  ...      premium    delevry         send\n",
              "13       2999      natural       12th  ...       killer        vry       saying\n",
              "14      11700       stereo     launch  ...        truly    exelent        staff\n",
              "15   billions    specially    packing  ...         beat       also      request\n",
              "16       8000        crisp     booked  ...    brilliant   fabulous      genuine\n",
              "17         8k     captures    dilevry  ...        terms       osom     rejected\n",
              "18      10000       camera       26th  ...       decent    bettery         said\n",
              "19       deal      display       22nd  ...     flagship         ok         fake\n",
              "\n",
              "[20 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R492LZn1S-4e"
      },
      "source": [
        "### Aspects Inference\n",
        "#### From above collection of words, we can conclude some of the aspects as below ---\n",
        "Aspect1 - Price\n",
        "\n",
        "Aspect2 - Camera and the picture display\n",
        "\n",
        "Aspect3 - Delivery\n",
        "\n",
        "Aspect4 - Positive intent words\n",
        "\n",
        "Aspect5 - Physical experience (or features) of mobile\n",
        "\n",
        "Aspect6 - Collection of Hindi Language words \n",
        "\n",
        "Aspect7 - Accessing different applications\n",
        "\n",
        "Aspect8 - Extreme positive intent about product\n",
        "\n",
        "Aspect9 - Incorrectly spelled positive intent about product\n",
        "\n",
        "Aspect10- Bad CustomerService experience"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjkIO1GYS01U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}